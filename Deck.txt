#!/usr/bin/env python3
"""
TTL to CSV to FalkorDB Ultra-Fast Pipeline (Updated for Latest FalkorDB API)
Converts TTL to CSV format then uses FalkorDB's native bulk loader for maximum speed
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import subprocess
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
import sys
import shutil
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log')
    ]
)
logger = logging.getLogger(__name__)

class TTLToCSVConverter:
    def __init__(self, output_dir='csv_output', use_schema=True):
        """Initialize the TTL to CSV converter"""
        self.output_dir = Path(output_dir)
        self.use_schema = use_schema  # Use enforce-schema format for better compatibility
        self.nodes_by_type = {}  # type -> dict of {id: properties}
        self.edges_by_type = {}  # relationship -> list of edges
        self.node_id_map = {}    # URI -> unique_id
        self.next_id = 1
        self.bulk_loader_cmd = None
        
        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # CSV writing parameters for consistency
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            uri_str = str(uri_or_literal)
            parsed = urlparse(uri_str)
            
            # Try fragment first (after #)
            if parsed.fragment:
                name = parsed.fragment
            # Then try last path component
            elif parsed.path and parsed.path != '/':
                name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
            # Fall back to netloc
            elif parsed.netloc:
                name = parsed.netloc.replace('.', '_')
            else:
                # Use hash of full URI as fallback
                name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
            
            # Clean the name - only alphanumeric and underscore
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            # Ensure it starts with letter or underscore
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"n_{cleaned}"
            # Ensure it's not empty and has reasonable length
            if not cleaned or len(cleaned) < 1:
                cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
            return cleaned[:50]  # Limit length
        else:
            # Handle literals or other types
            name = str(uri_or_literal)
            cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
            if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                cleaned = f"l_{cleaned}"
            return (cleaned or "literal")[:50]
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique sequential ID for a resource"""
        uri = str(resource)
        if uri not in self.node_id_map:
            self.node_id_map[uri] = str(self.next_id)
            self.next_id += 1
        return self.node_id_map[uri]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path and parsed.path != '/':
            parts = [p for p in parsed.path.strip('/').split('/') if p]
            if parts:
                properties['local_name'] = parts[-1]
                properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[str, str]:
        """Process literal value and return (cleaned_value, datatype)"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                # Handle common XSD datatypes
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int(literal)  # Validate it's actually an integer
                        return str(literal), 'INT'
                    except ValueError:
                        return str(literal), 'STRING'
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float(literal)  # Validate it's actually a number
                        return str(literal), 'DOUBLE'
                    except ValueError:
                        return str(literal), 'STRING'
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower(), 'BOOL'
                    
                elif any(x in datatype_str.lower() for x in ['date', 'time']):
                    return str(literal), 'STRING'  # Store dates as strings
                    
                else:
                    return str(literal), 'STRING'
            else:
                # No datatype specified
                return str(literal), 'STRING'
                
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal), 'STRING'
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        if value is None:
            return ''
        
        if isinstance(value, bool):
            return 'true' if value else 'false'
        
        if isinstance(value, (list, dict)):
            # Convert arrays to string representation for CSV
            if isinstance(value, list):
                # Join array elements with semicolon (safer than comma)
                return ';'.join(str(item) for item in value)
            else:
                return json.dumps(value, ensure_ascii=False)
        
        # Convert to string and clean
        str_value = str(value).strip()
        
        # Replace problematic characters for CSV
        str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
        str_value = re.sub(r'\s+', ' ', str_value)  # Normalize whitespace
        
        # Escape quotes by doubling them (CSV standard)
        if '"' in str_value:
            str_value = str_value.replace('"', '""')
        
        return str_value
    
    def convert_ttl_to_csv(self, ttl_file_path: str):
        """Convert TTL file to CSV format optimized for FalkorDB bulk loader"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Parse TTL file
        graph = Graph()
        try:
            logger.info("Parsing TTL file...")
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Successfully parsed TTL file. Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found in the TTL file!")
            return []
        
        # Track properties and their types for schema
        node_properties = {}  # node_type -> dict of {prop_name: datatype}
        edge_properties = {}  # edge_type -> dict of {prop_name: datatype}
        
        logger.info("Processing triples and building data structures...")
        with tqdm(total=total_triples, desc="Processing triples") as pbar:
            for subject, predicate, obj in graph:
                pbar.update(1)
                
                # Get or create subject node
                subject_id = self.get_or_create_node_id(subject)
                subject_type = self.clean_identifier(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Initialize node type tracking
                if subject_type not in self.nodes_by_type:
                    self.nodes_by_type[subject_type] = {}
                    node_properties[subject_type] = {}
                
                # Initialize subject node if not exists
                if subject_id not in self.nodes_by_type[subject_type]:
                    if isinstance(subject, URIRef):
                        base_props = self.extract_properties_from_uri(subject)
                    else:  # BNode
                        base_props = {
                            'uri': str(subject),
                            'resource_type': 'blank_node'
                        }
                    
                    self.nodes_by_type[subject_type][subject_id] = base_props
                    
                    # Track properties with types
                    for prop_name, prop_value in base_props.items():
                        node_properties[subject_type][prop_name] = 'STRING'
                
                # Handle object
                if isinstance(obj, Literal):
                    # Add as property to subject node
                    value, datatype = self.process_literal_value(obj)
                    
                    # Clean property name
                    clean_prop = predicate_clean
                    
                    # Store the value
                    current_node = self.nodes_by_type[subject_type][subject_id]
                    current_node[clean_prop] = self.sanitize_csv_value(value)
                    
                    # Track property type
                    node_properties[subject_type][clean_prop] = datatype
                    
                    # Store language if present
                    if obj.language:
                        lang_prop = f"{clean_prop}_lang"
                        current_node[lang_prop] = obj.language
                        node_properties[subject_type][lang_prop] = 'STRING'
                
                else:
                    # Object is a resource - create edge
                    object_id = self.get_or_create_node_id(obj)
                    object_type = self.clean_identifier(obj)
                    
                    # Initialize object node if not exists
                    if object_type not in self.nodes_by_type:
                        self.nodes_by_type[object_type] = {}
                        node_properties[object_type] = {}
                    
                    if object_id not in self.nodes_by_type[object_type]:
                        if isinstance(obj, URIRef):
                            base_props = self.extract_properties_from_uri(obj)
                        else:  # BNode
                            base_props = {
                                'uri': str(obj),
                                'resource_type': 'blank_node'
                            }
                        
                        self.nodes_by_type[object_type][object_id] = base_props
                        
                        # Track properties with types
                        for prop_name, prop_value in base_props.items():
                            node_properties[object_type][prop_name] = 'STRING'
                    
                    # Create edge
                    if predicate_clean not in self.edges_by_type:
                        self.edges_by_type[predicate_clean] = []
                        edge_properties[predicate_clean] = {'predicate_uri': 'STRING'}
                    
                    edge = {
                        'source_id': subject_id,
                        'target_id': object_id,
                        'predicate_uri': self.sanitize_csv_value(str(predicate))
                    }
                    self.edges_by_type[predicate_clean].append(edge)
        
        logger.info("Schema analysis complete:")
        logger.info(f"  Node types: {len(self.nodes_by_type)} ({list(self.nodes_by_type.keys())})")
        logger.info(f"  Edge types: {len(self.edges_by_type)} ({list(self.edges_by_type.keys())})")
        
        # Write CSV files in FalkorDB format
        csv_files = []
        
        # Write node CSV files
        logger.info("Writing node CSV files...")
        for node_type, nodes in self.nodes_by_type.items():
            if not nodes:
                logger.warning(f"Skipping empty node type: {node_type}")
                continue
                
            # Use proper filename (label derived from filename)
            filename = self.output_dir / f"{node_type}.csv"
            csv_files.append(('nodes', str(filename)))
            
            # Get properties for this node type
            props_with_types = node_properties.get(node_type, {})
            
            try:
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.writer(csvfile, **self.csv_params)
                    
                    if self.use_schema:
                        # Use schema format with type annotations
                        headers = [':ID']  # ID column first
                        for prop_name, prop_type in sorted(props_with_types.items()):
                            headers.append(f"{prop_name}:{prop_type}")
                    else:
                        # Use simple format (ID first, then properties)
                        headers = ['id'] + sorted(props_with_types.keys())
                    
                    writer.writerow(headers)
                    
                    for node_id, node_data in sorted(nodes.items()):
                        row = [node_id]  # ID first
                        for prop_name in sorted(props_with_types.keys()):
                            value = node_data.get(prop_name, '')
                            row.append(self.sanitize_csv_value(value))
                        writer.writerow(row)
                
                logger.info(f"  ‚úÖ Written {len(nodes):,} {node_type} nodes to {filename.name}")
                
            except Exception as e:
                logger.error(f"Error writing node file {filename}: {e}")
                raise
        
        # Write edge CSV files
        logger.info("Writing edge CSV files...")
        for edge_type, edges in self.edges_by_type.items():
            if not edges:
                logger.warning(f"Skipping empty edge type: {edge_type}")
                continue
                
            # Use proper filename (relationship type derived from filename)
            filename = self.output_dir / f"{edge_type}.csv"
            csv_files.append(('relationships', str(filename)))
            
            # Get properties for this edge type
            props_with_types = edge_properties[edge_type]
            
            try:
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.writer(csvfile, **self.csv_params)
                    
                    if self.use_schema:
                        # Use schema format
                        headers = [':START_ID', ':END_ID']  # Required first two columns
                        for prop_name, prop_type in sorted(props_with_types.items()):
                            if prop_name not in ['source_id', 'target_id']:
                                headers.append(f"{prop_name}:{prop_type}")
                    else:
                        # Use simple format
                        headers = ['source_id', 'target_id']
                        for prop_name in sorted(props_with_types.keys()):
                            if prop_name not in ['source_id', 'target_id']:
                                headers.append(prop_name)
                    
                    writer.writerow(headers)
                    
                    for edge in edges:
                        row = [edge['source_id'], edge['target_id']]
                        for prop_name in sorted(props_with_types.keys()):
                            if prop_name not in ['source_id', 'target_id']:
                                value = edge.get(prop_name, '')
                                row.append(self.sanitize_csv_value(value))
                        writer.writerow(row)
                
                logger.info(f"  ‚úÖ Written {len(edges):,} {edge_type} edges to {filename.name}")
                
            except Exception as e:
                logger.error(f"Error writing edge file {filename}: {e}")
                raise
        
        # Validate CSV files
        self.validate_csv_files(csv_files)
        
        logger.info(f"‚úÖ CSV conversion completed! {len(csv_files)} files in {self.output_dir}/")
        return csv_files
    
    def validate_csv_files(self, csv_files: List[Tuple[str, str]]):
        """Validate CSV files for FalkorDB bulk loader compatibility"""
        logger.info("Validating CSV files...")
        
        all_node_ids = set()
        edge_references = set()
        validation_errors = []
        
        for file_type, filename in csv_files:
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    
                    try:
                        headers = next(reader)
                    except StopIteration:
                        validation_errors.append(f"Empty file: {filename}")
                        continue
                    
                    row_count = 0
                    for row_num, row in enumerate(reader, 1):
                        if not row or len(row) != len(headers):
                            if len(row) > 0:  # Only report non-empty rows with wrong length
                                validation_errors.append(
                                    f"{filename}:{row_num} - Row length mismatch: "
                                    f"expected {len(headers)}, got {len(row)}"
                                )
                            continue
                        
                        row_count += 1
                        
                        if file_type == 'nodes':
                            # Check ID column (first column)
                            expected_id_col = ':ID' if self.use_schema else 'id'
                            if headers[0] != expected_id_col:
                                validation_errors.append(
                                    f"{filename} - First column should be '{expected_id_col}', found '{headers[0]}'"
                                )
                            
                            node_id = row[0]
                            if not node_id:
                                validation_errors.append(f"{filename}:{row_num} - Empty node ID")
                            else:
                                all_node_ids.add(node_id)
                        
                        elif file_type == 'relationships':
                            # Check source/target columns
                            if self.use_schema:
                                expected_src, expected_tgt = ':START_ID', ':END_ID'
                            else:
                                expected_src, expected_tgt = 'source_id', 'target_id'
                            
                            if len(headers) < 2:
                                validation_errors.append(f"{filename} - Must have at least 2 columns")
                            elif headers[0] != expected_src or headers[1] != expected_tgt:
                                validation_errors.append(
                                    f"{filename} - First two columns should be '{expected_src}', '{expected_tgt}', "
                                    f"found '{headers[0]}', '{headers[1]}'"
                                )
                            
                            if len(row) >= 2:
                                source_id, target_id = row[0], row[1]
                                if not source_id or not target_id:
                                    validation_errors.append(
                                        f"{filename}:{row_num} - Empty source or target ID"
                                    )
                                else:
                                    edge_references.add(source_id)
                                    edge_references.add(target_id)
                    
                    logger.info(f"  üìÑ {filename}: {row_count:,} rows, {len(headers)} columns")
                    
            except Exception as e:
                validation_errors.append(f"Error reading {filename}: {e}")
        
        # Check referential integrity
        missing_nodes = edge_references - all_node_ids
        if missing_nodes:
            validation_errors.append(
                f"Found {len(missing_nodes)} edge references without corresponding nodes"
            )
            if len(missing_nodes) <= 10:
                validation_errors.append(f"Missing node IDs: {sorted(list(missing_nodes))}")
        
        # Report validation results
        if validation_errors:
            logger.warning(f"‚ö†Ô∏è  Found {len(validation_errors)} validation issues:")
            for error in validation_errors[:10]:  # Show first 10 errors
                logger.warning(f"  - {error}")
            if len(validation_errors) > 10:
                logger.warning(f"  ... and {len(validation_errors) - 10} more")
        else:
            logger.info("‚úÖ All CSV files passed validation")
        
        logger.info(f"Summary: {len(all_node_ids):,} nodes, {len(edge_references):,} edge references")
        return len(validation_errors) == 0
    
    def find_bulk_loader(self) -> Optional[str]:
        """Find available FalkorDB bulk loader command"""
        commands_to_try = [
            'falkordb-bulk-insert',
            'python3 -m falkordb_bulk_loader',
            'python -m falkordb_bulk_loader'
        ]
        
        for cmd in commands_to_try:
            try:
                # Test if command exists by checking help
                process = subprocess.run(
                    cmd.split() + ['--help'],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if process.returncode == 0:
                    logger.info(f"‚úÖ Found bulk loader: {cmd}")
                    return cmd
            except (subprocess.TimeoutExpired, FileNotFoundError, OSError):
                continue
        
        return None
    
    def run_falkor_bulk_loader(self, csv_files: List[Tuple[str, str]], graph_name: str,
                              redis_url: str = 'redis://127.0.0.1:6379'):
        """Run FalkorDB bulk loader with proper error handling"""
        
        # Find bulk loader
        self.bulk_loader_cmd = self.find_bulk_loader()
        
        if not self.bulk_loader_cmd:
            logger.error("‚ùå FalkorDB bulk loader not found!")
            logger.error("Install it with: pip install falkordb-bulk-loader")
            self.print_manual_instructions(csv_files, graph_name, redis_url)
            return False
        
        # Separate node and relationship files
        node_files = [f for f_type, f in csv_files if f_type == 'nodes']
        rel_files = [f for f_type, f in csv_files if f_type == 'relationships']
        
        if not node_files:
            logger.error("‚ùå No node files found - cannot proceed with bulk loading")
            return False
        
        # Build command using latest FalkorDB bulk loader format
        cmd_parts = self.bulk_loader_cmd.split()
        cmd_parts.append(graph_name)  # Graph name is positional
        
        # Add connection URL
        cmd_parts.extend(['--redis-url', redis_url])
        
        # Add schema flag if using schema format
        if self.use_schema:
            cmd_parts.append('--enforce-schema')
        
        # Add node files with -n flag
        for filename in node_files:
            cmd_parts.extend(['-n', filename])
        
        # Add relationship files with -r flag
        for filename in rel_files:
            cmd_parts.extend(['-r', filename])
        
        # Execute bulk loader
        logger.info(f"üöÄ Executing bulk loader:")
        logger.info(f"  Command: {' '.join(cmd_parts)}")
        logger.info(f"  Graph: {graph_name}")
        logger.info(f"  Redis URL: {redis_url}")
        logger.info(f"  Schema mode: {self.use_schema}")
        logger.info(f"  Files: {len(node_files)} node files, {len(rel_files)} relationship files")
        
        try:
            start_time = time.time()
            
            process = subprocess.run(
                cmd_parts,
                capture_output=True,
                text=True,
                timeout=1800,  # 30 minutes
                check=False  # Don't raise exception on non-zero exit
            )
            
            duration = time.time() - start_time
            
            # Process results
            if process.returncode == 0:
                logger.info(f"‚úÖ Bulk loading completed successfully in {duration:.1f}s!")
                
                if process.stdout:
                    logger.info("üìã Output:")
                    for line in process.stdout.strip().split('\n'):
                        if line.strip():
                            logger.info(f"  {line}")
                
                return True
            else:
                logger.error(f"‚ùå Bulk loading failed (exit code {process.returncode})")
                
                if process.stdout:
                    logger.error("STDOUT:")
                    for line in process.stdout.strip().split('\n'):
                        if line.strip():
                            logger.error(f"  {line}")
                
                if process.stderr:
                    logger.error("STDERR:")
                    for line in process.stderr.strip().split('\n'):
                        if line.strip():
                            logger.error(f"  {line}")
                
                self.diagnose_bulk_loader_failure(process, cmd_parts)
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("‚ùå Bulk loading timed out (>30 minutes)")
            self.print_manual_instructions(csv_files, graph_name, redis_url)
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Unexpected error running bulk loader: {e}")
            self.print_manual_instructions(csv_files, graph_name, redis_url)
            return False
    
    def diagnose_bulk_loader_failure(self, process, cmd_parts):
        """Provide diagnostic information for bulk loader failures"""
        logger.error("\nüîß Troubleshooting suggestions:")
        
        stderr_lower = process.stderr.lower() if process.stderr else ""
        stdout_lower = process.stdout.lower() if process.stdout else ""
        
        if "connection" in stderr_lower or "redis" in stderr_lower:
            logger.error("‚Ä¢ Connection issue - check if FalkorDB is running:")
            logger.error("  docker run -p 6379:6379 falkordb/falkordb:latest")
            logger.error("  redis-cli -h 127.0.0.1 -p 6379 ping")
        
        if "permission" in stderr_lower or "auth" in stderr_lower:
            logger.error("‚Ä¢ Permission/auth issue - check Redis configuration")
        
        if "format" in stderr_lower or "parse" in stderr_lower or "csv" in stderr_lower:
            logger.error("‚Ä¢ CSV format issue - check file encoding and structure")
            logger.error("  Try without --enforce-schema flag")
        
        if "memory" in stderr_lower or "token" in stderr_lower:
            logger.error("‚Ä¢ Memory issue - try reducing batch sizes:")
            logger.error("  Add --max-token-count 512 --max-buffer-size 32")
        
        if "already exists" in stderr_lower or "graph" in stderr_lower:
            logger.error("‚Ä¢ Graph already exists - choose different name or delete existing graph")
        
        logger.error("‚Ä¢ Test with simpler command:")
        logger.error(f"  falkordb-bulk-insert test_graph -n {cmd_parts[-2]}")
        
    def print_manual_instructions(self, csv_files: List[Tuple[str, str]], graph_name: str,
                                redis_url: str):
        """Print manual instructions"""
        logger.info("\n" + "="*70)
        logger.info("üìã MANUAL BULK LOADER INSTRUCTIONS")
        logger.info("="*70)
        
        # Build manual command using correct FalkorDB format
        cmd_parts = ['falkordb-bulk-insert', graph_name]
        cmd_parts.extend(['--redis-url', redis_url])
        
        if self.use_schema:
            cmd_parts.append('--enforce-schema')
        
        node_files = [f for f_type, f in csv_files if f_type == 'nodes']
        rel_files = [f for f_type, f in csv_files if f_type == 'relationships']
        
        for filename in node_files:
            cmd_parts.extend(['-n', filename])
        
        for filename in rel_files:
            cmd_parts.extend(['-r', filename])
        
        logger.info("1. Install bulk loader if needed:")
        logger.info("   pip install --upgrade falkordb-bulk-loader")
        logger.info("")
        logger.info("2. Ensure FalkorDB is running:")
        logger.info("   docker run -p 6379:6379 falkordb/falkordb:latest")
        logger.info("")
        logger.info("3. Run bulk loader manually:")
        logger.info(f"   {' '.join(cmd_parts)}")
        logger.info("")
        logger.info("Alternative commands to try:")
        logger.info(f"   python3 -m falkordb_bulk_loader {' '.join(cmd_parts[1:])}")
        logger.info("")
        logger.info("Created CSV files:")
        for file_type, filename in csv_files:
            logger.info(f"  {file_type}: {filename}")
        logger.info("")
        logger.info("Format used: Schema format" if self.use_schema else "Format used: Simple format")
        logger.info("="*70)

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Convert TTL to CSV and bulk load into FalkorDB')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--redis_url', default='redis://127.0.0.1:6379', help='Redis URL (default: redis://127.0.0.1:6379)')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--keep_csv', action='store_true', help='Keep CSV files after loading')
    parser.add_argument('--csv_only', action='store_true', help='Only convert to CSV, skip bulk loading')
    parser.add_argument('--no_schema', action='store_true', help='Use simple CSV format instead of schema format')
    
    args = parser.parse_args()
    
    # Validate input
    if not os.path.exists(args.ttl_file):
        logger.error(f"‚ùå TTL file not found: {args.ttl_file}")
        sys.exit(1)
    
    file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
    logger.info(f"üìÅ Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    
    # Create converter
    use_schema = not args.no_schema
    converter = TTLToCSVConverter(args.output_dir, use_schema=use_schema)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        csv_files = converter.convert_ttl_to_csv(args.ttl_file)
        conversion_time = time.time() - start_time
        
        logger.info(f"‚úÖ TTL‚ÜíCSV conversion: {conversion_time:.1f}s ({file_size_mb/conversion_time:.1f} MB/s)")
        
        if args.csv_only:
            logger.info("üéØ CSV-only mode: Skipping bulk loading")
            logger.info(f"üìÇ CSV files available in: {args.output_dir}")
            return
        
        # Run bulk loader
        success = converter.run_falkor_bulk_loader(
            csv_files, args.graph_name, args.redis_url
        )
        
        total_time = time.time() - start_time
        
        if success:
            logger.info(f"üöÄ PIPELINE SUCCESS! Total time: {total_time:.1f}s")
            logger.info(f"   Performance: {file_size_mb/total_time:.1f} MB/s")
            
            # Cleanup if requested
            if not args.keep_csv:
                try:
                    shutil.rmtree(args.output_dir)
                    logger.info(f"üßπ Cleaned up CSV files in {args.output_dir}")
                except Exception as e:
                    logger.warning(f"Could not cleanup {args.output_dir}: {e}")
        else:
            logger.error("‚ùå Pipeline failed during bulk loading")
            logger.info(f"üìÇ CSV files preserved in: {args.output_dir}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.error("‚ùå Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Pipeline failed: {e}")
        logger.exception("Full error details:")
        sys.exit(1)

if __name__ == "__main__":
    main()
