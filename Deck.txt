#!/usr/bin/env python3
"""
TTL File Path Configuration and Usage Guide
Complete setup for processing your TTL files with the reasoning system
"""

import os
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
import argparse
import sys
from dataclasses import dataclass

# Import the reasoning system (assuming it's in the same directory)
from advanced_reasoning_enhancements import GeneralPurposeLegalReasoningSystem

@dataclass
class ProcessingConfig:
    """Configuration for TTL file processing"""
    input_path: str
    output_path: str
    api_key: str
    base_url: str = "https://api.openai.com/v1"
    model: str = "o3-mini"
    batch_processing: bool = True
    create_subdirs: bool = True
    preserve_structure: bool = True
    max_concurrent: int = 3

class TTLFileProcessor:
    """Main processor for TTL files with flexible path configuration"""
    
    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.reasoning_system = GeneralPurposeLegalReasoningSystem(
            base_url=config.base_url,
            api_key=config.api_key,
            model=config.model
        )
        self.setup_paths()
    
    def setup_paths(self):
        """Setup input and output paths"""
        self.input_path = Path(self.config.input_path).resolve()
        self.output_path = Path(self.config.output_path).resolve()
        
        # Validate input path exists
        if not self.input_path.exists():
            raise FileNotFoundError(f"Input path does not exist: {self.input_path}")
        
        # Create output directory if it doesn't exist
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"✅ Input path: {self.input_path}")
        print(f"✅ Output path: {self.output_path}")
    
    async def process_single_file(self, ttl_file_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """Process a single TTL file"""
        
        file_path = Path(ttl_file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"TTL file not found: {file_path}")
        
        print(f"\n🔄 Processing: {file_path.name}")
        
        # Read the TTL content
        with open(file_path, 'r', encoding='utf-8') as f:
            ttl_content = f.read()
        
        # Process with reasoning system
        analysis_result = await self.reasoning_system.comprehensive_analysis(ttl_content)
        
        # Determine output directory
        if output_dir:
            output_directory = Path(output_dir)
        else:
            output_directory = self.output_path
            
        output_directory.mkdir(parents=True, exist_ok=True)
        
        # Generate output file names
        base_name = file_path.stem
        
        # Save enhanced TTL
        enhanced_ttl_file = output_directory / f"{base_name}_enhanced.ttl"
        with open(enhanced_ttl_file, 'w', encoding='utf-8') as f:
            f.write(analysis_result['content_analysis']['enhanced_content'])
        
        # Save original TTL (for reference)
        original_ttl_file = output_directory / f"{base_name}_original.ttl"
        with open(original_ttl_file, 'w', encoding='utf-8') as f:
            f.write(analysis_result['content_analysis']['original_content'])
        
        # Save comprehensive analysis report
        analysis_report_file = output_directory / f"{base_name}_analysis.json"
        with open(analysis_report_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, default=str)
        
        # Save preservation map
        preservation_file = output_directory / f"{base_name}_preservation.json"
        with open(preservation_file, 'w', encoding='utf-8') as f:
            json.dump({
                'preservation_map': analysis_result['content_analysis']['preservation_map'],
                'enhancement_log': analysis_result['content_analysis']['enhancement_log'],
                'information_preserved': analysis_result['input_preservation']['information_preserved'],
                'preservation_ratio': analysis_result['input_preservation']['preservation_ratio']
            }, f, indent=2)
        
        # Save executive summary
        summary_file = output_directory / f"{base_name}_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(self._generate_summary_report(analysis_result, file_path.name))
        
        print(f"✅ Completed: {file_path.name}")
        print(f"   📁 Output directory: {output_directory}")
        print(f"   📄 Files created: {base_name}_enhanced.ttl, {base_name}_analysis.json, {base_name}_summary.txt")
        
        return {
            'input_file': str(file_path),
            'output_directory': str(output_directory),
            'files_created': [
                str(enhanced_ttl_file),
                str(analysis_report_file),
                str(preservation_file),
                str(summary_file)
            ],
            'analysis_result': analysis_result
        }
    
    async def process_directory(self, preserve_directory_structure: bool = True) -> Dict[str, Any]:
        """Process all TTL files in the input directory"""
        
        if not self.input_path.is_dir():
            raise ValueError(f"Input path is not a directory: {self.input_path}")
        
        # Find all TTL files
        ttl_files = list(self.input_path.rglob("*.ttl"))
        
        if not ttl_files:
            raise ValueError(f"No TTL files found in: {self.input_path}")
        
        print(f"\n🔍 Found {len(ttl_files)} TTL files to process")
        
        results = {}
        processing_stats = {
            'total_files': len(ttl_files),
            'successful': 0,
            'failed': 0,
            'start_time': None,
            'end_time': None
        }
        
        processing_stats['start_time'] = asyncio.get_event_loop().time()
        
        for ttl_file in ttl_files:
            try:
                # Determine output subdirectory
                if preserve_directory_structure:
                    # Maintain the relative directory structure
                    relative_path = ttl_file.relative_to(self.input_path)
                    output_subdir = self.output_path / relative_path.parent
                else:
                    # Put everything in the root output directory
                    output_subdir = self.output_path
                
                # Process the file
                result = await self.process_single_file(str(ttl_file), str(output_subdir))
                results[str(ttl_file)] = result
                processing_stats['successful'] += 1
                
            except Exception as e:
                print(f"❌ Error processing {ttl_file.name}: {e}")
                results[str(ttl_file)] = {'error': str(e)}
                processing_stats['failed'] += 1
        
        processing_stats['end_time'] = asyncio.get_event_loop().time()
        processing_stats['total_duration'] = processing_stats['end_time'] - processing_stats['start_time']
        
        # Save batch processing summary
        batch_summary_file = self.output_path / "batch_processing_summary.json"
        with open(batch_summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'processing_stats': processing_stats,
                'results_summary': {
                    file_path: {'success': 'error' not in result, 'output_dir': result.get('output_directory')}
                    for file_path, result in results.items()
                },
                'config': {
                    'input_path': str(self.input_path),
                    'output_path': str(self.output_path),
                    'preserve_structure': preserve_directory_structure
                }
            }, f, indent=2, default=str)
        
        print(f"\n📊 Batch Processing Complete!")
        print(f"   ✅ Successful: {processing_stats['successful']}")
        print(f"   ❌ Failed: {processing_stats['failed']}")
        print(f"   ⏱️  Duration: {processing_stats['total_duration']:.2f} seconds")
        print(f"   📁 Summary saved: {batch_summary_file}")
        
        return {
            'processing_stats': processing_stats,
            'results': results,
            'batch_summary_file': str(batch_summary_file)
        }
    
    def _generate_summary_report(self, analysis_result: Dict[str, Any], filename: str) -> str:
        """Generate a human-readable summary report"""
        
        domain_info = analysis_result.get('domain_discovery', {}).get('domain_classification', {})
        overall_assessment = analysis_result.get('consolidated_insights', {}).get('overall_assessment', {})
        preservation_info = analysis_result.get('input_preservation', {})
        
        summary = f"""
TTL FILE ANALYSIS SUMMARY
========================

File: {filename}
Analysis ID: {analysis_result.get('analysis_id', 'N/A')}
Processing Date: {analysis_result.get('analysis_timestamp', 'N/A')}
Duration: {analysis_result.get('processing_duration_seconds', 0):.2f} seconds

INFORMATION PRESERVATION
-----------------------
✓ Information Preserved: {preservation_info.get('information_preserved', False)}
✓ Preservation Ratio: {preservation_info.get('preservation_ratio', 0.0):.1%}
✓ Original Size: {preservation_info.get('original_size', 0):,} characters
✓ Enhanced Size: {preservation_info.get('enhanced_size', 0):,} characters

DOMAIN DISCOVERY
---------------
✓ Detected Domain: {domain_info.get('primary_domain', 'Unknown')}
✓ Confidence: {domain_info.get('confidence', 0.0):.1%}
✓ Evidence: {', '.join(domain_info.get('evidence', [])[:3])}

QUALITY ASSESSMENT
-----------------
✓ Overall Quality: {overall_assessment.get('quality_score', 0.0):.1%}
✓ Consistency: {overall_assessment.get('consistency_score', 0.0):.1%}
✓ Completeness: {overall_assessment.get('completeness_score', 0.0):.1%}

KEY FINDINGS
-----------
"""
        
        for i, finding in enumerate(analysis_result.get('consolidated_insights', {}).get('key_findings', [])[:5], 1):
            summary += f"{i}. {finding.get('finding', 'N/A')}\n"
        
        summary += f"""
RECOMMENDATIONS
--------------
"""
        
        for i, rec in enumerate(analysis_result.get('consolidated_insights', {}).get('actionable_recommendations', [])[:5], 1):
            summary += f"{i}. {rec.get('recommendation', 'N/A')} (Priority: {rec.get('priority', 'N/A')})\n"
        
        summary += f"""
TECHNICAL DETAILS
----------------
✓ Original Triples: {analysis_result.get('content_analysis', {}).get('structural_analysis', {}).get('triple_patterns', []).__len__():,}
✓ Enhanced Triples: {preservation_info.get('enhanced_size', 0)}
✓ Reasoning Confidence: {analysis_result.get('consolidated_insights', {}).get('confidence_assessment', {}).get('overall_confidence', 0.0):.1%}

For detailed technical analysis, see the corresponding JSON files.
"""
        
        return summary

# Configuration helper functions
def load_config_from_file(config_file: str) -> ProcessingConfig:
    """Load configuration from JSON file"""
    with open(config_file, 'r') as f:
        config_data = json.load(f)
    
    return ProcessingConfig(**config_data)

def create_sample_config(config_file: str = "ttl_config.json"):
    """Create a sample configuration file"""
    sample_config = {
        "input_path": "./input_ttl_files",
        "output_path": "./output_analysis",
        "api_key": "your-openai-api-key-here",
        "base_url": "https://api.openai.com/v1",
        "model": "o3-mini",
        "batch_processing": True,
        "create_subdirs": True,
        "preserve_structure": True,
        "max_concurrent": 3
    }
    
    with open(config_file, 'w') as f:
        json.dump(sample_config, f, indent=2)
    
    print(f"Sample configuration created: {config_file}")
    print("Please edit this file with your specific paths and API key.")

# Command line interface
def main():
    """Main function with command line interface"""
    parser = argparse.ArgumentParser(description="Process TTL files with advanced reasoning")
    parser.add_argument('--input', '-i', type=str, help='Input TTL file or directory path')
    parser.add_argument('--output', '-o', type=str, help='Output directory path')
    parser.add_argument('--config', '-c', type=str, help='Configuration file path')
    parser.add_argument('--api-key', type=str, help='OpenAI API key')
    parser.add_argument('--model', type=str, default='o3-mini', help='Model to use (default: o3-mini)')
    parser.add_argument('--single', action='store_true', help='Process single file instead of directory')
    parser.add_argument('--create-config', action='store_true', help='Create sample configuration file')
    
    args = parser.parse_args()
    
    # Create sample configuration if requested
    if args.create_config:
        create_sample_config()
        return
    
    # Load configuration
    if args.config:
        config = load_config_from_file(args.config)
    else:
        # Create config from command line arguments
        if not args.input or not args.output or not args.api_key:
            print("Error: Either provide --config file or --input, --output, and --api-key")
            print("Use --create-config to generate a sample configuration file")
            sys.exit(1)
        
        config = ProcessingConfig(
            input_path=args.input,
            output_path=args.output,
            api_key=args.api_key,
            model=args.model
        )
    
    # Run the processor
    async def run_processor():
        processor = TTLFileProcessor(config)
        
        if args.single or Path(config.input_path).is_file():
            # Process single file
            result = await processor.process_single_file(config.input_path)
            print(f"\n🎉 Single file processing complete!")
        else:
            # Process directory
            result = await processor.process_directory(config.preserve_structure)
            print(f"\n🎉 Batch processing complete!")
    
    # Run the async function
    asyncio.run(run_processor())

# Example usage functions
async def example_single_file():
    """Example: Process a single TTL file"""
    config = ProcessingConfig(
        input_path="./my_legal_document.ttl",  # Your TTL file
        output_path="./analysis_results",       # Where to save results
        api_key="your-api-key-here"            # Your OpenAI API key
    )
    
    processor = TTLFileProcessor(config)
    result = await processor.process_single_file(config.input_path)
    print("Single file processing complete!")

async def example_batch_processing():
    """Example: Process all TTL files in a directory"""
    config = ProcessingConfig(
        input_path="./legal_documents",         # Directory with TTL files
        output_path="./batch_analysis_results", # Where to save all results
        api_key="your-api-key-here",           # Your OpenAI API key
        preserve_structure=True                 # Maintain subdirectory structure
    )
    
    processor = TTLFileProcessor(config)
    result = await processor.process_directory()
    print("Batch processing complete!")

async def example_custom_config():
    """Example: Using a configuration file"""
    # First create the config file
    create_sample_config("my_ttl_config.json")
    
    # Edit the config file with your paths and API key, then:
    config = load_config_from_file("my_ttl_config.json")
    
    processor = TTLFileProcessor(config)
    result = await processor.process_directory()
    print("Processing with custom config complete!")

if __name__ == "__main__":
    # Uncomment one of these examples or use command line interface
    
    # Command line interface
    main()
    
    # Or run examples directly:
    # asyncio.run(example_single_file())
    # asyncio.run(example_batch_processing())
    # asyncio.run(example_custom_config())
