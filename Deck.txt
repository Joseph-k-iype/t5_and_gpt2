#!/usr/bin/env python3
"""
FIXED: Complete Optimized Async Triple-Based RDF to FalkorDB Property Graph Converter

This module converts RDF data to FalkorDB property graphs with optimized relationship creation
for handling large datasets efficiently.

FIXES APPLIED:
- Fixed SPARQL connection initialization (main issue)
- Removed TIMEOUT syntax from Cypher queries (FalkorDB doesn't support it)
- Fixed property setting syntax for better FalkorDB compatibility
- Simplified label creation and property assignment
- Added proper error handling for FalkorDB-specific constraints
- Fixed connection management and retry logic

Key Optimizations:
- Ultra-fast bulk relationship creation using UNWIND
- Strategic indexing for optimal lookup performance
- Grouped batch processing by relationship type
- Parallel processing with controlled concurrency
- Memory-efficient streaming approach

Dependencies:
    pip install rdflib falkordb redis asyncio
"""

import logging
import json
import time
import asyncio
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD

# Async imports
import falkordb.asyncio as falkordb_async
import redis.asyncio as redis_async
from redis.asyncio import BlockingConnectionPool

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Utility function to avoid code duplication
def clean_label_name(label: str) -> str:
    """Clean label name to ensure valid Cypher identifier"""
    if not label:
        return 'Resource'  # Default fallback
    
    # Remove invalid characters and ensure valid Cypher identifier
    clean_label = re.sub(r'[^a-zA-Z0-9_]', '_', label)
    
    # Ensure doesn't start with number
    if clean_label and clean_label[0].isdigit():
        clean_label = f"_{clean_label}"
    
    # Ensure not empty and not too long
    if not clean_label:
        clean_label = 'Resource'
    elif len(clean_label) > 50:  # Limit length
        clean_label = clean_label[:50]
    
    return clean_label

@dataclass
class OptimizedAsyncTripleConfig:
    """Optimized configuration for async triple-based RDF conversion with fast edge loading"""
    # The main SPARQL query with 6 variables
    triples_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Graph management settings
    append_to_existing_graph: bool = True      # Append to existing graph instead of clearing
    clear_existing_graph: bool = False         # Set to True to clear graph before loading
    handle_duplicates: bool = True             # Use MERGE instead of CREATE to handle duplicates
    skip_existing_nodes: bool = True           # Skip nodes that already exist
    skip_existing_relationships: bool = True   # Skip relationships that already exist
    
    # Optimized async processing settings for large datasets
    batch_size: int = 2000              # Larger batches for relationships
    max_concurrent_batches: int = 3     # Controlled concurrency
    connection_pool_size: int = 10      # Moderate pool size for stability
    sparql_timeout: int = 7200          # 2 hours for very large queries
    falkordb_timeout: Optional[int] = 300  # 5 minutes timeout for FalkorDB operations
    max_retries: int = 5                # More retries for large operations
    retry_delay: int = 3                # Delay between retries
    
    # Relationship optimization settings
    preserve_uri_properties: bool = False   # Disable for maximum speed
    disable_relationship_properties: bool = True  # Skip rel properties for speed
    group_relationships_by_type: bool = True      # Group for batch efficiency
    use_bulk_relationship_creation: bool = True   # Use UNWIND for bulk operations
    
    # Performance settings
    use_shortened_uris: bool = True
    create_indexes: bool = True
    default_node_label: str = 'Resource'  # Must be valid Cypher identifier
    
    # Memory and performance optimizations
    exclude_rdf_type_properties: bool = False  # Keep RDF types to preserve structure
    validate_conversion: bool = False      # Skip validation for speed
    export_stats: bool = True
    progress_update_interval: int = 50     # Log progress every N batches

@dataclass
class AsyncConversionStats:
    """Statistics tracking for async conversion process with incremental loading support"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution
    query_execution_time: float = 0.0
    total_triples_retrieved: int = 0
    
    # Processing stats
    processed_triples: int = 0
    property_triples: int = 0
    relationship_triples: int = 0
    
    # Batch processing stats
    total_batches: int = 0
    completed_batches: int = 0
    failed_batches: int = 0
    
    # Graph state tracking (for incremental loading)
    initial_nodes: int = 0              # Nodes in graph before this run
    initial_relationships: int = 0      # Relationships in graph before this run
    
    # Created entities (new in this run)
    unique_subjects: int = 0
    unique_objects: int = 0
    created_nodes: int = 0              # New nodes created
    created_relationships: int = 0      # New relationships created
    skipped_nodes: int = 0              # Existing nodes skipped
    skipped_relationships: int = 0      # Existing relationships skipped
    
    # Final totals (after this run)
    final_nodes: int = 0
    final_relationships: int = 0
    
    # Performance metrics
    relationship_creation_time: float = 0.0
    node_creation_time: float = 0.0
    relationship_creation_rate: float = 0.0
    
    # Discovered metadata
    subject_classes: Set[str] = None
    object_classes: Set[str] = None
    predicates_used: Set[str] = None
    relationship_types_count: Dict[str, int] = None
    
    # Incremental loading info
    append_mode: bool = False
    graph_was_cleared: bool = False
    
    # Errors
    processing_errors: int = 0
    
    def __post_init__(self):
        if self.subject_classes is None:
            self.subject_classes = set()
        if self.object_classes is None:
            self.object_classes = set()
        if self.predicates_used is None:
            self.predicates_used = set()
        if self.relationship_types_count is None:
            self.relationship_types_count = {}
    
    def get_incremental_summary(self) -> str:
        """Get a summary of incremental changes"""
        if self.append_mode:
            return (f"Added {self.created_nodes:,} nodes and {self.created_relationships:,} relationships. "
                   f"Graph now has {self.final_nodes:,} nodes and {self.final_relationships:,} relationships total.")
        else:
            return (f"Created {self.created_nodes:,} nodes and {self.created_relationships:,} relationships "
                   f"in new graph.")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        
        # Convert sets to lists for JSON serialization
        result['subject_classes'] = list(self.subject_classes)
        result['object_classes'] = list(self.object_classes) 
        result['predicates_used'] = list(self.predicates_used)
        result['incremental_summary'] = self.get_incremental_summary()
        
        return result

class URIProcessor:
    """Handles URI processing and identifier creation"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos',
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return self._clean_identifier(uri_str)
        
        # Check cache first
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        # Try namespace mapping
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        # Fallback: extract from URI structure
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        clean_name = clean_name.strip('_')
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)

class AsyncNodeManager:
    """Manages node creation and properties with async operations"""
    
    def __init__(self, uri_processor: URIProcessor, config: OptimizedAsyncTripleConfig):
        self.uri_processor = uri_processor
        self.config = config
        self.nodes: Dict[str, Dict[str, Any]] = {}
        self._lock = asyncio.Lock()
    
    async def ensure_node_exists(self, uri: str, class_uri: Optional[str] = None):
        """Ensure a node exists with appropriate labeling"""
        async with self._lock:
            if uri not in self.nodes:
                await self._create_node(uri, class_uri)
    
    async def _create_node(self, uri: str, class_uri: Optional[str] = None):
        """Create a new node entry with proper RDF class as primary label"""
        labels = set()
        primary_label = None
        
        # Use the actual RDF class as the primary label
        if class_uri:
            class_label = self.uri_processor.process_uri(class_uri)
            clean_class_label = clean_label_name(class_label)
            labels.add(clean_class_label)
            primary_label = clean_class_label
        else:
            # Only fall back to default if no class is specified
            primary_label = clean_label_name(self.config.default_node_label)
            labels.add(primary_label)
        
        # Handle blank nodes - add as additional label
        if uri.startswith('_:'):
            labels.add('BlankNode')
        
        node_data = {
            'labels': labels,
            'primary_label': primary_label,  # Store the main label to use in queries
            'properties': {}
        }
        
        # Always add URI for lookup purposes
        node_data['properties']['uri'] = uri
        
        # Add processed identifier for easier querying
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        # Add original class URI if preserving URIs
        if self.config.preserve_uri_properties and class_uri:
            node_data['properties']['rdf_type'] = class_uri
        
        self.nodes[uri] = node_data
    
    async def add_property(self, subject_uri: str, predicate_uri: str, literal_value: Any):
        """Add a property to a node"""
        async with self._lock:
            if subject_uri not in self.nodes:
                logger.warning(f"Node {subject_uri} not found when adding property")
                return
            
            # Skip rdf:type if configured to do so
            if (self.config.exclude_rdf_type_properties and 
                predicate_uri == str(RDF.type)):
                return
            
            prop_name = self.uri_processor.process_uri(predicate_uri)
            
            # Handle multiple values for the same property
            properties = self.nodes[subject_uri]['properties']
            if prop_name in properties:
                existing = properties[prop_name]
                if isinstance(existing, list):
                    existing.append(literal_value)
                else:
                    properties[prop_name] = [existing, literal_value]
            else:
                properties[prop_name] = literal_value
    
    async def get_nodes(self) -> Dict[str, Dict[str, Any]]:
        """Get all nodes"""
        async with self._lock:
            return self.nodes.copy()
    
    async def clear(self):
        """Clear all nodes"""
        async with self._lock:
            self.nodes.clear()

class OptimizedAsyncFalkorDBManager:
    """FIXED: Optimized FalkorDB manager with corrected syntax for FalkorDB compatibility"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.pool = None
        self.db = None
        self.graph = None
    
    async def connect(self):
        """Establish async connection to FalkorDB and configure for optimal performance"""
        try:
            # Validate default label (without modifying config)
            clean_default_label = clean_label_name(self.config.default_node_label)
            if clean_default_label != self.config.default_node_label:
                logger.warning(f"Default label '{self.config.default_node_label}' will be cleaned to '{clean_default_label}' for FalkorDB compatibility")
            
            # Create Redis connection pool with optimized settings
            pool_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port,
                'max_connections': self.config.connection_pool_size,
                'decode_responses': True,
                'retry_on_timeout': True,
                'health_check_interval': 30,
                'socket_connect_timeout': 30,
            }
            
            # Add timeout only if specified
            if self.config.falkordb_timeout:
                pool_kwargs['timeout'] = self.config.falkordb_timeout
            
            if self.config.falkordb_password:
                pool_kwargs['password'] = self.config.falkordb_password
            
            self.pool = BlockingConnectionPool(**pool_kwargs)
            
            # Test the connection
            test_conn = redis_async.Redis(connection_pool=self.pool)
            await test_conn.ping()
            await test_conn.aclose()
            
            self.db = falkordb_async.FalkorDB(connection_pool=self.pool)
            self.graph = self.db.select_graph(self.config.graph_name)
            
            logger.info(f"✅ FalkorDB connection established to graph '{self.config.graph_name}'")
            logger.info(f"✅ Using default node label: '{clean_default_label}'")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    async def clear_graph(self):
        """Clear existing graph data (only if explicitly requested)"""
        try:
            await self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("🗑️  Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    async def get_initial_graph_stats(self) -> Dict[str, int]:
        """Get initial graph statistics before processing"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            if node_count > 0 or rel_count > 0:
                logger.info(f"📊 Existing graph: {node_count:,} nodes, {rel_count:,} relationships")
            else:
                logger.info("📊 Starting with empty graph")
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting initial statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def execute_query_with_retry(self, query: str, params: Optional[Dict] = None) -> Any:
        """FIXED: Execute a Cypher query with async retry logic (removed TIMEOUT syntax)"""
        for attempt in range(self.config.max_retries):
            try:
                # Execute query without adding TIMEOUT to the Cypher query
                # FalkorDB handles timeouts at the command level, not in Cypher
                return await self.graph.query(query, params or {})
            except Exception as e:
                if attempt == self.config.max_retries - 1:
                    logger.error(f"Query failed after {self.config.max_retries} attempts: {e}")
                    raise
                
                await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
    
    async def create_node_batch(self, nodes_batch: List[Tuple[str, Dict[str, Any]]]) -> int:
        """FIXED: Create nodes with their actual RDF class labels, not generic 'Resource'"""
        if not nodes_batch:
            return 0
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        try:
            # Group nodes by their primary label for efficient batch creation
            nodes_by_label = defaultdict(list)
            
            for uri, node_data in nodes_batch:
                # Simplify properties to basic types only
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                # Use the actual primary label from RDF class
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                nodes_by_label[primary_label].append({
                    'uri': uri,
                    'properties': properties
                })
            
            total_created = 0
            
            # Create nodes grouped by label for better performance
            for label, nodes_data in nodes_by_label.items():
                logger.info(f"Creating {len(nodes_data)} nodes with label '{label}'")
                
                if use_merge:
                    # Use MERGE with the actual class label
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    MERGE (n:{label} {{uri: node_data.uri}})
                    SET n += node_data.properties
                    RETURN count(n) as total_processed
                    """
                else:
                    # Use CREATE with the actual class label
                    query = f"""
                    UNWIND $nodes_data AS node_data
                    CREATE (n:{label})
                    SET n += node_data.properties
                    RETURN count(n) as total_created
                    """
                
                result = await self.execute_query_with_retry(query, {'nodes_data': nodes_data})
                batch_created = result.result_set[0][0] if result.result_set else len(nodes_data)
                total_created += batch_created
                
                logger.info(f"✅ Created {batch_created} nodes with label '{label}'")
            
            return total_created
                
        except Exception as e:
            logger.error(f"Batch node creation failed: {e}")
            if "expected label" in str(e).lower():
                logger.error(f"❌ Label syntax error in batch creation. Check node labels")
            # Fallback to individual creation
            return await self._create_nodes_individual_fallback(nodes_batch, use_merge)
    
    async def _create_nodes_individual_fallback(self, nodes_batch: List[Tuple[str, Dict[str, Any]]], use_merge: bool = False) -> int:
        """FIXED: Fallback to create nodes individually with their actual RDF class labels"""
        created_count = 0
        
        for uri, node_data in nodes_batch:
            try:
                # Simplify properties
                properties = {}
                for key, value in node_data['properties'].items():
                    if isinstance(value, (str, int, float, bool)):
                        properties[key] = value
                    else:
                        properties[key] = str(value)
                
                # Use the actual primary label from RDF class
                primary_label = node_data.get('primary_label', clean_label_name(self.config.default_node_label))
                
                if use_merge:
                    query = f"""
                    MERGE (n:{primary_label} {{uri: $uri}})
                    SET n += $properties
                    """
                else:
                    query = f"""
                    CREATE (n:{primary_label})
                    SET n += $properties
                    """
                
                await self.execute_query_with_retry(query, {
                    'uri': uri,
                    'properties': properties
                })
                created_count += 1
                
            except Exception as e:
                logger.warning(f"Failed to create individual node {uri}: {e}")
                if "expected label" in str(e).lower():
                    logger.error(f"❌ Label syntax error for node {uri}. Label: '{primary_label}'. Available labels: {list(node_data.get('labels', []))}")
                continue
        
        return created_count
    
    async def create_relationships_ultra_fast(self, relationships_by_type: Dict[str, List[Tuple[str, str, str]]]) -> int:
        """Ultra-fast relationship creation using optimized bulk operations"""
        if not relationships_by_type:
            return 0
        
        total_relationships = sum(len(rels) for rels in relationships_by_type.values())
        logger.info(f"Creating {total_relationships} relationships using ultra-fast bulk approach...")
        
        start_time = time.time()
        total_created = 0
        
        # Process each relationship type in parallel with controlled concurrency
        semaphore = asyncio.Semaphore(3)  # Limit concurrent operations
        
        async def create_relationships_for_type(rel_type: str, relationships: List[Tuple[str, str, str]]):
            async with semaphore:
                return await self._create_relationships_bulk_by_type(rel_type, relationships)
        
        # Create tasks for all relationship types
        tasks = []
        for rel_type, relationships in relationships_by_type.items():
            task = asyncio.create_task(create_relationships_for_type(rel_type, relationships))
            tasks.append(task)
        
        # Execute all tasks
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        for i, result in enumerate(results):
            if isinstance(result, int):
                total_created += result
            else:
                rel_type = list(relationships_by_type.keys())[i]
                logger.error(f"Failed to create relationships for type {rel_type}: {result}")
        
        duration = time.time() - start_time
        rate = total_created / duration if duration > 0 else 0
        
        logger.info(f"Ultra-fast relationship creation completed:")
        logger.info(f"  Created: {total_created:,} relationships")
        logger.info(f"  Time: {duration:.2f} seconds")
        logger.info(f"  Rate: {rate:.1f} relationships/second")
        
        return total_created
    
    async def _create_relationships_bulk_by_type(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Create all relationships of a specific type using optimized batching"""
        if not relationships:
            return 0
        
        logger.info(f"Creating {len(relationships):,} relationships of type '{rel_type}'")
        
        # Adaptive batch sizing
        if len(relationships) > 10000:
            batch_size = 5000
        elif len(relationships) > 1000:
            batch_size = 2000
        else:
            batch_size = len(relationships)  # Process all at once for small sets
        
        total_created = 0
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            
            try:
                created = await self._execute_relationship_batch_query(rel_type, batch)
                total_created += created
                
                # Progress logging for large relationship types
                if len(relationships) > 1000 and (i // batch_size) % 25 == 0:
                    progress = (i + len(batch)) / len(relationships) * 100
                    logger.info(f"  Progress {rel_type}: {progress:.1f}% ({i + len(batch):,}/{len(relationships):,})")
                    
            except Exception as e:
                logger.error(f"Failed to create relationship batch for {rel_type}: {e}")
                # Try smaller batches
                created = await self._create_relationships_smaller_batches(rel_type, batch)
                total_created += created
        
        logger.info(f"Completed {rel_type}: {total_created:,} relationships created")
        return total_created
    
    async def _execute_relationship_batch_query(self, rel_type: str, batch: List[Tuple[str, str, str]]) -> int:
        """FIXED: Execute optimized batch relationship creation query with proper syntax"""
        batch_data = []
        for subject_uri, predicate_uri, object_uri in batch:
            rel_data = {
                'subject_uri': subject_uri,
                'object_uri': object_uri
            }
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                rel_data['predicate_uri'] = predicate_uri
            batch_data.append(rel_data)
        
        # Clean relationship type for Cypher
        clean_rel_type = self._clean_relationship_type(rel_type)
        
        use_merge = self.config.handle_duplicates and self.config.append_to_existing_graph
        
        if use_merge:
            # Use MERGE to handle existing relationships
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                MERGE (s)-[:{clean_rel_type}]->(o)
                """
        else:
            # Use CREATE for new graphs (faster)
            if not self.config.disable_relationship_properties and self.config.preserve_uri_properties:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[r:{clean_rel_type}]->(o)
                SET r.predicate_uri = rel.predicate_uri
                """
            else:
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
        
        await self.execute_query_with_retry(query, {'batch_data': batch_data})
        return len(batch)
    
    def _clean_relationship_type(self, rel_type: str) -> str:
        """Clean relationship type to ensure valid Cypher identifier"""
        # Remove invalid characters and ensure valid Cypher identifier
        clean_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        # Ensure doesn't start with number
        if clean_type and clean_type[0].isdigit():
            clean_type = f"_{clean_type}"
        
        # Ensure not empty and not too long
        if not clean_type:
            clean_type = 'RELATED_TO'
        elif len(clean_type) > 50:  # Limit length
            clean_type = clean_type[:50]
        
        return clean_type
    
    async def _create_relationships_smaller_batches(self, rel_type: str, relationships: List[Tuple[str, str, str]]) -> int:
        """Fallback: create relationships in much smaller batches"""
        batch_size = 100  # Very small batches for problematic data
        total_created = 0
        clean_rel_type = self._clean_relationship_type(rel_type)
        
        for i in range(0, len(relationships), batch_size):
            batch = relationships[i:i+batch_size]
            try:
                # Use the main batch creation method
                batch_data = []
                for subject_uri, predicate_uri, object_uri in batch:
                    batch_data.append({
                        'subject_uri': subject_uri,
                        'object_uri': object_uri
                    })
                
                # Simple batch query
                query = f"""
                UNWIND $batch_data AS rel
                MATCH (s {{uri: rel.subject_uri}})
                MATCH (o {{uri: rel.object_uri}})
                CREATE (s)-[:{clean_rel_type}]->(o)
                """
                
                await self.execute_query_with_retry(query, {'batch_data': batch_data})
                total_created += len(batch)
                
            except Exception as e:
                logger.warning(f"Small batch failed for {rel_type}: {e}")
                # Try individual creation as last resort
                for subject_uri, predicate_uri, object_uri in batch:
                    try:
                        # Simple individual query
                        query = """
                        MATCH (s {uri: $subject_uri})
                        MATCH (o {uri: $object_uri})
                        CREATE (s)-[:RELATED_TO]->(o)
                        """
                        await self.execute_query_with_retry(query, {
                            'subject_uri': subject_uri,
                            'object_uri': object_uri
                        })
                        total_created += 1
                    except Exception as individual_error:
                        logger.debug(f"Failed to create individual relationship: {individual_error}")
                        continue  # Skip problematic relationships
        
        return total_created
    
    async def create_indexes(self, discovered_labels: Dict[str, int] = None):
        """FIXED: Create indexes for discovered RDF class labels (passed as parameter)"""
        if not self.config.create_indexes:
            return
        
        logger.info("Creating performance indexes for discovered RDF classes...")
        
        # Get clean default label for fallback
        clean_default_label = clean_label_name(self.config.default_node_label)
        
        if not discovered_labels:
            logger.warning("No discovered labels provided - creating fallback index only")
            try:
                query = f"CREATE INDEX FOR (n:{clean_default_label}) ON (n.uri)"
                await self.execute_query_with_retry(query)
                logger.info(f"✅ Created fallback index for label '{clean_default_label}'")
            except Exception as e:
                logger.warning(f"Could not create fallback index: {e}")
            return
        
        # Create indexes for the most common labels (top 10)
        top_labels = sorted(discovered_labels.items(), key=lambda x: x[1], reverse=True)[:10]
        logger.info(f"Creating indexes for top {len(top_labels)} RDF classes:")
        
        indexed_labels = set()
        
        for label, count in top_labels:
            logger.info(f"  - {label}: {count:,} nodes")
            try:
                query = f"CREATE INDEX FOR (n:{label}) ON (n.uri)"
                await self.execute_query_with_retry(query)
                logger.info(f"✅ Created index for label '{label}'")
                indexed_labels.add(label)
            except Exception as e:
                logger.warning(f"Could not create index for label '{label}': {e}")
                if "expected label" in str(e).lower():
                    logger.error(f"❌ Label syntax error for label: '{label}'")
                # Index creation failures are not critical, continue
        
        # Create fallback index only if it wasn't already created
        if clean_default_label not in indexed_labels:
            try:
                query = f"CREATE INDEX FOR (n:{clean_default_label}) ON (n.uri)"
                await self.execute_query_with_retry(query)
                logger.info(f"✅ Created fallback index for label '{clean_default_label}'")
            except Exception as e:
                logger.warning(f"Could not create fallback index: {e}")
    
    async def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = await self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = await self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}
    
    async def monitor_memory_usage(self) -> Dict[str, Any]:
        """Monitor FalkorDB memory usage"""
        try:
            redis_client = redis_async.Redis(connection_pool=self.pool)
            info = await redis_client.info('memory')
            await redis_client.aclose()
            
            return {
                'used_memory': info.get('used_memory', 0),
                'used_memory_human': info.get('used_memory_human', '0B'),
                'used_memory_peak': info.get('used_memory_peak', 0),
                'used_memory_peak_human': info.get('used_memory_peak_human', '0B'),
                'memory_fragmentation_ratio': info.get('mem_fragmentation_ratio', 0),
            }
        except Exception as e:
            logger.warning(f"Could not get memory stats: {e}")
            return {}
    
    async def close(self):
        """Close async connections"""
        try:
            if self.pool:
                await self.pool.aclose()
                logger.info("Closed FalkorDB connection pool")
        except Exception as e:
            logger.warning(f"Error closing FalkorDB connection pool: {e}")

class OptimizedAsyncTripleBasedConverter:
    """Main async converter with ultra-fast relationship creation"""
    
    def __init__(self, config: OptimizedAsyncTripleConfig):
        self.config = config
        self.stats = AsyncConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.node_manager = AsyncNodeManager(self.uri_processor, config)
        self.falkordb_manager = OptimizedAsyncFalkorDBManager(config)
        self.rdf_graph = None
        
        # Optimized relationship storage
        self.relationships_by_type: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        self.total_relationships = 0
        self.semaphore = asyncio.Semaphore(config.max_concurrent_batches)
        
        # Don't setup RDF connection in init - do it fresh for each conversion
        self.rdf_graph = None
    
    def _reset_conversion_state(self):
        """Reset converter state for fresh conversion (important for reusing converter)"""
        logger.info("🔄 Resetting converter state for fresh conversion...")
        
        # Reset statistics
        self.stats = AsyncConversionStats(start_time=datetime.now())
        
        # Clear relationship storage
        self.relationships_by_type.clear()
        self.total_relationships = 0
        
        logger.info("✅ Converter state reset complete")
    
    def _setup_fresh_rdf_connection(self):
        """FIXED: Setup a fresh RDF graph connection to SPARQL endpoint"""
        try:
            # Close existing connection if any
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                try:
                    if hasattr(self.rdf_graph.store, 'close'):
                        self.rdf_graph.store.close()
                except:
                    pass  # Ignore close errors
            
            # Setup authentication if provided
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            # Create a fresh store for each conversion
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.sparql_timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"✅ Fresh SPARQL connection established to: {self.config.sparql_endpoint}")
            
            # Test the connection with a simple query
            test_query = "SELECT (COUNT(*) as ?count) WHERE { ?s ?p ?o } LIMIT 1"
            try:
                list(self.rdf_graph.query(test_query))
                logger.info("✅ SPARQL connection test successful")
            except Exception as test_error:
                logger.warning(f"⚠️  SPARQL connection test failed: {test_error}")
                # Don't fail completely, might still work
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def _cleanup_rdf_connection(self):
        """Clean up RDF connection resources"""
        try:
            if hasattr(self, 'rdf_graph') and self.rdf_graph is not None:
                if hasattr(self.rdf_graph.store, 'close'):
                    self.rdf_graph.store.close()
                    logger.info("🧹 Cleaned up SPARQL connection")
                self.rdf_graph = None
        except Exception as e:
            logger.warning(f"Error cleaning up SPARQL connection: {e}")
    
    def _execute_sparql_query_with_retry(self):
        """FIXED: Execute SPARQL query with connection retry logic"""
        max_attempts = 3
        
        for attempt in range(max_attempts):
            try:
                # FIXED: Always setup connection (including first attempt)
                if attempt == 0:
                    logger.info("Setting up initial SPARQL connection...")
                else:
                    logger.info(f"SPARQL query attempt {attempt + 1}/{max_attempts} - setting up fresh connection...")
                
                # Create fresh connection for each attempt
                self._setup_fresh_rdf_connection()
                
                # Verify connection was established
                if self.rdf_graph is None:
                    raise Exception("Failed to establish SPARQL connection - rdf_graph is None")
                
                logger.info(f"Executing SPARQL query with timeout {self.config.sparql_timeout}s")
                logger.info(f"Query preview: {self.config.triples_query[:200]}...")
                
                start_time = time.time()
                query_result = self.rdf_graph.query(self.config.triples_query)
                results = list(query_result)
                execution_time = time.time() - start_time
                
                logger.info(f"✅ SPARQL query completed in {execution_time:.2f}s, retrieved {len(results)} triples")
                return results
                
            except Exception as e:
                logger.error(f"SPARQL query attempt {attempt + 1} failed: {e}")
                
                if attempt < max_attempts - 1:
                    # Clean up and wait before retry
                    self._cleanup_rdf_connection()
                    retry_wait = 5 * (attempt + 1)  # Exponential backoff
                    logger.info(f"Retrying in {retry_wait} seconds...")
                    time.sleep(retry_wait)
                else:
                    # Final attempt failed
                    logger.error(f"❌ SPARQL query failed after {max_attempts} attempts")
                    raise
        
        raise Exception("SPARQL query failed - should not reach here")
    
    async def convert(self) -> AsyncConversionStats:
        """Main async conversion method with incremental loading support"""
        try:
            # Reset state for fresh conversion
            self._reset_conversion_state()
            
            # Set append mode in stats
            self.stats.append_mode = self.config.append_to_existing_graph
            
            if self.config.append_to_existing_graph:
                logger.info("📈 Starting incremental RDF to FalkorDB conversion (append mode)...")
            else:
                logger.info("🚀 Starting fresh RDF to FalkorDB conversion...")
            
            # Connect to FalkorDB
            await self.falkordb_manager.connect()
            
            # Get initial graph statistics
            initial_stats = await self.falkordb_manager.get_initial_graph_stats()
            self.stats.initial_nodes = initial_stats['nodes']
            self.stats.initial_relationships = initial_stats['relationships']
            
            # Handle graph clearing logic
            if self.config.clear_existing_graph:
                logger.info("🗑️  Clearing existing graph as requested...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            elif not self.config.append_to_existing_graph:
                # If not in append mode and not explicitly keeping graph, clear it
                logger.info("🗑️  Clearing graph for fresh conversion...")
                await self.falkordb_manager.clear_graph()
                self.stats.graph_was_cleared = True
                self.stats.initial_nodes = 0
                self.stats.initial_relationships = 0
            
            # Clear node manager for fresh start
            await self.node_manager.clear()
            
            # Execute the main query and process results
            await self._execute_and_process_query()
            
            # Create nodes first (fast)
            node_start = time.time()
            await self._create_nodes_in_falkordb_async()
            self.stats.node_creation_time = time.time() - node_start
            
            # Create relationships using ultra-fast approach
            rel_start = time.time()
            await self._create_relationships_ultra_fast()
            self.stats.relationship_creation_time = time.time() - rel_start
            
            # Calculate relationship creation rate
            if self.stats.relationship_creation_time > 0:
                self.stats.relationship_creation_rate = self.stats.created_relationships / self.stats.relationship_creation_time
            
            # Create indexes for discovered labels
            await self._create_indexes_with_discovered_labels()
            
            # Validate if requested
            if self.config.validate_conversion:
                await self._validate_conversion()
            
            # Finalize statistics
            await self._finalize_stats()
            
            # Log performance summary
            await self._log_performance_summary()
            
            if self.config.append_to_existing_graph:
                logger.info("✅ Incremental conversion completed successfully!")
                logger.info(f"📊 {self.stats.get_incremental_summary()}")
            else:
                logger.info("✅ Fresh conversion completed successfully!")
            
            return self.stats
            
        except Exception as e:
            logger.error(f"❌ Conversion failed: {e}")
            raise
        finally:
            # Clean up connections
            self._cleanup_rdf_connection()
            await self.falkordb_manager.close()
    
    async def _execute_and_process_query(self):
        """Execute the main SPARQL query and process results with retry logic"""
        start_time = time.time()
        
        try:
            logger.info("📊 Executing triples query with fresh connection...")
            
            # Execute query in executor to avoid blocking event loop, with retry logic
            loop = asyncio.get_running_loop()
            results = await loop.run_in_executor(None, self._execute_sparql_query_with_retry)
            
            self.stats.query_execution_time = time.time() - start_time
            self.stats.total_triples_retrieved = len(results)
            
            logger.info(f"📈 Retrieved {len(results):,} triples in {self.stats.query_execution_time:.2f}s")
            logger.info("⚡ Processing triples in optimized async batches...")
            
            # Process in async batches
            await self._process_triples_in_batches(results)
            
        except Exception as e:
            logger.error(f"Error executing/processing query: {e}")
            raise
        finally:
            # Clean up SPARQL connection after query execution
            self._cleanup_rdf_connection()
    
    async def _process_triples_in_batches(self, results: List[Tuple]):
        """Process triples in async batches with concurrency control"""
        batch_size = self.config.batch_size
        total_batches = (len(results) + batch_size - 1) // batch_size
        self.stats.total_batches = total_batches
        
        logger.info(f"🔄 Processing {len(results):,} triples in {total_batches} batches of {batch_size}")
        
        # Create batches
        batches = []
        for i in range(0, len(results), batch_size):
            batch = results[i:i+batch_size]
            batches.append(batch)
        
        # Process batches concurrently
        tasks = []
        for i, batch in enumerate(batches):
            task = asyncio.create_task(
                self._process_single_batch_async(batch, i + 1, total_batches)
            )
            tasks.append(task)
        
        # Process results as they complete
        completed = 0
        failed = 0
        
        for future in asyncio.as_completed(tasks):
            try:
                await future
                completed += 1
                self.stats.completed_batches = completed
                
                if completed % self.config.progress_update_interval == 0:
                    progress = (completed / total_batches) * 100
                    logger.info(f"📊 Batch progress: {progress:.1f}% ({completed}/{total_batches})")
            except Exception as e:
                failed += 1
                self.stats.failed_batches = failed
                logger.error(f"Batch processing failed: {e}")
        
        logger.info(f"✅ Batch processing completed: {completed} successful, {failed} failed")
    
    async def _process_single_batch_async(self, batch: List[Tuple], batch_num: int, total_batches: int):
        """Process a single batch of triples with semaphore control"""
        async with self.semaphore:
            try:
                for triple in batch:
                    if len(triple) >= 6:
                        subject, subject_class, predicate, predicate_class, obj, object_class = triple[:6]
                        await self._process_single_triple_async(subject, subject_class, predicate, predicate_class, obj, object_class)
                        self.stats.processed_triples += 1
                    else:
                        logger.warning(f"Invalid triple format in batch {batch_num}: {triple}")
                        
            except Exception as e:
                logger.error(f"Error processing batch {batch_num}: {e}")
                self.stats.processing_errors += 1
                raise
    
    async def _process_single_triple_async(self, subject, subject_class, predicate, predicate_class, obj, object_class):
        """Process a single 6-variable triple asynchronously with relationship optimization"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        # Track metadata
        self.stats.predicates_used.add(predicate_uri)
        if subject_class:
            self.stats.subject_classes.add(str(subject_class))
        
        # Ensure subject node exists
        await self.node_manager.ensure_node_exists(subject_uri, str(subject_class) if subject_class else None)
        
        if isinstance(obj, Literal):
            # Object is literal -> add as property
            literal_value = self._convert_literal_value(obj)
            await self.node_manager.add_property(subject_uri, predicate_uri, literal_value)
            self.stats.property_triples += 1
            
        elif isinstance(obj, (URIRef, BNode)):
            # Object is URI/BNode -> prepare relationship for optimized batch creation
            object_uri = str(obj)
            
            # Track object class
            if object_class:
                self.stats.object_classes.add(str(object_class))
            
            # Ensure object node exists
            await self.node_manager.ensure_node_exists(object_uri, str(object_class) if object_class else None)
            
            # Group relationships by type for ultra-fast batch processing
            rel_type = self._get_relationship_type(predicate_uri)
            self.relationships_by_type[rel_type].append((subject_uri, predicate_uri, object_uri))
            self.total_relationships += 1
            self.stats.relationship_triples += 1
            
            # Track relationship type counts
            self.stats.relationship_types_count[rel_type] = self.stats.relationship_types_count.get(rel_type, 0) + 1
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python value"""
        try:
            if literal.datatype:
                if literal.datatype == XSD.integer:
                    return int(literal)
                elif literal.datatype in (XSD.decimal, XSD.float, XSD.double):
                    return float(literal)
                elif literal.datatype == XSD.boolean:
                    return str(literal).lower() in ('true', '1')
                else:
                    return str(literal)
            else:
                if literal.language:
                    return f"{literal}@{literal.language}"
                return str(literal)
        except Exception as e:
            logger.warning(f"Error converting literal {literal}: {e}")
            return str(literal)
    
    def _get_relationship_type(self, predicate_uri: str) -> str:
        """Extract and clean relationship type from predicate URI"""
        parsed = urlparse(predicate_uri)
        if parsed.fragment:
            rel_type = parsed.fragment
        else:
            rel_type = predicate_uri.split('/')[-1] if '/' in predicate_uri else predicate_uri
        
        # Clean relationship type for Cypher compatibility
        rel_type = re.sub(r'[^a-zA-Z0-9_]', '_', rel_type)
        
        # Ensure valid identifier
        if not rel_type or rel_type.isdigit():
            rel_type = 'RELATED_TO'
        elif rel_type[0].isdigit():
            rel_type = f"_{rel_type}"
        
        # Limit length for performance
        if len(rel_type) > 50:
            rel_type = rel_type[:50]
        
        return rel_type
    
    async def _create_nodes_in_falkordb_async(self):
        """Create all nodes in FalkorDB using optimized async batching"""
        nodes = await self.node_manager.get_nodes()
        logger.info(f"🏗️  Creating {len(nodes):,} nodes in FalkorDB...")
        
        if not nodes:
            logger.warning("No nodes to create")
            return
        
        # Convert to list for batching
        nodes_list = list(nodes.items())
        batch_size = self.config.batch_size
        
        # Create node batches
        node_batches = []
        for i in range(0, len(nodes_list), batch_size):
            batch = nodes_list[i:i+batch_size]
            node_batches.append(batch)
        
        # Process node batches concurrently
        tasks = []
        for batch in node_batches:
            task = asyncio.create_task(
                self.falkordb_manager.create_node_batch(batch)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_created = 0
        for result in results:
            if isinstance(result, int):
                total_created += result
            else:
                logger.error(f"Node batch creation failed: {result}")
        
        logger.info(f"✅ Successfully created {total_created:,} nodes")
        self.stats.created_nodes = total_created
    
    async def _create_relationships_ultra_fast(self):
        """Create all relationships using ultra-fast optimized approach"""
        if not self.relationships_by_type:
            logger.info("No relationships to create")
            return
        
        logger.info(f"⚡ Creating {self.total_relationships:,} relationships using ultra-fast approach...")
        
        # Log relationship type distribution
        logger.info("📊 Relationship type distribution:")
        for rel_type, count in sorted(self.stats.relationship_types_count.items(), key=lambda x: x[1], reverse=True)[:10]:
            logger.info(f"  {rel_type}: {count:,} relationships")
        
        # Use the optimized FalkorDB manager method
        created_count = await self.falkordb_manager.create_relationships_ultra_fast(self.relationships_by_type)
        self.stats.created_relationships = created_count
        
        logger.info(f"✅ Successfully created {created_count:,} relationships")
    
    async def _create_indexes_with_discovered_labels(self):
        """Collect discovered labels and create indexes for them"""
        # Get all unique labels from the created nodes
        nodes = await self.node_manager.get_nodes()
        label_counts = Counter()
        
        for node_data in nodes.values():
            primary_label = node_data.get('primary_label')
            if primary_label:
                label_counts[primary_label] += 1
        
        if not label_counts:
            logger.warning("No nodes with labels found - creating fallback indexes only")
            await self.falkordb_manager.create_indexes()
            return
        
        logger.info(f"Discovered {len(label_counts)} RDF class labels from {sum(label_counts.values()):,} nodes")
        
        # Convert Counter to dict and pass to FalkorDB manager
        discovered_labels = dict(label_counts)
        await self.falkordb_manager.create_indexes(discovered_labels)
    
    async def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("🔍 Validating conversion results...")
        
        falkor_stats = await self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("⚠️  No nodes were created in FalkorDB")
        
        if self.stats.relationship_triples > 0 and falkor_stats['relationships'] == 0:
            logger.warning("⚠️  No relationships were created despite processing relationship triples")
        
        logger.info(f"✅ Validation complete: {falkor_stats['nodes']:,} nodes, {falkor_stats['relationships']:,} relationships")
    
    async def _finalize_stats(self):
        """Finalize conversion statistics with incremental loading support"""
        self.stats.end_time = datetime.now()
        
        # Get final counts from FalkorDB
        final_falkor_stats = await self.falkordb_manager.get_graph_stats()
        self.stats.final_nodes = final_falkor_stats['nodes']
        self.stats.final_relationships = final_falkor_stats['relationships']
        
        # Calculate actual nodes and relationships created in this run
        if self.config.append_to_existing_graph and not self.stats.graph_was_cleared:
            # In append mode, calculate the difference
            actual_nodes_created = self.stats.final_nodes - self.stats.initial_nodes
            actual_rels_created = self.stats.final_relationships - self.stats.initial_relationships
            
            # Update stats with actual created counts (may be less than processed due to duplicates)
            if actual_nodes_created >= 0:
                self.stats.created_nodes = actual_nodes_created
            if actual_rels_created >= 0:
                self.stats.created_relationships = actual_rels_created
                
            # Calculate skipped items (duplicates)
            nodes_processed = len(await self.node_manager.get_nodes())
            self.stats.skipped_nodes = max(0, nodes_processed - actual_nodes_created)
            self.stats.skipped_relationships = max(0, self.total_relationships - actual_rels_created)
        else:
            # In fresh mode, use the final counts as created counts
            self.stats.created_nodes = self.stats.final_nodes
            self.stats.created_relationships = self.stats.final_relationships
        
        # Calculate unique entities
        nodes = await self.node_manager.get_nodes()
        self.stats.unique_subjects = len(nodes)
        
        # Count unique objects from relationships
        unique_objects = set()
        for rel_list in self.relationships_by_type.values():
            for _, _, obj_uri in rel_list:
                unique_objects.add(obj_uri)
        self.stats.unique_objects = len(unique_objects)
        
        if self.config.export_stats:
            await self._export_stats()
    
    async def _log_performance_summary(self):
        """Log comprehensive performance summary with incremental loading info"""
        if not self.stats.end_time:
            return
        
        duration = (self.stats.end_time - self.stats.start_time).total_seconds()
        
        logger.info("=" * 80)
        
        # Show discovered RDF classes for verification
        if len(self.stats.subject_classes.union(self.stats.object_classes)) > 0:
            all_classes = self.stats.subject_classes.union(self.stats.object_classes)
            sample_classes = sorted(list(all_classes))[:10]  # Show top 10
            logger.info(f"🏷️  Discovered RDF Classes (sample):")
            for rdf_class in sample_classes:
                clean_label = self.uri_processor.process_uri(rdf_class)
                logger.info(f"   {rdf_class} → :{clean_label}")
            if len(all_classes) > 10:
                logger.info(f"   ... and {len(all_classes) - 10} more classes")
            logger.info("")
        
        if self.stats.append_mode:
            logger.info("🎯 INCREMENTAL LOADING SUMMARY")
        else:
            logger.info("🎯 PERFORMANCE SUMMARY")
        logger.info("=" * 80)
        
        # Graph state information
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            logger.info(f"📊 Graph State:")
            logger.info(f"   Initial: {self.stats.initial_nodes:,} nodes, {self.stats.initial_relationships:,} relationships")
            logger.info(f"   Final: {self.stats.final_nodes:,} nodes, {self.stats.final_relationships:,} relationships")
            logger.info(f"   Added: {self.stats.created_nodes:,} nodes, {self.stats.created_relationships:,} relationships")
            if self.stats.skipped_nodes > 0 or self.stats.skipped_relationships > 0:
                logger.info(f"   Skipped (duplicates): {self.stats.skipped_nodes:,} nodes, {self.stats.skipped_relationships:,} relationships")
            logger.info(f"")
        
        logger.info(f"📊 Data Processed:")
        logger.info(f"   Total triples: {self.stats.total_triples_retrieved:,}")
        logger.info(f"   Property triples: {self.stats.property_triples:,}")
        logger.info(f"   Relationship triples: {self.stats.relationship_triples:,}")
        logger.info(f"   RDF Classes discovered: {len(self.stats.subject_classes.union(self.stats.object_classes))}")
        logger.info(f"   Predicates used: {len(self.stats.predicates_used)}")
        logger.info(f"")
        
        if not self.stats.append_mode or self.stats.graph_was_cleared:
            logger.info(f"🏗️  Entities Created:")
            logger.info(f"   Nodes: {self.stats.created_nodes:,}")
            logger.info(f"   Relationships: {self.stats.created_relationships:,}")
        else:
            logger.info(f"🏗️  Entities Added:")
            logger.info(f"   New nodes: {self.stats.created_nodes:,}")
            logger.info(f"   New relationships: {self.stats.created_relationships:,}")
            
        logger.info(f"   Relationship types: {len(self.stats.relationship_types_count)}")
        logger.info(f"")
        logger.info(f"⏱️  Performance Metrics:")
        logger.info(f"   Total time: {duration:.2f} seconds")
        logger.info(f"   Query execution: {self.stats.query_execution_time:.2f} seconds")
        logger.info(f"   Node creation: {self.stats.node_creation_time:.2f} seconds")
        logger.info(f"   Relationship creation: {self.stats.relationship_creation_time:.2f} seconds")
        logger.info(f"")
        logger.info(f"🚀 Throughput Rates:")
        logger.info(f"   Overall: {self.stats.processed_triples/duration:.1f} triples/second")
        if self.stats.relationship_creation_rate > 0:
            logger.info(f"   Relationships: {self.stats.relationship_creation_rate:.1f} relationships/second")
        logger.info(f"")
        
        # Performance assessment
        if self.stats.relationship_creation_rate > 1000:
            logger.info("🎉 EXCELLENT: Ultra-fast relationship creation (>1000 rel/sec)")
        elif self.stats.relationship_creation_rate > 500:
            logger.info("✅ GOOD: Fast relationship creation (>500 rel/sec)")
        elif self.stats.relationship_creation_rate > 100:
            logger.info("⚠️  ACCEPTABLE: Moderate relationship creation (>100 rel/sec)")
        else:
            logger.info("❌ SLOW: Consider optimizing configuration (<100 rel/sec)")
        
        # Incremental loading efficiency
        if self.stats.append_mode and not self.stats.graph_was_cleared:
            total_processed = self.stats.unique_subjects + len(set(obj for rels in self.relationships_by_type.values() for _, _, obj in rels))
            actual_added = self.stats.created_nodes + self.stats.created_relationships
            if total_processed > 0:
                efficiency = (actual_added / total_processed) * 100
                logger.info(f"📈 Incremental efficiency: {efficiency:.1f}% new data added")
                
                if efficiency > 80:
                    logger.info("🎉 HIGH efficiency: Mostly new data")
                elif efficiency > 50:
                    logger.info("✅ GOOD efficiency: Good mix of new data")
                elif efficiency > 20:
                    logger.info("⚠️  MODERATE efficiency: Some duplicate data")
                else:
                    logger.info("❌ LOW efficiency: Mostly duplicate data")
        
        logger.info("=" * 80)
    
    async def _export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"optimized_conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['config'] = {
                'batch_size': self.config.batch_size,
                'max_concurrent_batches': self.config.max_concurrent_batches,
                'connection_pool_size': self.config.connection_pool_size,
                'use_bulk_relationship_creation': self.config.use_bulk_relationship_creation,
                'disable_relationship_properties': self.config.disable_relationship_properties,
            }
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"📄 Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted RDF graph"""
        return [
            "MATCH (n) RETURN labels(n) as rdf_classes, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as rdf_predicates, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) WHERE n.rdf_type IS NOT NULL RETURN n.rdf_type, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri, labels(n) LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN labels(s), type(r), labels(o) LIMIT 10",
            "MATCH (n) RETURN labels(n) as class, size((n)--()) as degree ORDER BY degree DESC LIMIT 10",
            "MATCH (n) RETURN count(n) as total_nodes",
            "MATCH ()-[r]->() RETURN count(r) as total_relationships",
            "CALL db.labels() YIELD label RETURN label as discovered_rdf_classes",
            "CALL db.relationshipTypes() YIELD relationshipType RETURN relationshipType as rdf_predicates"
        ]

# Utility functions
def create_optimized_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph", 
                          append_mode: bool = True) -> OptimizedAsyncTripleConfig:
    """Create an optimized configuration for fast edge loading with incremental support"""
    
    return OptimizedAsyncTripleConfig(
        triples_query=triples_query,
        sparql_endpoint=sparql_endpoint,
        graph_name=graph_name,
        
        # Incremental loading settings
        append_to_existing_graph=append_mode,       # Append by default
        clear_existing_graph=False,                 # Don't clear by default
        handle_duplicates=append_mode,              # Handle duplicates in append mode
        
        # Optimized settings for fast relationship creation
        batch_size=2000,                         # Larger batches for relationships
        max_concurrent_batches=3,                # Controlled concurrency
        connection_pool_size=10,                 # Moderate pool size
        sparql_timeout=7200,                     # 2 hours for large queries
        falkordb_timeout=300,                    # 5 minutes for FalkorDB operations
        
        # Speed optimizations
        preserve_uri_properties=False,           # Disable for maximum speed
        disable_relationship_properties=True,    # Skip relationship properties
        group_relationships_by_type=True,        # Enable grouping
        use_bulk_relationship_creation=True,     # Enable bulk operations
        
        # Performance settings
        use_shortened_uris=True,
        create_indexes=True,
        validate_conversion=False,               # Skip validation for speed
        export_stats=True,
        progress_update_interval=25,             # Frequent progress updates
    )

def create_fresh_graph_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for fresh graph creation (clears existing data)"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=False)
    config.clear_existing_graph = True
    config.handle_duplicates = False  # No need to handle duplicates in fresh graph
    return config

def create_append_config(sparql_endpoint: str, triples_query: str, graph_name: str = "rdf_graph") -> OptimizedAsyncTripleConfig:
    """Create configuration for appending to existing graph"""
    config = create_optimized_config(sparql_endpoint, triples_query, graph_name, append_mode=True)
    config.append_to_existing_graph = True
    config.clear_existing_graph = False
    config.handle_duplicates = True
    return config

async def test_optimized_converter(sparql_endpoint: str, triples_query: str, sample_size: int = 1000):
    """Test the optimized converter with a sample dataset"""
    
    logger.info(f"🧪 Testing optimized converter with sample query...")
    
    # Add LIMIT to query for testing if not already present
    test_query = triples_query
    if "LIMIT" not in test_query.upper():
        test_query = f"{triples_query.rstrip()} LIMIT {sample_size}"
    
    # Create test configuration
    config = create_optimized_config(sparql_endpoint, test_query, "test_optimization")
    
    # Run conversion
    converter = OptimizedAsyncTripleBasedConverter(config)
    
    try:
        stats = await converter.convert()
        
        # Display results
        logger.info("🎯 TEST RESULTS:")
        logger.info(f"   Processed: {stats.processed_triples:,} triples")
        logger.info(f"   Created: {stats.created_nodes:,} nodes, {stats.created_relationships:,} relationships")
        
        if stats.relationship_creation_rate > 0:
            logger.info(f"   Relationship rate: {stats.relationship_creation_rate:.1f} rel/sec")
            
            if stats.relationship_creation_rate > 1000:
                logger.info("🎉 EXCELLENT performance! Ready for large datasets.")
            elif stats.relationship_creation_rate > 500:
                logger.info("✅ GOOD performance! Should handle large datasets well.")
            else:
                logger.info("⚠️  Consider optimizing Docker/system configuration.")
        
        return stats
        
    except Exception as e:
        logger.error(f"❌ Test failed: {e}")
        raise

async def main():
    """Example usage of the FIXED optimized converter - customize with your own endpoint and query"""
    
    # CUSTOMIZE THESE VALUES FOR YOUR USE CASE
    sparql_endpoint = "https://dbpedia.org/sparql"  # Example endpoint - replace with yours
    graph_name = "your_graph_name"                 # Replace with your graph name
    
    # Example SPARQL query - MUST return these 6 variables:
    # ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass
    your_custom_query = """
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
        # Example filter to limit results for testing
        FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/"))
        FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
    }
    """
    
    # Validation message for users
    logger.info("🚀 Starting FIXED Optimized FalkorDB RDF Conversion")
    logger.info("=" * 70)
    logger.info(f"📡 Endpoint: {sparql_endpoint}")
    logger.info(f"📊 Graph: {graph_name}")
    logger.info("🔧 FIXES APPLIED:")
    logger.info("   ✅ Fixed SPARQL connection initialization")
    logger.info("   ✅ Removed TIMEOUT syntax from Cypher queries")
    logger.info("   ✅ Fixed property setting syntax")
    logger.info("   ✅ Simplified relationship creation")
    logger.info("   ✅ Enhanced error handling")
    logger.info("")
    logger.info("💡 TO USE WITH YOUR DATA:")
    logger.info("   1. Replace 'sparql_endpoint' with your SPARQL endpoint URL")
    logger.info("   2. Replace 'your_custom_query' with your SPARQL query")
    logger.info("   3. Ensure your query returns: ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass")
    logger.info("   4. Replace 'graph_name' with your desired graph name")
    
    try:
        # Option 1: Test with small sample first (recommended)
        logger.info("\nPhase 1: Testing with sample dataset...")
        await test_optimized_converter(sparql_endpoint, your_custom_query, sample_size=100)
        
        logger.info("\n" + "=" * 70)
        logger.info("Phase 2: Full dataset conversion...")
        
        # Option 2: Full conversion
        config = create_append_config(sparql_endpoint, your_custom_query, graph_name)
        
        # Customize configuration if needed
        # config.batch_size = 3000                    # Adjust batch size
        # config.max_concurrent_batches = 2           # Adjust concurrency
        # config.preserve_uri_properties = True       # Enable if you need URI properties
        # config.clear_existing_graph = True          # Enable to clear existing data
        
        converter = OptimizedAsyncTripleBasedConverter(config)
        stats = await converter.convert()
        
        # Final results
        logger.info("\n" + "🎉 CONVERSION COMPLETED SUCCESSFULLY!")
        logger.info("=" * 70)
        logger.info(f"📊 {stats.get_incremental_summary()}")
        logger.info("")
        logger.info("Sample queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            logger.info(f"{i}. {query}")
        
        return stats
        
    except Exception as e:
        logger.error(f"❌ Conversion failed: {e}")
        import traceback
        traceback.print_exc()
        raise

# Additional utility functions for different use cases
async def single_query_append(sparql_endpoint: str, triples_query: str, graph_name: str = "default_graph"):
    """Append a single SPARQL query result to existing graph"""
    logger.info(f"📈 Appending query results to graph '{graph_name}'...")
    
    config = create_append_config(sparql_endpoint, triples_query, graph_name)
    converter = OptimizedAsyncTripleBasedConverter(config)
    stats = await converter.convert()
    
    logger.info(f"✅ Append complete: {stats.get_incremental_summary()}")
    return stats

async def fresh_graph_from_query(sparql_endpoint: str, triples_query: str, graph_name: str = "fresh_graph"):
    """Create a fresh graph from a SPARQL query (clears existing data)"""
    logger.info(f"🔄 Creating fresh graph '{graph_name}'...")
    
    config = create_fresh_graph_config(sparql_endpoint, triples_query, graph_name)
    converter = OptimizedAsyncTripleBasedConverter(config)
    stats = await converter.convert()
    
    logger.info(f"✅ Fresh graph created: {stats.created_nodes:,} nodes, {stats.created_relationships:,} relationships")
    return stats

async def incremental_loading_example():
    """Example: Multiple incremental loads - customize with your own queries"""
    
    # CUSTOMIZE THESE
    endpoint = "https://dbpedia.org/sparql"  # Example endpoint
    graph_name = "incremental_graph"
    
    # Example queries - replace with yours
    query1 = """
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
        FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/A"))
        FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
    }
    LIMIT 50
    """
    
    query2 = """
    SELECT ?subject ?subjectClass ?predicate ?predicateClass ?object ?objectClass WHERE {
        ?subject ?predicate ?object .
        OPTIONAL { ?subject a ?subjectClass }
        OPTIONAL { ?predicate a ?predicateClass }
        OPTIONAL { ?object a ?objectClass . FILTER(isURI(?object)) }
        FILTER(STRSTARTS(STR(?subject), "http://dbpedia.org/resource/B"))
        FILTER(?predicate = <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>)
    }
    LIMIT 50
    """
    
    try:
        # Load 1
        logger.info("Load 1: Running first query...")
        stats1 = await single_query_append(endpoint, query1, graph_name)
        logger.info(f"Load 1: {stats1.get_incremental_summary()}")
        
        # Load 2
        logger.info("Load 2: Running second query...")
        stats2 = await single_query_append(endpoint, query2, graph_name)
        logger.info(f"Load 2: {stats2.get_incremental_summary()}")
        
        logger.info("🎉 Incremental loading completed!")
        
    except Exception as e:
        logger.error(f"❌ Incremental loading failed: {e}")
        raise

if __name__ == "__main__":
    # Run the FIXED converter with your custom configuration
    asyncio.run(main())
    
    # Uncomment to test incremental loading:
    # asyncio.run(incremental_loading_example())
