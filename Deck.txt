#!/usr/bin/env python3
"""
Dynamic RDF to FalkorDB Direct Converter
========================================

A completely adaptive converter that:
- Analyzes TTL structure dynamically
- Extracts namespaces, classes, and properties from actual data
- Loads directly into FalkorDB using Cypher queries
- No hardcoded URIs, labels, or CSV intermediates
- Preserves complete RDF semantics

Requirements:
    pip install falkordb rdflib psutil tqdm pyyaml
"""

import gc
import time
import logging
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re

# Core libraries
import psutil
from tqdm import tqdm

# RDF processing
import rdflib
from rdflib import Graph, URIRef, Literal, BNode
from rdflib.namespace import RDF, RDFS, OWL, XSD

# FalkorDB
import falkordb


@dataclass
class ConversionMetrics:
    """Comprehensive metrics for the conversion process."""
    start_time: float = 0.0
    end_time: float = 0.0
    
    # RDF Analysis
    total_triples: int = 0
    unique_subjects: int = 0
    unique_predicates: int = 0
    unique_objects: int = 0
    
    # Schema Discovery
    discovered_classes: int = 0
    discovered_properties: int = 0
    namespaces_found: int = 0
    
    # Conversion Results
    nodes_created: int = 0
    edges_created: int = 0
    properties_set: int = 0
    
    # Performance
    memory_peak_mb: float = 0.0
    processing_chunks: int = 0
    
    @property
    def processing_time(self) -> float:
        return (self.end_time or time.time()) - self.start_time


@dataclass
class ConversionConfig:
    """Configuration for dynamic RDF conversion."""
    # Processing
    chunk_size: int = 50000
    max_memory_mb: int = 2000
    batch_size: int = 1000  # Direct Cypher batch size
    
    # Database
    host: str = "localhost"
    port: int = 6379
    password: Optional[str] = None
    graph_name: str = "knowledge_graph"
    
    # Analysis
    analyze_schema: bool = True
    preserve_namespaces: bool = True
    create_indexes: bool = True
    
    # Optimization
    parallel_loading: bool = True
    skip_duplicates: bool = True
    normalize_labels: bool = True  # Clean labels for Cypher compatibility


class NamespaceAnalyzer:
    """Dynamically discovers and manages namespaces from RDF data."""
    
    def __init__(self):
        self.namespace_map: Dict[str, str] = {}
        self.prefix_map: Dict[str, str] = {}
        self.usage_counts: Counter = Counter()
        
    def analyze_graph(self, graph: Graph):
        """Analyze graph to discover all namespaces."""
        # Extract from graph's namespace manager
        for prefix, namespace in graph.namespaces():
            if prefix and namespace:
                self.register_namespace(str(prefix), str(namespace))
        
        # Discover additional namespaces from URIs
        all_uris = set()
        for s, p, o in graph:
            if isinstance(s, URIRef):
                all_uris.add(str(s))
            if isinstance(p, URIRef):
                all_uris.add(str(p))
            if isinstance(o, URIRef):
                all_uris.add(str(o))
        
        # Extract namespaces from URIs
        discovered_namespaces = set()
        for uri in all_uris:
            namespace = self._extract_namespace(uri)
            if namespace and namespace not in discovered_namespaces:
                discovered_namespaces.add(namespace)
                self.usage_counts[namespace] += 1
        
        # Auto-generate prefixes for discovered namespaces
        for namespace in discovered_namespaces:
            if namespace not in self.prefix_map:
                prefix = self._generate_prefix(namespace)
                self.register_namespace(prefix, namespace)
    
    def register_namespace(self, prefix: str, namespace: str):
        """Register a namespace with its prefix."""
        self.namespace_map[namespace] = prefix
        self.prefix_map[prefix] = namespace
    
    def _extract_namespace(self, uri: str) -> Optional[str]:
        """Extract namespace from URI."""
        # Try different separators
        for sep in ['#', '/']:
            if sep in uri:
                parts = uri.rsplit(sep, 1)
                if len(parts) == 2 and parts[1]:  # Has local part
                    return parts[0] + sep
        return None
    
    def _generate_prefix(self, namespace: str) -> str:
        """Generate a meaningful prefix for a namespace."""
        # Try to extract from URL structure
        parsed = urlparse(namespace)
        
        # Use domain name or path segments
        candidates = []
        if parsed.netloc:
            # Extract from domain
            domain_parts = parsed.netloc.split('.')
            for part in reversed(domain_parts):
                if part not in ['www', 'com', 'org', 'net', 'edu']:
                    candidates.append(part)
        
        if parsed.path:
            # Extract from path
            path_parts = [p for p in parsed.path.split('/') if p]
            candidates.extend(path_parts[-2:])  # Last two path segments
        
        # Clean and create prefix
        for candidate in candidates:
            # Clean up candidate
            clean = re.sub(r'[^a-zA-Z0-9]', '', candidate).lower()
            if clean and len(clean) >= 2:
                # Ensure uniqueness
                base_prefix = clean[:8]  # Max 8 chars
                prefix = base_prefix
                counter = 1
                while prefix in self.prefix_map:
                    prefix = f"{base_prefix}{counter}"
                    counter += 1
                return prefix
        
        # Fallback to hash-based prefix
        hash_val = abs(hash(namespace)) % 10000
        prefix = f"ns{hash_val}"
        while prefix in self.prefix_map:
            hash_val = (hash_val + 1) % 10000
            prefix = f"ns{hash_val}"
        return prefix
    
    def get_local_name(self, uri: str) -> Tuple[Optional[str], str]:
        """Get namespace prefix and local name for URI."""
        namespace = self._extract_namespace(uri)
        if namespace and namespace in self.namespace_map:
            local_name = uri[len(namespace):]
            return self.namespace_map[namespace], local_name
        return None, uri


class SchemaAnalyzer:
    """Analyzes RDF data to discover classes, properties, and patterns."""
    
    def __init__(self, namespace_analyzer: NamespaceAnalyzer):
        self.namespace_analyzer = namespace_analyzer
        
        # Discovered schema elements
        self.classes: Set[str] = set()
        self.properties: Set[str] = set()
        self.data_properties: Set[str] = set()
        self.object_properties: Set[str] = set()
        
        # Class-property relationships
        self.class_properties: Dict[str, Set[str]] = defaultdict(set)
        self.property_domains: Dict[str, Set[str]] = defaultdict(set)
        self.property_ranges: Dict[str, Set[str]] = defaultdict(set)
        
        # Instance data
        self.subject_types: Dict[str, Set[str]] = defaultdict(set)
        self.literal_datatypes: Dict[str, Set[str]] = defaultdict(set)
        
        # Statistics
        self.triple_patterns: Counter = Counter()
        self.predicate_usage: Counter = Counter()
    
    def analyze_graph(self, graph: Graph):
        """Comprehensive analysis of RDF graph structure."""
        logging.info("Analyzing RDF graph structure...")
        
        # First pass: collect basic statistics and type information
        for s, p, o in tqdm(graph, desc="Analyzing triples"):
            self._analyze_triple(s, p, o)
        
        # Second pass: infer additional schema information
        self._infer_schema_patterns()
        
        logging.info(f"Schema analysis complete:")
        logging.info(f"  Classes discovered: {len(self.classes)}")
        logging.info(f"  Properties discovered: {len(self.properties)}")
        logging.info(f"  Data properties: {len(self.data_properties)}")
        logging.info(f"  Object properties: {len(self.object_properties)}")
    
    def _analyze_triple(self, subject: Any, predicate: Any, obj: Any):
        """Analyze a single triple for schema information."""
        s_str = str(subject)
        p_str = str(predicate)
        o_str = str(obj)
        
        # Track predicate usage
        self.predicate_usage[p_str] += 1
        
        # Handle rdf:type triples
        if p_str == str(RDF.type):
            if isinstance(obj, URIRef):
                class_uri = str(obj)
                self.classes.add(class_uri)
                self.subject_types[s_str].add(class_uri)
                
                # Extract clean class name
                _, class_name = self.namespace_analyzer.get_local_name(class_uri)
                self.subject_types[s_str].add(class_name)
        
        # Classify properties
        if isinstance(obj, Literal):
            self.data_properties.add(p_str)
            if obj.datatype:
                self.literal_datatypes[p_str].add(str(obj.datatype))
        else:
            self.object_properties.add(p_str)
        
        self.properties.add(p_str)
        
        # Track property usage by class
        subject_classes = self.subject_types.get(s_str, set())
        for class_uri in subject_classes:
            self.class_properties[class_uri].add(p_str)
        
        # Track triple patterns
        s_type = 'URI' if isinstance(subject, URIRef) else 'BNode' if isinstance(subject, BNode) else 'Literal'
        p_type = 'URI'  # Predicates are always URIs in RDF
        o_type = 'URI' if isinstance(obj, URIRef) else 'BNode' if isinstance(obj, BNode) else 'Literal'
        pattern = f"{s_type}-{p_type}-{o_type}"
        self.triple_patterns[pattern] += 1
    
    def _infer_schema_patterns(self):
        """Infer additional schema information from patterns."""
        # Infer domains and ranges
        for s_str, types in self.subject_types.items():
            # For each subject with known types
            for prop in self.properties:
                if any(prop in self.class_properties[cls] for cls in types):
                    for cls in types:
                        self.property_domains[prop].add(cls)
    
    def get_class_label(self, class_uri: str) -> str:
        """Get a clean label for a class."""
        prefix, local_name = self.namespace_analyzer.get_local_name(class_uri)
        
        if local_name and local_name != class_uri:
            # Clean the local name for use as Cypher label
            return self._clean_label(local_name)
        else:
            # Fallback: extract from full URI
            return self._clean_label(class_uri.split('/')[-1] or class_uri.split('#')[-1] or 'Entity')
    
    def get_property_label(self, property_uri: str) -> str:
        """Get a clean label for a property."""
        prefix, local_name = self.namespace_analyzer.get_local_name(property_uri)
        
        if local_name and local_name != property_uri:
            return self._clean_property_name(local_name)
        else:
            return self._clean_property_name(property_uri.split('/')[-1] or property_uri.split('#')[-1] or 'property')
    
    def _clean_label(self, label: str) -> str:
        """Clean a label for use as Cypher node label."""
        # Remove special characters, ensure it starts with letter
        cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', label)
        cleaned = re.sub(r'_+', '_', cleaned)  # Collapse multiple underscores
        cleaned = cleaned.strip('_')
        
        # Ensure starts with letter
        if cleaned and not cleaned[0].isalpha():
            cleaned = f"C_{cleaned}"
        
        # Capitalize for convention
        return cleaned.capitalize() if cleaned else 'Entity'
    
    def _clean_property_name(self, prop_name: str) -> str:
        """Clean property name for use as Cypher property."""
        # Similar to label cleaning but lowercase
        cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', prop_name)
        cleaned = re.sub(r'_+', '_', cleaned)
        cleaned = cleaned.strip('_')
        
        # Ensure starts with letter or underscore
        if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
            cleaned = f"p_{cleaned}"
        
        return cleaned.lower() if cleaned else 'property'


class ResourceMapper:
    """Maps RDF resources to graph database identifiers."""
    
    def __init__(self, schema_analyzer: SchemaAnalyzer):
        self.schema_analyzer = schema_analyzer
        self.resource_ids: Dict[str, str] = {}
        self.id_counter = 0
        
    def get_node_id(self, resource: Union[URIRef, BNode, str]) -> str:
        """Get or create a unique node ID for a resource."""
        resource_str = str(resource)
        
        if resource_str not in self.resource_ids:
            if isinstance(resource, URIRef):
                # Use hash of URI for consistency
                uri_hash = hashlib.md5(resource_str.encode()).hexdigest()[:12]
                self.resource_ids[resource_str] = f"uri_{uri_hash}"
            elif isinstance(resource, BNode):
                # Sequential ID for blank nodes
                self.resource_ids[resource_str] = f"bn_{self.id_counter}"
                self.id_counter += 1
            else:
                # Generic resource
                gen_hash = hashlib.md5(resource_str.encode()).hexdigest()[:12]
                self.resource_ids[resource_str] = f"res_{gen_hash}"
        
        return self.resource_ids[resource_str]
    
    def get_node_labels(self, resource: Union[URIRef, BNode, str]) -> List[str]:
        """Get node labels for a resource based on its rdf:type."""
        resource_str = str(resource)
        labels = []
        
        # Get types from schema analysis
        types = self.schema_analyzer.subject_types.get(resource_str, set())
        
        for type_uri in types:
            if type_uri != resource_str:  # Avoid self-reference
                label = self.schema_analyzer.get_class_label(type_uri)
                labels.append(label)
        
        # Default label based on resource type
        if not labels:
            if isinstance(resource, URIRef):
                labels.append('Resource')
            elif isinstance(resource, BNode):
                labels.append('BlankNode')
            else:
                labels.append('Entity')
        
        return labels
    
    def should_create_literal_node(self, literal: Literal) -> bool:
        """Determine if a literal should become a separate node."""
        literal_str = str(literal)
        
        # Create nodes for very long literals or special datatypes
        if len(literal_str) > 200:
            return True
        
        # Create nodes for complex structured literals
        if literal.datatype and str(literal.datatype) in [
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#XMLLiteral',
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#HTML'
        ]:
            return True
        
        return False


class DirectFalkorDBLoader:
    """Direct loader that converts RDF to FalkorDB using Cypher queries."""
    
    def __init__(self, config: ConversionConfig, schema_analyzer: SchemaAnalyzer, 
                 resource_mapper: ResourceMapper):
        self.config = config
        self.schema_analyzer = schema_analyzer
        self.resource_mapper = resource_mapper
        self.db = None
        self.graph = None
        
        # Batch management
        self.node_batch: List[Dict] = []
        self.edge_batch: List[Dict] = []
        self.property_batch: List[Dict] = []
        
        # Statistics
        self.nodes_created = 0
        self.edges_created = 0
        self.properties_set = 0
    
    def connect(self):
        """Connect to FalkorDB."""
        try:
            self.db = falkordb.FalkorDB(
                host=self.config.host,
                port=self.config.port,
                password=self.config.password
            )
            self.graph = self.db.select_graph(self.config.graph_name)
            logging.info(f"Connected to FalkorDB at {self.config.host}:{self.config.port}")
        except Exception as e:
            logging.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def setup_indexes(self):
        """Create indexes based on discovered schema."""
        if not self.config.create_indexes:
            return
        
        # Create indexes for discovered classes
        for class_uri in self.schema_analyzer.classes:
            try:
                class_label = self.schema_analyzer.get_class_label(class_uri)
                index_query = f"CREATE INDEX FOR (n:{class_label}) ON (n.uri)"
                self.graph.query(index_query)
                logging.info(f"Created index for class: {class_label}")
            except Exception as e:
                logging.warning(f"Failed to create index for {class_label}: {e}")
        
        # Generic indexes
        try:
            self.graph.query("CREATE INDEX FOR (n:Resource) ON (n.uri)")
            self.graph.query("CREATE INDEX FOR (n:BlankNode) ON (n.id)")
            logging.info("Created generic indexes")
        except Exception as e:
            logging.warning(f"Failed to create generic indexes: {e}")
    
    def load_triple(self, subject: Any, predicate: Any, obj: Any):
        """Load a single RDF triple into the graph."""
        # Handle rdf:type specially
        if str(predicate) == str(RDF.type):
            self._handle_type_triple(subject, obj)
        elif isinstance(obj, Literal):
            self._handle_data_property(subject, predicate, obj)
        else:
            self._handle_object_property(subject, predicate, obj)
        
        # Flush batches if they're full
        if len(self.node_batch) >= self.config.batch_size:
            self._flush_node_batch()
        if len(self.edge_batch) >= self.config.batch_size:
            self._flush_edge_batch()
        if len(self.property_batch) >= self.config.batch_size:
            self._flush_property_batch()
    
    def _handle_type_triple(self, subject: Any, type_obj: Any):
        """Handle rdf:type triples by creating/updating nodes with labels."""
        node_id = self.resource_mapper.get_node_id(subject)
        labels = self.resource_mapper.get_node_labels(subject)
        
        # Create or update node
        node_data = {
            'id': node_id,
            'uri': str(subject) if isinstance(subject, URIRef) else None,
            'labels': labels
        }
        
        self.node_batch.append(node_data)
    
    def _handle_data_property(self, subject: Any, predicate: Any, literal: Literal):
        """Handle data properties (literal values)."""
        if self.resource_mapper.should_create_literal_node(literal):
            # Create literal as separate node
            self._create_literal_node(subject, predicate, literal)
        else:
            # Add as property to subject node
            self._add_node_property(subject, predicate, literal)
    
    def _handle_object_property(self, subject: Any, predicate: Any, obj: Any):
        """Handle object properties (relationships)."""
        subj_id = self.resource_mapper.get_node_id(subject)
        obj_id = self.resource_mapper.get_node_id(obj)
        
        # Ensure both nodes exist
        self._ensure_node_exists(subject)
        self._ensure_node_exists(obj)
        
        # Create edge
        edge_type = self.schema_analyzer.get_property_label(str(predicate))
        edge_data = {
            'source_id': subj_id,
            'target_id': obj_id,
            'type': edge_type,
            'uri': str(predicate)
        }
        
        self.edge_batch.append(edge_data)
    
    def _ensure_node_exists(self, resource: Any):
        """Ensure a node exists for the given resource."""
        node_id = self.resource_mapper.get_node_id(resource)
        labels = self.resource_mapper.get_node_labels(resource)
        
        node_data = {
            'id': node_id,
            'uri': str(resource) if isinstance(resource, URIRef) else None,
            'labels': labels
        }
        
        # Check if already in batch to avoid duplicates
        if not any(n['id'] == node_id for n in self.node_batch):
            self.node_batch.append(node_data)
    
    def _add_node_property(self, subject: Any, predicate: Any, literal: Literal):
        """Add a property to a node."""
        node_id = self.resource_mapper.get_node_id(subject)
        prop_name = self.schema_analyzer.get_property_label(str(predicate))
        
        # Convert literal value
        prop_value = self._convert_literal_value(literal)
        
        prop_data = {
            'node_id': node_id,
            'property': prop_name,
            'value': prop_value,
            'datatype': str(literal.datatype) if literal.datatype else None,
            'language': literal.language
        }
        
        self.property_batch.append(prop_data)
    
    def _create_literal_node(self, subject: Any, predicate: Any, literal: Literal):
        """Create a separate node for a complex literal."""
        subj_id = self.resource_mapper.get_node_id(subject)
        lit_id = self.resource_mapper.get_node_id(f"literal_{hash(str(literal))}")
        
        # Create literal node
        lit_node = {
            'id': lit_id,
            'uri': None,
            'labels': ['Literal'],
            'value': str(literal),
            'datatype': str(literal.datatype) if literal.datatype else None,
            'language': literal.language
        }
        self.node_batch.append(lit_node)
        
        # Create edge to literal
        edge_type = self.schema_analyzer.get_property_label(str(predicate))
        edge_data = {
            'source_id': subj_id,
            'target_id': lit_id,
            'type': edge_type,
            'uri': str(predicate)
        }
        self.edge_batch.append(edge_data)
    
    def _convert_literal_value(self, literal: Literal) -> Any:
        """Convert RDF literal to appropriate Python type."""
        if literal.datatype:
            datatype_str = str(literal.datatype)
            try:
                if datatype_str in [str(XSD.integer), str(XSD.int), str(XSD.long)]:
                    return int(literal)
                elif datatype_str in [str(XSD.decimal), str(XSD.double), str(XSD.float)]:
                    return float(literal)
                elif datatype_str == str(XSD.boolean):
                    return str(literal).lower() in ('true', '1')
                elif datatype_str in [str(XSD.date), str(XSD.dateTime), str(XSD.time)]:
                    return str(literal)  # Keep as string for compatibility
            except (ValueError, TypeError):
                pass
        
        return str(literal)
    
    def _flush_node_batch(self):
        """Flush the node batch to FalkorDB."""
        if not self.node_batch:
            return
        
        # Group nodes by labels for efficient creation
        nodes_by_labels = defaultdict(list)
        for node in self.node_batch:
            labels_key = ':'.join(sorted(node['labels']))
            nodes_by_labels[labels_key].append(node)
        
        for labels_key, nodes in nodes_by_labels.items():
            labels = labels_key.split(':')
            label_clause = ':'.join(labels)
            
            # Create nodes with MERGE to avoid duplicates
            query = f"""
            UNWIND $nodes AS nodeData
            MERGE (n:{label_clause} {{id: nodeData.id}})
            SET n.uri = nodeData.uri
            """
            
            try:
                self.graph.query(query, {'nodes': nodes})
                self.nodes_created += len(nodes)
            except Exception as e:
                logging.error(f"Failed to create nodes with labels {labels_key}: {e}")
                # Try individual creation as fallback
                self._create_nodes_individually(nodes, labels)
        
        self.node_batch.clear()
    
    def _flush_edge_batch(self):
        """Flush the edge batch to FalkorDB."""
        if not self.edge_batch:
            return
        
        # Group edges by type
        edges_by_type = defaultdict(list)
        for edge in self.edge_batch:
            edges_by_type[edge['type']].append(edge)
        
        for edge_type, edges in edges_by_type.items():
            query = f"""
            UNWIND $edges AS edgeData
            MATCH (source {{id: edgeData.source_id}})
            MATCH (target {{id: edgeData.target_id}})
            MERGE (source)-[r:{edge_type}]->(target)
            SET r.uri = edgeData.uri
            """
            
            try:
                self.graph.query(query, {'edges': edges})
                self.edges_created += len(edges)
            except Exception as e:
                logging.error(f"Failed to create edges of type {edge_type}: {e}")
        
        self.edge_batch.clear()
    
    def _flush_property_batch(self):
        """Flush the property batch to FalkorDB."""
        if not self.property_batch:
            return
        
        # Group properties by property name
        props_by_name = defaultdict(list)
        for prop in self.property_batch:
            props_by_name[prop['property']].append(prop)
        
        for prop_name, props in props_by_name.items():
            query = f"""
            UNWIND $props AS propData
            MATCH (n {{id: propData.node_id}})
            SET n.{prop_name} = propData.value
            """
            
            try:
                self.graph.query(query, {'props': props})
                self.properties_set += len(props)
            except Exception as e:
                logging.error(f"Failed to set property {prop_name}: {e}")
        
        self.property_batch.clear()
    
    def _create_nodes_individually(self, nodes: List[Dict], labels: List[str]):
        """Fallback: create nodes individually."""
        for node in nodes:
            try:
                label_clause = ':'.join(labels)
                query = f"MERGE (n:{label_clause} {{id: $id}}) SET n.uri = $uri"
                self.graph.query(query, {'id': node['id'], 'uri': node['uri']})
                self.nodes_created += 1
            except Exception as e:
                logging.error(f"Failed to create individual node {node['id']}: {e}")
    
    def finalize(self):
        """Flush all remaining batches."""
        self._flush_node_batch()
        self._flush_edge_batch()
        self._flush_property_batch()


class RDFToFalkorDBConverter:
    """Main converter that orchestrates the dynamic conversion process."""
    
    def __init__(self, config: ConversionConfig):
        self.config = config
        self.metrics = ConversionMetrics()
        
        # Components
        self.namespace_analyzer = NamespaceAnalyzer()
        self.schema_analyzer = SchemaAnalyzer(self.namespace_analyzer)
        self.resource_mapper = ResourceMapper(self.schema_analyzer)
        self.loader = DirectFalkorDBLoader(config, self.schema_analyzer, self.resource_mapper)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
    
    def convert_rdf_file(self, rdf_file_path: str) -> ConversionMetrics:
        """Convert RDF file to FalkorDB with complete dynamic analysis."""
        self.metrics.start_time = time.time()
        
        try:
            logging.info(f"Starting dynamic conversion of {rdf_file_path}")
            
            # Load and analyze RDF
            graph = self._load_rdf_file(rdf_file_path)
            self._analyze_rdf_structure(graph)
            
            # Connect to FalkorDB and setup
            self.loader.connect()
            self.loader.setup_indexes()
            
            # Convert RDF data
            self._convert_rdf_data(graph)
            
            # Finalize and collect metrics
            self.loader.finalize()
            self._collect_final_metrics()
            
            self.metrics.end_time = time.time()
            
            logging.info(f"Conversion completed in {self.metrics.processing_time:.2f} seconds")
            self._print_conversion_summary()
            
            return self.metrics
            
        except Exception as e:
            logging.error(f"Conversion failed: {e}")
            raise
    
    def _load_rdf_file(self, file_path: str) -> Graph:
        """Load RDF file with format detection."""
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"RDF file not found: {file_path}")
        
        # Detect format
        format_map = {
            '.ttl': 'turtle', '.turtle': 'turtle',
            '.nt': 'nt', '.ntriples': 'nt',
            '.rdf': 'xml', '.xml': 'xml',
            '.jsonld': 'json-ld', '.json': 'json-ld',
            '.n3': 'n3', '.nq': 'nquads'
        }
        
        file_format = format_map.get(file_path.suffix.lower(), 'turtle')
        
        logging.info(f"Loading {file_path} as {file_format}")
        
        graph = Graph()
        graph.parse(str(file_path), format=file_format)
        
        self.metrics.total_triples = len(graph)
        logging.info(f"Loaded {self.metrics.total_triples:,} triples")
        
        return graph
    
    def _analyze_rdf_structure(self, graph: Graph):
        """Analyze RDF structure to understand schema and patterns."""
        logging.info("Analyzing RDF structure...")
        
        # Analyze namespaces
        self.namespace_analyzer.analyze_graph(graph)
        self.metrics.namespaces_found = len(self.namespace_analyzer.namespace_map)
        
        # Analyze schema
        if self.config.analyze_schema:
            self.schema_analyzer.analyze_graph(graph)
            self.metrics.discovered_classes = len(self.schema_analyzer.classes)
            self.metrics.discovered_properties = len(self.schema_analyzer.properties)
        
        # Calculate statistics
        subjects = set()
        predicates = set()
        objects = set()
        
        for s, p, o in graph:
            subjects.add(str(s))
            predicates.add(str(p))
            objects.add(str(o))
        
        self.metrics.unique_subjects = len(subjects)
        self.metrics.unique_predicates = len(predicates)
        self.metrics.unique_objects = len(objects)
        
        logging.info("RDF analysis complete:")
        logging.info(f"  Namespaces: {self.metrics.namespaces_found}")
        logging.info(f"  Classes: {self.metrics.discovered_classes}")
        logging.info(f"  Properties: {self.metrics.discovered_properties}")
        logging.info(f"  Unique subjects: {self.metrics.unique_subjects}")
        logging.info(f"  Unique predicates: {self.metrics.unique_predicates}")
    
    def _convert_rdf_data(self, graph: Graph):
        """Convert RDF triples to FalkorDB using streaming approach."""
        logging.info("Converting RDF data to FalkorDB...")
        
        chunk_count = 0
        processed_triples = 0
        
        # Process in chunks for memory efficiency
        chunk_size = self.config.chunk_size
        chunk_triples = []
        
        with tqdm(total=self.metrics.total_triples, desc="Converting triples") as pbar:
            for triple in graph:
                chunk_triples.append(triple)
                
                if len(chunk_triples) >= chunk_size:
                    self._process_triple_chunk(chunk_triples)
                    processed_triples += len(chunk_triples)
                    chunk_count += 1
                    
                    pbar.update(len(chunk_triples))
                    chunk_triples.clear()
                    
                    # Memory management
                    self._monitor_memory()
                    
                    if chunk_count % 10 == 0:
                        logging.info(f"Processed {processed_triples:,} triples in {chunk_count} chunks")
            
            # Process remaining triples
            if chunk_triples:
                self._process_triple_chunk(chunk_triples)
                processed_triples += len(chunk_triples)
                pbar.update(len(chunk_triples))
        
        self.metrics.processing_chunks = chunk_count
        logging.info(f"Completed processing {processed_triples:,} triples")
    
    def _process_triple_chunk(self, triples: List[Tuple]):
        """Process a chunk of RDF triples."""
        for subject, predicate, obj in triples:
            try:
                self.loader.load_triple(subject, predicate, obj)
            except Exception as e:
                logging.warning(f"Failed to load triple {subject} {predicate} {obj}: {e}")
    
    def _monitor_memory(self):
        """Monitor memory usage and trigger cleanup if needed."""
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > self.metrics.memory_peak_mb:
            self.metrics.memory_peak_mb = memory_mb
        
        if memory_mb > self.config.max_memory_mb:
            logging.warning(f"Memory usage ({memory_mb:.1f}MB) exceeds threshold")
            gc.collect()
    
    def _collect_final_metrics(self):
        """Collect final metrics from the loader."""
        self.metrics.nodes_created = self.loader.nodes_created
        self.metrics.edges_created = self.loader.edges_created
        self.metrics.properties_set = self.loader.properties_set
    
    def _print_conversion_summary(self):
        """Print a comprehensive summary of the conversion."""
        print("\n" + "="*80)
        print("RDF TO FALKORDB CONVERSION SUMMARY")
        print("="*80)
        
        print(f"ðŸ“ Input Statistics:")
        print(f"   Total triples: {self.metrics.total_triples:,}")
        print(f"   Unique subjects: {self.metrics.unique_subjects:,}")
        print(f"   Unique predicates: {self.metrics.unique_predicates:,}")
        print(f"   Unique objects: {self.metrics.unique_objects:,}")
        
        print(f"\nðŸ” Schema Discovery:")
        print(f"   Namespaces found: {self.metrics.namespaces_found}")
        print(f"   Classes discovered: {self.metrics.discovered_classes}")
        print(f"   Properties discovered: {self.metrics.discovered_properties}")
        
        print(f"\nðŸŽ¯ Conversion Results:")
        print(f"   Nodes created: {self.metrics.nodes_created:,}")
        print(f"   Edges created: {self.metrics.edges_created:,}")
        print(f"   Properties set: {self.metrics.properties_set:,}")
        
        print(f"\nâš¡ Performance:")
        print(f"   Processing time: {self.metrics.processing_time:.2f} seconds")
        print(f"   Peak memory: {self.metrics.memory_peak_mb:.1f} MB")
        print(f"   Processing chunks: {self.metrics.processing_chunks}")
        
        if self.metrics.processing_time > 0:
            rate = self.metrics.total_triples / self.metrics.processing_time
            print(f"   Processing rate: {rate:.0f} triples/second")
        
        print("="*80)
    
    def get_discovered_schema(self) -> Dict[str, Any]:
        """Get the discovered schema information."""
        return {
            'namespaces': dict(self.namespace_analyzer.namespace_map),
            'classes': list(self.schema_analyzer.classes),
            'properties': list(self.schema_analyzer.properties),
            'data_properties': list(self.schema_analyzer.data_properties),
            'object_properties': list(self.schema_analyzer.object_properties),
            'class_properties': {k: list(v) for k, v in self.schema_analyzer.class_properties.items()},
            'predicate_usage': dict(self.schema_analyzer.predicate_usage.most_common(20))
        }
    
    def get_validation_queries(self) -> List[str]:
        """Get validation queries based on discovered schema."""
        queries = [
            "MATCH (n) RETURN count(n) as total_nodes",
            "MATCH ()-[r]->() RETURN count(r) as total_edges",
            "MATCH (n) RETURN labels(n) as node_labels, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as edge_types, count(r) as count ORDER BY count DESC LIMIT 10",
        ]
        
        # Add class-specific queries
        for class_uri in list(self.schema_analyzer.classes)[:5]:  # Top 5 classes
            class_label = self.schema_analyzer.get_class_label(class_uri)
            queries.append(f"MATCH (n:{class_label}) RETURN count(n) as {class_label}_count")
        
        return queries


# Example usage
def main():
    """Example usage of the dynamic converter."""
    
    # Configuration for dynamic conversion
    config = ConversionConfig(
        chunk_size=50000,
        max_memory_mb=2000,
        batch_size=1000,
        host="localhost",
        port=6379,
        graph_name="dynamic_knowledge_graph",
        analyze_schema=True,
        create_indexes=True,
        normalize_labels=True
    )
    
    # Initialize converter
    converter = RDFToFalkorDBConverter(config)
    
    try:
        # Convert your TTL file
        ttl_file_path = "your_file.ttl"  # Replace with your TTL file
        
        if not Path(ttl_file_path).exists():
            print(f"Please provide a valid TTL file. Current path: {ttl_file_path}")
            return
        
        print(f"Starting dynamic conversion of {ttl_file_path}")
        
        # Perform conversion
        metrics = converter.convert_rdf_file(ttl_file_path)
        
        # Show discovered schema
        schema = converter.get_discovered_schema()
        print(f"\nðŸ“‹ Discovered Schema:")
        print(f"   Namespaces: {list(schema['namespaces'].keys())}")
        print(f"   Classes: {schema['classes'][:10]}...")  # Show first 10
        print(f"   Properties: {schema['properties'][:10]}...")  # Show first 10
        
        # Run validation
        print(f"\nâœ… Validation:")
        db = falkordb.FalkorDB(host=config.host, port=config.port)
        graph = db.select_graph(config.graph_name)
        
        for query in converter.get_validation_queries():
            try:
                result = graph.query(query)
                if result.result_set:
                    print(f"   {query}: {result.result_set[0]}")
            except Exception as e:
                print(f"   Query failed: {query} - {e}")
    
    except Exception as e:
        print(f"Conversion failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
