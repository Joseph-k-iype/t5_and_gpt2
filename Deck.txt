#!/usr/bin/env python3
"""
Complete Native ReAct Contextual Case Mapping System
===================================================

A production-ready contextual case mapping system using LangGraph's native StateGraph 
and ToolNode implementation with OpenAI's o3-mini model. This system maps cases from 
CSV files to predefined names from JSON files using sophisticated ReAct (Reasoning + Acting) 
agent architecture.

Features:
- Native LangGraph StateGraph + ToolNode ReAct implementation
- 5 specialized analysis tools for comprehensive contextual understanding
- Multi-factor confidence scoring with detailed reasoning
- Domain classification with pattern matching
- Semantic similarity analysis with multiple metrics
- Complete audit trail and transparent decision making
- Production-ready error handling and logging
- Async processing with rate limiting
- Comprehensive result generation with alternatives

Author: AI Assistant
Date: June 2025
License: MIT

Requirements:
    langchain>=0.3.18
    langchain-openai>=0.3.4
    langgraph>=0.4.7
    pandas>=2.0.0
    pydantic>=2.0.0

Usage:
    python complete_react_mapper.py --json-file definitions.json --csv-file cases.csv --output-file results.json

Environment Variables:
    OPENAI_API_KEY: Your OpenAI API key
    LANGSMITH_API_KEY: Optional LangSmith API key for tracing
"""

import json
import pandas as pd
import asyncio
import os
import sys
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Sequence, Literal, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import argparse
import logging
from datetime import datetime
import re
import traceback
from enum import Enum

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from pydantic import BaseModel, Field, validator

# LangGraph imports - native implementation
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.errors import GraphRecursionError

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('react_mapper.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Constants
MAX_RETRIES = 3
DEFAULT_TIMEOUT = 60
RATE_LIMIT_DELAY = 0.5
MAX_RECURSION_LIMIT = 25

class DomainType(Enum):
    """Enumeration of supported domain types"""
    TECHNICAL = "technical"
    CUSTOMER_SERVICE = "customer_service"
    SECURITY = "security"
    SALES = "sales"
    PRODUCT_DEVELOPMENT = "product_development"
    UNKNOWN = "unknown"

class ConfidenceLevel(Enum):
    """Enumeration of confidence levels"""
    VERY_HIGH = "very_high"
    HIGH = "high"
    MODERATE = "moderate"
    LOW = "low"
    VERY_LOW = "very_low"

# Data Models
@dataclass
class NameDefinition:
    """Represents a name with its definition from JSON file"""
    name: str
    definition: str
    
    def to_dict(self) -> Dict[str, str]:
        return asdict(self)

@dataclass
class CaseData:
    """Represents a case from CSV file"""
    id: str
    case: str
    
    def to_dict(self) -> Dict[str, str]:
        return asdict(self)

class DomainAnalysis(BaseModel):
    """Domain classification analysis results"""
    primary_domain: DomainType
    confidence: float = Field(ge=0.0, le=1.0)
    domain_scores: Dict[str, int]
    detected_patterns: Dict[str, List[str]]
    key_entities: List[str]
    urgency_level: str
    complexity_level: str
    word_count: int

class SimilarityScore(BaseModel):
    """Similarity scoring between case and definition"""
    name: str
    definition: str
    keyword_overlap: float = Field(ge=0.0, le=1.0)
    domain_match: float = Field(ge=0.0, le=1.0)
    entity_relevance: float = Field(ge=0.0, le=1.0)
    concept_score: float = Field(ge=0.0, le=1.0)
    overall_score: float = Field(ge=0.0, le=1.0)
    common_words: List[str]

class MatchResult(BaseModel):
    """Final match result with comprehensive analysis"""
    matched_name: str = Field(description="The best matching name from definitions")
    reasoning: str = Field(description="Detailed explanation of match selection")
    confidence_score: float = Field(description="Confidence score 0.0-1.0", ge=0.0, le=1.0)
    confidence_level: ConfidenceLevel
    alternative_matches: List[str] = Field(default_factory=list)
    analysis_steps: List[str] = Field(default_factory=list)
    domain_analysis: Optional[DomainAnalysis] = None
    similarity_scores: List[SimilarityScore] = Field(default_factory=list)
    processing_time: Optional[float] = None
    
    @validator('confidence_level', pre=True, always=True)
    def set_confidence_level(cls, v, values):
        if 'confidence_score' in values:
            score = values['confidence_score']
            if score >= 0.9:
                return ConfidenceLevel.VERY_HIGH
            elif score >= 0.7:
                return ConfidenceLevel.HIGH
            elif score >= 0.5:
                return ConfidenceLevel.MODERATE
            elif score >= 0.3:
                return ConfidenceLevel.LOW
            else:
                return ConfidenceLevel.VERY_LOW
        return v

# ReAct Agent State
class ReactAgentState(TypedDict):
    """Enhanced state for the native ReAct agent"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    definitions: Optional[List[NameDefinition]]
    current_case: Optional[CaseData]
    domain_analysis: Optional[DomainAnalysis]
    similarity_scores: Optional[List[SimilarityScore]]
    confidence_data: Optional[Dict[str, Any]]
    match_result: Optional[MatchResult]
    processing_context: Optional[Dict[str, Any]]
    error_count: int
    start_time: Optional[float]

class CompleteNativeReactMapper:
    """
    Complete Native ReAct Contextual Case Mapping System
    
    This class implements a sophisticated contextual case mapping system using
    LangGraph's native StateGraph and ToolNode for maximum control and customization.
    """
    
    def __init__(
        self, 
        api_key: str, 
        model_name: str = "o3-mini",
        temperature: float = 0.1,
        max_tokens: int = 2000,
        timeout: int = DEFAULT_TIMEOUT
    ):
        """
        Initialize the complete native ReAct mapper
        
        Args:
            api_key: OpenAI API key
            model_name: Model name (default: o3-mini)
            temperature: Model temperature for consistency
            max_tokens: Maximum tokens per response
            timeout: Request timeout in seconds
        """
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )
        
        # Domain classification patterns
        self.domain_patterns = {
            DomainType.TECHNICAL: [
                'bug', 'error', 'crash', 'system', 'software', 'hardware',
                'installation', 'configuration', 'database', 'server', 'api',
                'timeout', 'failure', 'not working', 'malfunction', 'glitch',
                'code', 'programming', 'debug', 'technical', 'IT', 'infrastructure'
            ],
            DomainType.CUSTOMER_SERVICE: [
                'billing', 'account', 'subscription', 'payment', 'refund',
                'cancel', 'update', 'change', 'help', 'support', 'inquiry',
                'customer', 'service', 'address', 'information', 'profile',
                'contact', 'assistance', 'question', 'issue', 'problem'
            ],
            DomainType.SECURITY: [
                'security', 'breach', 'unauthorized', 'suspicious', 'hack',
                'threat', 'vulnerability', 'access', 'login', 'password',
                'attack', 'malware', 'phishing', 'fraud', 'incident',
                'violation', 'compromise', 'authentication', 'permission'
            ],
            DomainType.SALES: [
                'pricing', 'price', 'demo', 'purchase', 'buy', 'upgrade',
                'license', 'enterprise', 'commercial', 'quote', 'proposal',
                'contract', 'deal', 'negotiation', 'sales', 'revenue',
                'discount', 'offer', 'package', 'plan', 'subscription'
            ],
            DomainType.PRODUCT_DEVELOPMENT: [
                'feature', 'improvement', 'enhancement', 'suggestion',
                'request', 'functionality', 'capability', 'development',
                'new', 'add', 'implement', 'design', 'requirement',
                'specification', 'roadmap', 'backlog', 'iteration'
            ]
        }
        
        # Processing context storage
        self.processing_context: Dict[str, Any] = {}
        
        # Create specialized tools
        self.tools = [
            self.analyze_case_domain_tool,
            self.compare_all_definitions_tool,
            self.calculate_semantic_similarity_tool,
            self.generate_confidence_score_tool,
            self.finalize_match_result_tool
        ]
        
        # Create ToolNode
        self.tool_node = ToolNode(self.tools, handle_tool_errors=True)
        
        # Bind tools to model
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # Create the native ReAct graph
        self.graph = self._create_native_react_graph()
        
        logger.info(f"Complete Native ReAct Mapper initialized with model: {model_name}")
    
    @tool
    def analyze_case_domain_tool(self, case_description: str, case_id: str = "") -> str:
        """
        Comprehensive case analysis with domain classification and context extraction.
        
        This tool performs deep analysis of case descriptions to identify:
        - Primary domain classification
        - Key entities and concepts
        - Urgency and complexity indicators
        - Contextual patterns and relationships
        
        Args:
            case_description: The case text to analyze
            case_id: Case identifier for tracking
            
        Returns:
            Detailed analysis report with domain classification and key insights
        """
        try:
            logger.info(f"🔍 Starting comprehensive case analysis for: {case_id}")
            start_time = datetime.now()
            
            case_lower = case_description.lower()
            case_words = re.findall(r'\b[a-zA-Z]{3,}\b', case_description)
            
            # Domain classification analysis
            domain_scores = {}
            detected_patterns = {}
            
            for domain_type, patterns in self.domain_patterns.items():
                score = 0
                found_patterns = []
                
                for pattern in patterns:
                    if pattern in case_lower:
                        # Weight longer, more specific patterns higher
                        weight = len(pattern) / 10 + 1
                        score += weight
                        found_patterns.append(pattern)
                
                domain_scores[domain_type.value] = round(score, 2)
                detected_patterns[domain_type.value] = found_patterns
            
            # Determine primary domain
            if max(domain_scores.values()) == 0:
                primary_domain = DomainType.UNKNOWN
                domain_confidence = 0.0
            else:
                primary_domain_str = max(domain_scores, key=domain_scores.get)
                primary_domain = DomainType(primary_domain_str)
                
                # Calculate confidence based on score distribution
                total_score = sum(domain_scores.values())
                max_score = domain_scores[primary_domain_str]
                domain_confidence = min(max_score / total_score if total_score > 0 else 0, 1.0)
            
            # Extract key entities (important words > 4 characters)
            important_entities = []
            for word in case_words:
                if len(word) > 4 and word.lower() not in ['that', 'this', 'with', 'from', 'they', 'have', 'been', 'were', 'will']:
                    important_entities.append(word)
            
            # Remove duplicates and limit
            key_entities = list(dict.fromkeys(important_entities))[:10]
            
            # Analyze urgency indicators
            urgency_patterns = ['urgent', 'critical', 'immediately', 'asap', 'emergency', 'priority', 'rush']
            urgency_level = 'high' if any(pattern in case_lower for pattern in urgency_patterns) else 'normal'
            
            # Analyze complexity
            complexity_factors = [
                len(case_words) > 20,  # Long description
                len(set(case_words)) / len(case_words) > 0.7,  # High vocabulary diversity
                any(domain_scores[d] > 2 for d in domain_scores),  # Multiple domain indicators
                len([d for d in domain_scores if domain_scores[d] > 1]) > 1  # Multi-domain case
            ]
            
            complexity_score = sum(complexity_factors) / len(complexity_factors)
            if complexity_score > 0.6:
                complexity_level = 'high'
            elif complexity_score > 0.3:
                complexity_level = 'medium'
            else:
                complexity_level = 'low'
            
            # Create domain analysis object
            domain_analysis = DomainAnalysis(
                primary_domain=primary_domain,
                confidence=round(domain_confidence, 3),
                domain_scores=domain_scores,
                detected_patterns=detected_patterns,
                key_entities=key_entities,
                urgency_level=urgency_level,
                complexity_level=complexity_level,
                word_count=len(case_words)
            )
            
            # Store in processing context
            context_key = f"{case_id}_domain_analysis"
            self.processing_context[context_key] = domain_analysis
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"✅ Domain analysis completed in {processing_time:.2f}s")
            
            # Generate comprehensive analysis report
            report = f"""
🔍 COMPREHENSIVE CASE ANALYSIS COMPLETED
==========================================
Case ID: {case_id}
Processing Time: {processing_time:.2f} seconds

PRIMARY DOMAIN CLASSIFICATION:
Domain: {primary_domain.value.title().replace('_', ' ')}
Confidence: {domain_confidence:.3f}

DOMAIN BREAKDOWN:
{chr(10).join([f"• {domain.replace('_', ' ').title()}: {score} points ({len(patterns)} patterns)" 
               for domain, score, patterns in 
               [(d, domain_scores[d], detected_patterns[d]) for d in domain_scores 
                if domain_scores[d] > 0]])}

KEY INSIGHTS:
• Primary Entities: {', '.join(key_entities[:5])}
• Urgency Level: {urgency_level.title()}
• Complexity: {complexity_level.title()}
• Word Count: {len(case_words)} words
• Vocabulary Diversity: {len(set(case_words)) / max(len(case_words), 1):.2f}

DETECTED PATTERNS:
{chr(10).join([f"• {domain.replace('_', ' ').title()}: {', '.join(patterns[:3])}" 
               for domain, patterns in detected_patterns.items() 
               if patterns])}

Analysis stored for downstream processing. Ready for definition comparison.
"""
            
            return report.strip()
            
        except Exception as e:
            error_msg = f"Error in case domain analysis: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            return error_msg
    
    @tool
    def compare_all_definitions_tool(self, case_id: str, definitions_json: str) -> str:
        """
        Systematic comparison of case against all available definitions with detailed scoring.
        
        This tool performs comprehensive similarity analysis including:
        - Keyword overlap analysis
        - Domain relevance matching
        - Entity alignment scoring
        - Conceptual similarity assessment
        
        Args:
            case_id: Case identifier for context retrieval
            definitions_json: JSON string containing all name definitions
            
        Returns:
            Detailed comparison report with similarity scores for all definitions
        """
        try:
            logger.info(f"⚖️ Starting comprehensive definition comparison for: {case_id}")
            start_time = datetime.now()
            
            # Load definitions
            definitions_data = json.loads(definitions_json)
            
            # Get domain analysis from context
            context_key = f"{case_id}_domain_analysis"
            if context_key not in self.processing_context:
                return "❌ Error: Domain analysis not found. Please run analyze_case_domain_tool first."
            
            domain_analysis: DomainAnalysis = self.processing_context[context_key]
            
            # Extract case information
            case_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', 
                                      ' '.join(domain_analysis.key_entities).lower()))
            primary_domain = domain_analysis.primary_domain
            key_entities = [entity.lower() for entity in domain_analysis.key_entities]
            
            similarity_scores = []
            
            for definition_data in definitions_data:
                name = definition_data['name']
                definition = definition_data['definition']
                
                logger.debug(f"  📊 Analyzing similarity for: {name}")
                
                # 1. Keyword Overlap Analysis
                def_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', definition.lower()))
                common_words = case_words.intersection(def_words)
                keyword_overlap = len(common_words) / max(len(case_words.union(def_words)), 1)
                
                # 2. Domain Relevance Matching
                domain_match = self._calculate_domain_match(name, definition, primary_domain)
                
                # 3. Entity Alignment Scoring
                definition_lower = definition.lower()
                entity_matches = sum(1 for entity in key_entities if entity in definition_lower)
                entity_relevance = entity_matches / max(len(key_entities), 1)
                
                # 4. Conceptual Similarity Assessment
                concept_score = self._calculate_concept_similarity(
                    case_words, def_words, common_words, keyword_overlap, domain_match
                )
                
                # 5. Overall Score Calculation (weighted combination)
                overall_score = (
                    keyword_overlap * 0.25 +      # Word similarity
                    domain_match * 0.35 +         # Domain relevance (highest weight)
                    entity_relevance * 0.25 +     # Entity alignment
                    concept_score * 0.15          # Conceptual similarity
                )
                
                # Create similarity score object
                similarity_score = SimilarityScore(
                    name=name,
                    definition=definition,
                    keyword_overlap=round(keyword_overlap, 3),
                    domain_match=round(domain_match, 3),
                    entity_relevance=round(entity_relevance, 3),
                    concept_score=round(concept_score, 3),
                    overall_score=round(overall_score, 3),
                    common_words=list(common_words)[:10]
                )
                
                similarity_scores.append(similarity_score)
            
            # Sort by overall score (highest first)
            similarity_scores.sort(key=lambda x: x.overall_score, reverse=True)
            
            # Store results for later use
            context_key = f"{case_id}_similarity_scores"
            self.processing_context[context_key] = similarity_scores
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"✅ Definition comparison completed in {processing_time:.2f}s")
            
            # Generate detailed comparison report
            report = f"""
⚖️ COMPREHENSIVE DEFINITION COMPARISON RESULTS
=============================================
Case ID: {case_id}
Processing Time: {processing_time:.2f} seconds
Definitions Analyzed: {len(similarity_scores)}

PRIMARY DOMAIN CONTEXT:
Domain: {primary_domain.value.title().replace('_', ' ')}
Key Entities: {', '.join(domain_analysis.key_entities[:5])}

TOP MATCHES (Ranked by Overall Score):
"""
            
            for i, score in enumerate(similarity_scores[:5], 1):
                report += f"""
{i}. {score.name} | Overall Score: {score.overall_score:.3f}
   Definition: {score.definition[:80]}{'...' if len(score.definition) > 80 else ''}
   
   📊 Detailed Scores:
   • Keyword Overlap: {score.keyword_overlap:.3f}
   • Domain Match: {score.domain_match:.3f}
   • Entity Relevance: {score.entity_relevance:.3f}
   • Concept Score: {score.concept_score:.3f}
   
   🔗 Common Elements: {', '.join(score.common_words[:5])}
   {'─' * 60}"""
            
            report += f"""

ANALYSIS SUMMARY:
• Best Match: {similarity_scores[0].name} (Score: {similarity_scores[0].overall_score:.3f})
• Score Range: {similarity_scores[-1].overall_score:.3f} - {similarity_scores[0].overall_score:.3f}
• Clear Leader: {'Yes' if similarity_scores[0].overall_score - similarity_scores[1].overall_score > 0.1 else 'No'}

Comparison results stored for semantic similarity analysis.
"""
            
            return report.strip()
            
        except Exception as e:
            error_msg = f"Error in definition comparison: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            return error_msg
    
    @tool
    def calculate_semantic_similarity_tool(self, case_id: str, top_match_name: str) -> str:
        """
        Advanced semantic similarity analysis for the top matching definition.
        
        Performs deep semantic analysis including:
        - Contextual relevance assessment
        - Conceptual coherence evaluation
        - Domain-specific semantic factors
        - Complexity and ambiguity analysis
        
        Args:
            case_id: Case identifier for context retrieval
            top_match_name: Name of the top matching definition to analyze
            
        Returns:
            Detailed semantic similarity analysis with contributing factors
        """
        try:
            logger.info(f"🧠 Starting semantic similarity analysis for: {case_id} -> {top_match_name}")
            start_time = datetime.now()
            
            # Get comparison results and domain analysis
            similarity_key = f"{case_id}_similarity_scores"
            domain_key = f"{case_id}_domain_analysis"
            
            if similarity_key not in self.processing_context or domain_key not in self.processing_context:
                return "❌ Error: Required analysis data not found. Please run previous analysis tools first."
            
            similarity_scores: List[SimilarityScore] = self.processing_context[similarity_key]
            domain_analysis: DomainAnalysis = self.processing_context[domain_key]
            
            # Find the specific match
            top_match = None
            for score in similarity_scores:
                if score.name == top_match_name:
                    top_match = score
                    break
            
            if not top_match:
                return f"❌ Error: Match '{top_match_name}' not found in comparison results."
            
            # Advanced semantic analysis
            semantic_factors = []
            semantic_score = 0.0
            
            # Factor 1: Domain Alignment Strength
            domain_strength = self._analyze_domain_alignment(top_match, domain_analysis)
            semantic_score += domain_strength['score']
            semantic_factors.extend(domain_strength['factors'])
            
            # Factor 2: Keyword Density and Quality
            keyword_strength = self._analyze_keyword_quality(top_match, domain_analysis)
            semantic_score += keyword_strength['score']
            semantic_factors.extend(keyword_strength['factors'])
            
            # Factor 3: Entity Relevance and Context
            entity_strength = self._analyze_entity_relevance(top_match, domain_analysis)
            semantic_score += entity_strength['score']
            semantic_factors.extend(entity_strength['factors'])
            
            # Factor 4: Conceptual Coherence
            concept_strength = self._analyze_conceptual_coherence(top_match, domain_analysis)
            semantic_score += concept_strength['score']
            semantic_factors.extend(concept_strength['factors'])
            
            # Factor 5: Complexity and Specificity Alignment
            complexity_strength = self._analyze_complexity_alignment(top_match, domain_analysis)
            semantic_score += complexity_strength['score']
            semantic_factors.extend(complexity_strength['factors'])
            
            # Factor 6: Competitive Analysis (vs other matches)
            competitive_strength = self._analyze_competitive_advantage(top_match, similarity_scores)
            semantic_score += competitive_strength['score']
            semantic_factors.extend(competitive_strength['factors'])
            
            # Normalize semantic score to [0, 1]
            semantic_score = max(0.0, min(1.0, semantic_score))
            
            # Store semantic analysis results
            semantic_analysis = {
                'case_id': case_id,
                'top_match': top_match_name,
                'semantic_score': semantic_score,
                'semantic_factors': semantic_factors,
                'detailed_analysis': {
                    'domain_strength': domain_strength,
                    'keyword_strength': keyword_strength,
                    'entity_strength': entity_strength,
                    'concept_strength': concept_strength,
                    'complexity_strength': complexity_strength,
                    'competitive_strength': competitive_strength
                },
                'base_scores': top_match.dict()
            }
            
            context_key = f"{case_id}_semantic_analysis"
            self.processing_context[context_key] = semantic_analysis
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"✅ Semantic analysis completed in {processing_time:.2f}s")
            
            # Generate comprehensive semantic analysis report
            report = f"""
🧠 ADVANCED SEMANTIC SIMILARITY ANALYSIS
=======================================
Case ID: {case_id}
Top Match: {top_match_name}
Processing Time: {processing_time:.2f} seconds

FINAL SEMANTIC SCORE: {semantic_score:.3f}

CONTRIBUTING FACTORS:
{chr(10).join([f"• {factor}" for factor in semantic_factors])}

DETAILED BREAKDOWN:
─────────────────────────────────────────

📍 Domain Alignment Analysis:
Score Contribution: +{domain_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in domain_strength['factors']])}

🔤 Keyword Quality Analysis:
Score Contribution: +{keyword_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in keyword_strength['factors']])}

🎯 Entity Relevance Analysis:
Score Contribution: +{entity_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in entity_strength['factors']])}

💡 Conceptual Coherence Analysis:
Score Contribution: +{concept_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in concept_strength['factors']])}

⚖️ Complexity Alignment Analysis:
Score Contribution: +{complexity_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in complexity_strength['factors']])}

🏆 Competitive Advantage Analysis:
Score Contribution: +{competitive_strength['score']:.3f}
{chr(10).join([f"  • {factor}" for factor in competitive_strength['factors']])}

BASE SIMILARITY METRICS:
• Keyword Overlap: {top_match.keyword_overlap:.3f}
• Domain Match: {top_match.domain_match:.3f}
• Entity Relevance: {top_match.entity_relevance:.3f}
• Concept Score: {top_match.concept_score:.3f}
• Overall Score: {top_match.overall_score:.3f}

Common Elements: {', '.join(top_match.common_words[:8])}

Semantic analysis complete. Ready for confidence score generation.
"""
            
            return report.strip()
            
        except Exception as e:
            error_msg = f"Error in semantic similarity analysis: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            return error_msg
    
    @tool
    def generate_confidence_score_tool(self, case_id: str) -> str:
        """
        Generate comprehensive confidence score with multi-factor analysis.
        
        Combines all previous analysis results to calculate a final confidence score
        considering:
        - Semantic similarity strength
        - Domain classification confidence
        - Case complexity factors
        - Competitive analysis results
        - Ambiguity and uncertainty indicators
        
        Args:
            case_id: Case identifier for context retrieval
            
        Returns:
            Final confidence score with detailed reasoning and level classification
        """
        try:
            logger.info(f"📊 Generating comprehensive confidence score for: {case_id}")
            start_time = datetime.now()
            
            # Retrieve all analysis components
            semantic_key = f"{case_id}_semantic_analysis"
            domain_key = f"{case_id}_domain_analysis"
            similarity_key = f"{case_id}_similarity_scores"
            
            required_keys = [semantic_key, domain_key, similarity_key]
            missing_keys = [key for key in required_keys if key not in self.processing_context]
            
            if missing_keys:
                return f"❌ Error: Missing analysis components: {missing_keys}. Please run all previous analysis tools."
            
            semantic_analysis = self.processing_context[semantic_key]
            domain_analysis: DomainAnalysis = self.processing_context[domain_key]
            similarity_scores: List[SimilarityScore] = self.processing_context[similarity_key]
            
            # Base confidence from semantic analysis
            base_confidence = semantic_analysis['semantic_score']
            
            # Confidence adjustment factors
            adjustments = []
            total_adjustment = 0.0
            
            # Factor 1: Domain Classification Confidence
            domain_conf_adjustment = self._calculate_domain_confidence_adjustment(domain_analysis)
            total_adjustment += domain_conf_adjustment['adjustment']
            adjustments.extend(domain_conf_adjustment['factors'])
            
            # Factor 2: Case Complexity Impact
            complexity_adjustment = self._calculate_complexity_adjustment(domain_analysis)
            total_adjustment += complexity_adjustment['adjustment']
            adjustments.extend(complexity_adjustment['factors'])
            
            # Factor 3: Competitive Analysis Impact
            competitive_adjustment = self._calculate_competitive_adjustment(similarity_scores)
            total_adjustment += competitive_adjustment['adjustment']
            adjustments.extend(competitive_adjustment['factors'])
            
            # Factor 4: Data Quality and Completeness
            quality_adjustment = self._calculate_quality_adjustment(domain_analysis, similarity_scores)
            total_adjustment += quality_adjustment['adjustment']
            adjustments.extend(quality_adjustment['factors'])
            
            # Factor 5: Ambiguity and Uncertainty Indicators
            ambiguity_adjustment = self._calculate_ambiguity_adjustment(domain_analysis, similarity_scores)
            total_adjustment += ambiguity_adjustment['adjustment']
            adjustments.extend(ambiguity_adjustment['factors'])
            
            # Calculate final confidence
            final_confidence = base_confidence + total_adjustment
            final_confidence = max(0.0, min(1.0, final_confidence))  # Clamp to [0, 1]
            
            # Determine confidence level and description
            confidence_level, description = self._get_confidence_level_description(final_confidence)
            
            # Store confidence analysis
            confidence_data = {
                'case_id': case_id,
                'final_confidence': final_confidence,
                'base_confidence': base_confidence,
                'total_adjustment': total_adjustment,
                'adjustments': adjustments,
                'confidence_level': confidence_level,
                'description': description,
                'detailed_factors': {
                    'domain_confidence': domain_conf_adjustment,
                    'complexity': complexity_adjustment,
                    'competitive': competitive_adjustment,
                    'quality': quality_adjustment,
                    'ambiguity': ambiguity_adjustment
                }
            }
            
            context_key = f"{case_id}_confidence_analysis"
            self.processing_context[context_key] = confidence_data
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"✅ Confidence analysis completed in {processing_time:.2f}s")
            
            # Generate comprehensive confidence analysis report
            report = f"""
📊 COMPREHENSIVE CONFIDENCE SCORE ANALYSIS
=========================================
Case ID: {case_id}
Processing Time: {processing_time:.2f} seconds

FINAL CONFIDENCE SCORE: {final_confidence:.3f}
Confidence Level: {confidence_level.value.replace('_', ' ').title()}
Assessment: {description}

SCORE BREAKDOWN:
Base Semantic Score: {base_confidence:.3f}
Total Adjustments: {total_adjustment:+.3f}
Final Score: {final_confidence:.3f}

ADJUSTMENT FACTORS APPLIED:
{chr(10).join([f"• {factor}" for factor in adjustments])}

DETAILED FACTOR ANALYSIS:
─────────────────────────────────────────

🎯 Domain Classification Confidence:
Adjustment: {domain_conf_adjustment['adjustment']:+.3f}
{chr(10).join([f"  • {factor}" for factor in domain_conf_adjustment['factors']])}

⚙️ Case Complexity Impact:
Adjustment: {complexity_adjustment['adjustment']:+.3f}
{chr(10).join([f"  • {factor}" for factor in complexity_adjustment['factors']])}

🏆 Competitive Analysis Impact:
Adjustment: {competitive_adjustment['adjustment']:+.3f}
{chr(10).join([f"  • {factor}" for factor in competitive_adjustment['factors']])}

📈 Data Quality Assessment:
Adjustment: {quality_adjustment['adjustment']:+.3f}
{chr(10).join([f"  • {factor}" for factor in quality_adjustment['factors']])}

❓ Ambiguity Analysis:
Adjustment: {ambiguity_adjustment['adjustment']:+.3f}
{chr(10).join([f"  • {factor}" for factor in ambiguity_adjustment['factors']])}

CONFIDENCE INTERPRETATION:
{self._get_detailed_confidence_interpretation(final_confidence, confidence_level)}

Confidence analysis complete. Ready for final match result generation.
"""
            
            return report.strip()
            
        except Exception as e:
            error_msg = f"Error in confidence score generation: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            return error_msg
    
    @tool
    def finalize_match_result_tool(self, case_id: str, selected_match: str) -> str:
        """
        Finalize the complete match result with comprehensive analysis and alternatives.
        
        Creates the final match result combining all analysis components:
        - Complete reasoning chain
        - Alternative matches with explanations
        - Analysis steps and methodology
        - Performance metrics and timing
        
        Args:
            case_id: Case identifier for context retrieval
            selected_match: The final selected match name
            
        Returns:
            Complete match result confirmation with all analysis details
        """
        try:
            logger.info(f"✅ Finalizing comprehensive match result for: {case_id}")
            start_time = datetime.now()
            
            # Retrieve all analysis components
            confidence_key = f"{case_id}_confidence_analysis"
            semantic_key = f"{case_id}_semantic_analysis"
            similarity_key = f"{case_id}_similarity_scores"
            domain_key = f"{case_id}_domain_analysis"
            
            required_keys = [confidence_key, semantic_key, similarity_key, domain_key]
            missing_keys = [key for key in required_keys if key not in self.processing_context]
            
            if missing_keys:
                return f"❌ Error: Missing analysis components: {missing_keys}. Please run all previous analysis tools first."
            
            confidence_data = self.processing_context[confidence_key]
            semantic_analysis = self.processing_context[semantic_key]
            similarity_scores: List[SimilarityScore] = self.processing_context[similarity_key]
            domain_analysis: DomainAnalysis = self.processing_context[domain_key]
            
            # Generate comprehensive reasoning
            reasoning = self._generate_comprehensive_reasoning(
                selected_match, confidence_data, semantic_analysis, domain_analysis, similarity_scores
            )
            
            # Get alternative matches (top 4 excluding the selected one)
            alternatives = [
                score.name for score in similarity_scores 
                if score.name != selected_match
            ][:4]
            
            # Generate detailed analysis steps
            analysis_steps = [
                "1. Performed comprehensive case domain classification with pattern matching",
                "2. Executed systematic comparison against all available definitions",
                "3. Conducted advanced semantic similarity analysis with multiple factors",
                "4. Generated multi-factor confidence score with adjustment analysis",
                "5. Finalized match result with complete reasoning and alternatives"
            ]
            
            # Calculate total processing time
            case_start_time = self.processing_context.get(f"{case_id}_start_time")
            total_processing_time = (datetime.now() - datetime.fromtimestamp(case_start_time)).total_seconds() if case_start_time else None
            
            # Create comprehensive match result
            match_result = MatchResult(
                matched_name=selected_match,
                reasoning=reasoning,
                confidence_score=confidence_data['final_confidence'],
                confidence_level=confidence_data['confidence_level'],
                alternative_matches=alternatives,
                analysis_steps=analysis_steps,
                domain_analysis=domain_analysis,
                similarity_scores=similarity_scores,
                processing_time=total_processing_time
            )
            
            # Store final result
            final_result = {
                "case_id": case_id,
                "case_description": getattr(self.processing_context.get(f"{case_id}_case_data"), 'case', ''),
                "matched_name": match_result.matched_name,
                "reasoning": match_result.reasoning,
                "confidence_score": match_result.confidence_score,
                "confidence_level": match_result.confidence_level.value,
                "alternative_matches": match_result.alternative_matches,
                "analysis_steps": match_result.analysis_steps,
                "domain_analysis": domain_analysis.dict() if domain_analysis else None,
                "similarity_scores": [score.dict() for score in similarity_scores],
                "processing_time": total_processing_time,
                "status": "success",
                "timestamp": datetime.now().isoformat()
            }
            
            context_key = f"{case_id}_final_result"
            self.processing_context[context_key] = final_result
            
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"✅ Match finalization completed in {processing_time:.2f}s")
            
            # Generate final confirmation report
            report = f"""
✅ COMPLETE MATCH RESULT FINALIZED
=================================
Case ID: {case_id}
Selected Match: {selected_match}
Final Confidence: {confidence_data['final_confidence']:.3f} ({confidence_data['confidence_level'].value.replace('_', ' ').title()})
Total Processing Time: {total_processing_time:.2f}s

📋 COMPREHENSIVE REASONING:
{reasoning}

🎯 ALTERNATIVE MATCHES CONSIDERED:
{chr(10).join([f"{i+1}. {alt}" for i, alt in enumerate(alternatives)])}

📊 KEY ANALYSIS RESULTS:
• Primary Domain: {domain_analysis.primary_domain.value.replace('_', ' ').title()}
• Domain Confidence: {domain_analysis.confidence:.3f}
• Semantic Score: {semantic_analysis['semantic_score']:.3f}
• Top Similarity Score: {similarity_scores[0].overall_score:.3f}
• Case Complexity: {domain_analysis.complexity_level.title()}

🔄 ANALYSIS METHODOLOGY:
{chr(10).join(analysis_steps)}

📈 PERFORMANCE METRICS:
• Total Processing Time: {total_processing_time:.2f} seconds
• Definitions Analyzed: {len(similarity_scores)}
• Semantic Factors: {len(semantic_analysis['semantic_factors'])}
• Confidence Adjustments: {len(confidence_data['adjustments'])}

✅ FINAL STATUS: SUCCESS
Match result has been stored and is ready for output.
All analysis components completed successfully.
"""
            
            return report.strip()
            
        except Exception as e:
            error_msg = f"Error in match result finalization: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            return error_msg
    
    def _create_native_react_graph(self) -> StateGraph:
        """Create the native ReAct graph using StateGraph and ToolNode with enhanced control"""
        
        def call_model(state: ReactAgentState) -> ReactAgentState:
            """Enhanced model calling node with error handling and context management"""
            try:
                messages = state["messages"]
                
                # Add error tracking
                error_count = state.get("error_count", 0)
                if error_count >= MAX_RETRIES:
                    error_msg = f"Maximum retries ({MAX_RETRIES}) exceeded"
                    logger.error(f"❌ {error_msg}")
                    return {
                        "messages": [AIMessage(content=f"Error: {error_msg}")],
                        "error_count": error_count + 1
                    }
                
                # Call the model with tools
                response = self.llm_with_tools.invoke(messages)
                
                return {"messages": [response]}
                
            except Exception as e:
                error_count = state.get("error_count", 0) + 1
                error_msg = f"Error in model call: {str(e)}"
                logger.error(f"❌ {error_msg}")
                
                return {
                    "messages": [AIMessage(content=error_msg)],
                    "error_count": error_count
                }
        
        def should_continue(state: ReactAgentState) -> Literal["tools", "end"]:
            """Enhanced conditional edge with error handling and loop detection"""
            try:
                messages = state["messages"]
                
                if not messages:
                    return "end"
                
                last_message = messages[-1]
                
                # Check for errors
                error_count = state.get("error_count", 0)
                if error_count >= MAX_RETRIES:
                    logger.warning(f"⚠️ Ending due to maximum retries exceeded")
                    return "end"
                
                # Check for tool calls
                if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                    logger.debug(f"🛠️ Tool calls detected: {len(last_message.tool_calls)}")
                    return "tools"
                
                # End if no tool calls
                logger.debug("🏁 No tool calls detected, ending workflow")
                return "end"
                
            except Exception as e:
                logger.error(f"❌ Error in should_continue: {str(e)}")
                return "end"
        
        # Create the StateGraph with enhanced error handling
        workflow = StateGraph(ReactAgentState)
        
        # Add nodes
        workflow.add_node("agent", call_model)
        workflow.add_node("tools", self.tool_node)
        
        # Set entry point
        workflow.set_entry_point("agent")
        
        # Add conditional edge with error handling
        workflow.add_conditional_edges(
            "agent",
            should_continue,
            {
                "tools": "tools",
                "end": END
            }
        )
        
        # Add edge from tools back to agent
        workflow.add_edge("tools", "agent")
        
        # Compile with enhanced memory and configuration
        memory = MemorySaver()
        
        return workflow.compile(
            checkpointer=memory,
            debug=False  # Set to True for detailed debugging
        )
    
    # Helper methods for analysis
    def _calculate_domain_match(self, name: str, definition: str, primary_domain: DomainType) -> float:
        """Calculate domain relevance score between case and definition"""
        name_lower = name.lower()
        definition_lower = definition.lower()
        
        # Direct domain keyword matching
        domain_keywords = {
            DomainType.TECHNICAL: ['technical', 'support', 'tech', 'system', 'software', 'IT'],
            DomainType.CUSTOMER_SERVICE: ['customer', 'service', 'support', 'account', 'billing'],
            DomainType.SECURITY: ['security', 'incident', 'threat', 'breach', 'access'],
            DomainType.SALES: ['sales', 'inquiry', 'pricing', 'commercial', 'revenue'],
            DomainType.PRODUCT_DEVELOPMENT: ['product', 'development', 'feature', 'enhancement']
        }
        
        if primary_domain in domain_keywords:
            keywords = domain_keywords[primary_domain]
            name_matches = sum(1 for keyword in keywords if keyword in name_lower)
            definition_matches = sum(1 for keyword in keywords if keyword in definition_lower)
            
            total_matches = name_matches + definition_matches
            max_possible = len(keywords) * 2
            
            return min(total_matches / max_possible * 2, 1.0)  # Amplify strong matches
        
        return 0.0
    
    def _calculate_concept_similarity(self, case_words: set, def_words: set, 
                                    common_words: set, keyword_overlap: float, 
                                    domain_match: float) -> float:
        """Calculate conceptual similarity beyond simple word matching"""
        concept_score = 0.0
        
        # Factor 1: Vocabulary richness
        if len(common_words) > 3:
            concept_score += 0.2
        elif len(common_words) > 1:
            concept_score += 0.1
        
        # Factor 2: Word overlap quality
        if keyword_overlap > 0.3:
            concept_score += 0.3
        elif keyword_overlap > 0.15:
            concept_score += 0.2
        
        # Factor 3: Domain coherence
        if domain_match > 0.7:
            concept_score += 0.4
        elif domain_match > 0.3:
            concept_score += 0.2
        
        # Factor 4: Vocabulary diversity vs overlap balance
        union_size = len(case_words.union(def_words))
        if union_size > 0:
            diversity_balance = len(common_words) / union_size
            if 0.1 <= diversity_balance <= 0.4:  # Sweet spot
                concept_score += 0.1
        
        return min(concept_score, 1.0)
    
    def _analyze_domain_alignment(self, match: SimilarityScore, 
                                domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Analyze domain alignment strength"""
        factors = []
        score = 0.0
        
        if match.domain_match > 0.8:
            factors.append("Exceptional domain alignment (+0.25)")
            score += 0.25
        elif match.domain_match > 0.6:
            factors.append("Strong domain alignment (+0.20)")
            score += 0.20
        elif match.domain_match > 0.3:
            factors.append("Moderate domain alignment (+0.10)")
            score += 0.10
        else:
            factors.append("Weak domain alignment (+0.00)")
        
        # Check for domain confidence reinforcement
        if domain_analysis.confidence > 0.7 and match.domain_match > 0.5:
            factors.append("High domain classification confidence reinforces match (+0.05)")
            score += 0.05
        
        return {"score": score, "factors": factors}
    
    def _analyze_keyword_quality(self, match: SimilarityScore, 
                                domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Analyze keyword overlap quality and relevance"""
        factors = []
        score = 0.0
        
        if match.keyword_overlap > 0.4:
            factors.append("Exceptional keyword overlap (+0.20)")
            score += 0.20
        elif match.keyword_overlap > 0.25:
            factors.append("Strong keyword overlap (+0.15)")
            score += 0.15
        elif match.keyword_overlap > 0.1:
            factors.append("Moderate keyword overlap (+0.08)")
            score += 0.08
        else:
            factors.append("Limited keyword overlap (+0.00)")
        
        # Quality of common words
        if len(match.common_words) > 5:
            factors.append("Rich vocabulary overlap (+0.05)")
            score += 0.05
        elif len(match.common_words) > 2:
            factors.append("Good vocabulary overlap (+0.03)")
            score += 0.03
        
        return {"score": score, "factors": factors}
    
    def _analyze_entity_relevance(self, match: SimilarityScore, 
                                 domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Analyze entity relevance and alignment"""
        factors = []
        score = 0.0
        
        if match.entity_relevance > 0.6:
            factors.append("High entity relevance (+0.15)")
            score += 0.15
        elif match.entity_relevance > 0.3:
            factors.append("Moderate entity relevance (+0.10)")
            score += 0.10
        elif match.entity_relevance > 0.1:
            factors.append("Some entity relevance (+0.05)")
            score += 0.05
        else:
            factors.append("Limited entity relevance (+0.00)")
        
        return {"score": score, "factors": factors}
    
    def _analyze_conceptual_coherence(self, match: SimilarityScore, 
                                     domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Analyze conceptual coherence and logical consistency"""
        factors = []
        score = 0.0
        
        if match.concept_score > 0.8:
            factors.append("Exceptional conceptual coherence (+0.20)")
            score += 0.20
        elif match.concept_score > 0.6:
            factors.append("Strong conceptual coherence (+0.15)")
            score += 0.15
        elif match.concept_score > 0.4:
            factors.append("Moderate conceptual coherence (+0.08)")
            score += 0.08
        else:
            factors.append("Limited conceptual coherence (+0.00)")
        
        return {"score": score, "factors": factors}
    
    def _analyze_complexity_alignment(self, match: SimilarityScore, 
                                     domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Analyze complexity and specificity alignment"""
        factors = []
        score = 0.0
        
        complexity = domain_analysis.complexity_level
        
        # High complexity cases should match technical or specialized categories
        if complexity == 'high':
            if 'technical' in match.name.lower() or 'development' in match.name.lower():
                factors.append("High complexity case matches specialized category (+0.10)")
                score += 0.10
            else:
                factors.append("High complexity case with non-specialized match (-0.05)")
                score -= 0.05
        
        # Low complexity cases should match general service categories
        elif complexity == 'low':
            if 'service' in match.name.lower() or 'inquiry' in match.name.lower():
                factors.append("Low complexity case matches general service category (+0.05)")
                score += 0.05
        
        return {"score": score, "factors": factors}
    
    def _analyze_competitive_advantage(self, top_match: SimilarityScore, 
                                      all_matches: List[SimilarityScore]) -> Dict[str, Any]:
        """Analyze competitive advantage vs other potential matches"""
        factors = []
        score = 0.0
        
        if len(all_matches) < 2:
            return {"score": 0.0, "factors": ["Insufficient alternatives for comparison"]}
        
        # Calculate margin over second best
        second_best = all_matches[1]
        margin = top_match.overall_score - second_best.overall_score
        
        if margin > 0.2:
            factors.append("Clear leader with significant margin (+0.15)")
            score += 0.15
        elif margin > 0.1:
            factors.append("Moderate advantage over alternatives (+0.10)")
            score += 0.10
        elif margin > 0.05:
            factors.append("Slight advantage over alternatives (+0.05)")
            score += 0.05
        else:
            factors.append("Very close competition - reduces confidence (-0.05)")
            score -= 0.05
        
        # Check for multiple strong alternatives
        strong_alternatives = len([m for m in all_matches[1:] if m.overall_score > 0.6])
        if strong_alternatives >= 3:
            factors.append("Multiple strong alternatives present (-0.05)")
            score -= 0.05
        
        return {"score": score, "factors": factors}
    
    def _calculate_domain_confidence_adjustment(self, domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Calculate domain classification confidence adjustment"""
        factors = []
        adjustment = 0.0
        
        domain_conf = domain_analysis.confidence
        
        if domain_conf > 0.8:
            factors.append("Very high domain classification confidence (+0.10)")
            adjustment += 0.10
        elif domain_conf > 0.6:
            factors.append("High domain classification confidence (+0.05)")
            adjustment += 0.05
        elif domain_conf > 0.4:
            factors.append("Moderate domain classification confidence (+0.02)")
            adjustment += 0.02
        elif domain_conf > 0.2:
            factors.append("Low domain classification confidence (-0.03)")
            adjustment -= 0.03
        else:
            factors.append("Very low domain classification confidence (-0.08)")
            adjustment -= 0.08
        
        return {"adjustment": adjustment, "factors": factors}
    
    def _calculate_complexity_adjustment(self, domain_analysis: DomainAnalysis) -> Dict[str, Any]:
        """Calculate case complexity impact adjustment"""
        factors = []
        adjustment = 0.0
        
        complexity = domain_analysis.complexity_level
        urgency = domain_analysis.urgency_level
        
        if complexity == 'high':
            factors.append("High complexity case - increased analysis depth (+0.05)")
            adjustment += 0.05
        elif complexity == 'low':
            factors.append("Low complexity case - straightforward classification (+0.02)")
            adjustment += 0.02
        
        if urgency == 'high':
            factors.append("High urgency indicator - clear intent (+0.03)")
            adjustment += 0.03
        
        # Word count impact
        word_count = domain_analysis.word_count
        if word_count > 25:
            factors.append("Detailed case description provides rich context (+0.05)")
            adjustment += 0.05
        elif word_count < 8:
            factors.append("Brief case description limits analysis confidence (-0.05)")
            adjustment -= 0.05
        
        return {"adjustment": adjustment, "factors": factors}
    
    def _calculate_competitive_adjustment(self, similarity_scores: List[SimilarityScore]) -> Dict[str, Any]:
        """Calculate competitive landscape adjustment"""
        factors = []
        adjustment = 0.0
        
        if len(similarity_scores) < 2:
            return {"adjustment": 0.0, "factors": ["Insufficient data for competitive analysis"]}
        
        top_score = similarity_scores[0].overall_score
        score_distribution = [s.overall_score for s in similarity_scores]
        
        # Check for clear winner
        if top_score > 0.8:
            factors.append("Exceptionally strong top match (+0.08)")
            adjustment += 0.08
        elif top_score > 0.6:
            factors.append("Strong top match (+0.05)")
            adjustment += 0.05
        
        # Analyze score distribution
        std_dev = (sum([(s - sum(score_distribution)/len(score_distribution))**2 for s in score_distribution])/len(score_distribution))**0.5
        
        if std_dev > 0.15:
            factors.append("Clear differentiation between options (+0.05)")
            adjustment += 0.05
        elif std_dev < 0.05:
            factors.append("Very similar scores across options - ambiguous (-0.08)")
            adjustment -= 0.08
        
        return {"adjustment": adjustment, "factors": factors}
    
    def _calculate_quality_adjustment(self, domain_analysis: DomainAnalysis, 
                                     similarity_scores: List[SimilarityScore]) -> Dict[str, Any]:
        """Calculate data quality and completeness adjustment"""
        factors = []
        adjustment = 0.0
        
        # Entity extraction quality
        entity_count = len(domain_analysis.key_entities)
        if entity_count > 6:
            factors.append("Rich entity extraction - comprehensive analysis (+0.03)")
            adjustment += 0.03
        elif entity_count < 3:
            factors.append("Limited entity extraction - reduced context (-0.03)")
            adjustment -= 0.03
        
        # Pattern detection quality
        total_patterns = sum(len(patterns) for patterns in domain_analysis.detected_patterns.values())
        if total_patterns > 5:
            factors.append("Strong pattern detection - clear indicators (+0.03)")
            adjustment += 0.03
        elif total_patterns < 2:
            factors.append("Weak pattern detection - unclear classification (-0.05)")
            adjustment -= 0.05
        
        # Definition comparison completeness
        if len(similarity_scores) >= 3:
            factors.append("Comprehensive definition comparison (+0.02)")
            adjustment += 0.02
        
        return {"adjustment": adjustment, "factors": factors}
    
    def _calculate_ambiguity_adjustment(self, domain_analysis: DomainAnalysis, 
                                       similarity_scores: List[SimilarityScore]) -> Dict[str, Any]:
        """Calculate ambiguity and uncertainty adjustment"""
        factors = []
        adjustment = 0.0
        
        # Multi-domain indicators (potential ambiguity)
        high_scoring_domains = [d for d, score in domain_analysis.domain_scores.items() if score > 2]
        
        if len(high_scoring_domains) > 2:
            factors.append("Multiple domain indicators - increased ambiguity (-0.08)")
            adjustment -= 0.08
        elif len(high_scoring_domains) == 1:
            factors.append("Clear single domain focus - reduced ambiguity (+0.05)")
            adjustment += 0.05
        
        # Check for contradictory signals
        if domain_analysis.primary_domain == DomainType.UNKNOWN:
            factors.append("Unknown domain classification - high uncertainty (-0.10)")
            adjustment -= 0.10
        
        # Score clustering (multiple similar options)
        if len(similarity_scores) >= 3:
            top_three_scores = [s.overall_score for s in similarity_scores[:3]]
            score_range = max(top_three_scores) - min(top_three_scores)
            
            if score_range < 0.1:
                factors.append("Top options very similar - decision ambiguity (-0.05)")
                adjustment -= 0.05
        
        return {"adjustment": adjustment, "factors": factors}
    
    def _get_confidence_level_description(self, confidence_score: float) -> tuple:
        """Get confidence level and description based on score"""
        if confidence_score >= 0.9:
            return ConfidenceLevel.VERY_HIGH, "Extremely confident in this match with strong supporting evidence"
        elif confidence_score >= 0.7:
            return ConfidenceLevel.HIGH, "Confident in this match with good supporting evidence"
        elif confidence_score >= 0.5:
            return ConfidenceLevel.MODERATE, "Reasonably confident in this match with adequate evidence"
        elif confidence_score >= 0.3:
            return ConfidenceLevel.LOW, "Some uncertainty in this match due to limited evidence"
        else:
            return ConfidenceLevel.VERY_LOW, "Significant uncertainty in this match with weak evidence"
    
    def _get_detailed_confidence_interpretation(self, score: float, level: ConfidenceLevel) -> str:
        """Get detailed interpretation of confidence score"""
        interpretations = {
            ConfidenceLevel.VERY_HIGH: 
                "This match is exceptionally strong with multiple supporting factors. "
                "The case clearly aligns with the selected category across semantic, "
                "domain, and contextual dimensions. High reliability for automated processing.",
            
            ConfidenceLevel.HIGH:
                "This match shows strong alignment with good supporting evidence. "
                "The case demonstrates clear relevance to the selected category with "
                "minor ambiguities. Suitable for automated processing with monitoring.",
            
            ConfidenceLevel.MODERATE:
                "This match shows reasonable alignment with adequate supporting evidence. "
                "Some uncertainty exists but the selected category represents the best "
                "available option. Consider human review for critical applications.",
            
            ConfidenceLevel.LOW:
                "This match has limited supporting evidence with notable uncertainty. "
                "The selected category is the best available option but significant "
                "ambiguity exists. Human review recommended.",
            
            ConfidenceLevel.VERY_LOW:
                "This match has very weak supporting evidence with high uncertainty. "
                "The classification may be unreliable and should be flagged for "
                "human review or additional context gathering."
        }
        
        return interpretations.get(level, "Unable to provide detailed interpretation.")
    
    def _generate_comprehensive_reasoning(self, selected_match: str, confidence_data: Dict,
                                         semantic_analysis: Dict, domain_analysis: DomainAnalysis,
                                         similarity_scores: List[SimilarityScore]) -> str:
        """Generate comprehensive reasoning for the match selection"""
        
        reasoning_parts = []
        
        # Opening statement
        reasoning_parts.append(f"Selected '{selected_match}' as the optimal match for this case.")
        
        # Confidence-based assessment
        conf_level = confidence_data['confidence_level']
        if conf_level in [ConfidenceLevel.VERY_HIGH, ConfidenceLevel.HIGH]:
            reasoning_parts.append("This is a strong match with high confidence due to clear semantic alignment and robust contextual relevance.")
        elif conf_level == ConfidenceLevel.MODERATE:
            reasoning_parts.append("This is a reasonable match with moderate confidence showing good contextual fit with some minor uncertainties.")
        else:
            reasoning_parts.append("This represents the best available match though confidence is limited due to ambiguous contextual signals.")
        
        # Domain analysis reasoning
        primary_domain = domain_analysis.primary_domain.value.replace('_', ' ').title()
        reasoning_parts.append(f"The case was classified as primarily '{primary_domain}' domain with {domain_analysis.confidence:.2f} confidence based on comprehensive pattern analysis.")
        
        # Similarity analysis reasoning
        selected_similarity = next((s for s in similarity_scores if s.name == selected_match), None)
        if selected_similarity:
            if selected_similarity.keyword_overlap > 0.25:
                reasoning_parts.append(f"Strong keyword overlap ({selected_similarity.keyword_overlap:.2f}) provides solid lexical support for this classification.")
            
            if selected_similarity.domain_match > 0.5:
                reasoning_parts.append("Direct domain terminology alignment reinforces this match selection.")
            
            if selected_similarity.entity_relevance > 0.3:
                reasoning_parts.append("Key entity alignment supports the contextual relevance of this classification.")
        
        # Competitive analysis
        if len(similarity_scores) > 1:
            margin = similarity_scores[0].overall_score - similarity_scores[1].overall_score
            if margin > 0.15:
                reasoning_parts.append("This option significantly outperformed alternative classifications.")
            elif margin > 0.05:
                reasoning_parts.append("This option showed moderate advantage over alternative classifications.")
            else:
                reasoning_parts.append("This option emerged as the best choice among closely competing alternatives.")
        
        # Methodology note
        reasoning_parts.append("The analysis employed multi-dimensional semantic similarity assessment, domain-specific pattern matching, and comprehensive contextual evaluation to ensure optimal classification accuracy.")
        
        return " ".join(reasoning_parts)
    
    # Main processing methods
    def load_json_data(self, json_file_path: str) -> List[NameDefinition]:
        """Load and validate name definitions from JSON file"""
        try:
            file_path = Path(json_file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"JSON file not found: {json_file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            # Handle different JSON structures
            definitions = []
            if isinstance(data, list):
                for item in data:
                    if not isinstance(item, dict) or 'name' not in item or 'definition' not in item:
                        raise ValueError("Each item must be a dictionary with 'name' and 'definition' keys")
                    definitions.append(NameDefinition(name=item["name"], definition=item["definition"]))
            
            elif isinstance(data, dict):
                if "name" in data and "definition" in data:
                    definitions.append(NameDefinition(name=data["name"], definition=data["definition"]))
                else:
                    # Assume keys are names and values are definitions
                    for name, definition in data.items():
                        if not isinstance(definition, str):
                            raise ValueError("Definition values must be strings")
                        definitions.append(NameDefinition(name=name, definition=definition))
            else:
                raise ValueError("JSON must be a list of objects or a single object/dictionary")
            
            if not definitions:
                raise ValueError("No valid definitions found in JSON file")
            
            logger.info(f"✅ Successfully loaded {len(definitions)} name definitions from {json_file_path}")
            return definitions
                
        except Exception as e:
            logger.error(f"❌ Error loading JSON file {json_file_path}: {str(e)}")
            raise
    
    def load_csv_data(self, csv_file_path: str) -> List[CaseData]:
        """Load and validate cases from CSV file"""
        try:
            file_path = Path(csv_file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"CSV file not found: {csv_file_path}")
            
            df = pd.read_csv(file_path)
            
            if df.empty:
                raise ValueError("CSV file is empty")
            
            # Validate required columns
            required_columns = ["id", "case"]
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
            
            # Check for empty values
            if df['id'].isnull().any() or df['case'].isnull().any():
                raise ValueError("ID and case columns cannot contain null values")
            
            # Convert to CaseData objects
            cases = []
            for index, row in df.iterrows():
                case_id = str(row["id"]).strip()
                case_text = str(row["case"]).strip()
                
                if not case_id or not case_text:
                    logger.warning(f"⚠️ Skipping row {index + 1} with empty ID or case")
                    continue
                
                cases.append(CaseData(id=case_id, case=case_text))
            
            if not cases:
                raise ValueError("No valid cases found in CSV file")
            
            logger.info(f"✅ Successfully loaded {len(cases)} cases from {csv_file_path}")
            return cases
            
        except Exception as e:
            logger.error(f"❌ Error loading CSV file {csv_file_path}: {str(e)}")
            raise
    
    async def process_single_case_with_native_react(
        self, 
        case_data: CaseData, 
        definitions: List[NameDefinition]
    ) -> Dict[str, Any]:
        """Process a single case using the native ReAct agent with comprehensive error handling"""
        
        case_start_time = datetime.now()
        
        try:
            logger.info(f"🚀 Starting native ReAct processing for case: {case_data.id}")
            
            # Store case start time for performance tracking
            self.processing_context[f"{case_data.id}_start_time"] = case_start_time.timestamp()
            self.processing_context[f"{case_data.id}_case_data"] = case_data
            
            # Prepare definitions as JSON string
            definitions_json = json.dumps([
                {"name": nd.name, "definition": nd.definition} 
                for nd in definitions
            ])
            
            # Create comprehensive system prompt
            system_prompt = """You are an expert contextual analyst using a systematic native ReAct (Reasoning + Acting) approach.

You have access to 5 specialized analysis tools that must be used in sequence:

1. analyze_case_domain_tool - Performs comprehensive case analysis with domain classification
2. compare_all_definitions_tool - Executes systematic comparison against all definitions  
3. calculate_semantic_similarity_tool - Conducts advanced semantic analysis for top match
4. generate_confidence_score_tool - Generates multi-factor confidence scores with reasoning
5. finalize_match_result_tool - Creates complete match result with alternatives

IMPORTANT: You must work through ALL tools systematically. Each tool builds on the previous analysis.

Think carefully before each action. Use the tools in sequence to build a comprehensive analysis that culminates in a confident, well-reasoned match decision."""
            
            # Create detailed task prompt
            task_prompt = f"""
Please analyze and match the following case to the most appropriate name definition using the systematic native ReAct approach:

CASE DETAILS:
Case ID: {case_data.id}
Case Description: "{case_data.case}"

AVAILABLE DEFINITIONS:
{definitions_json}

SYSTEMATIC PROCESS:
1. First, use analyze_case_domain_tool to perform comprehensive case analysis
2. Then, use compare_all_definitions_tool for systematic comparison
3. Next, use calculate_semantic_similarity_tool for the top match
4. Then, use generate_confidence_score_tool for confidence analysis
5. Finally, use finalize_match_result_tool to create the complete result

Work through each step methodically. Think through your approach before taking each action.
Be thorough and systematic in your analysis.
"""
            
            # Configure the agent execution
            config = {
                "configurable": {
                    "thread_id": f"native_case_{case_data.id}_{int(case_start_time.timestamp())}"
                },
                "recursion_limit": MAX_RECURSION_LIMIT
            }
            
            # Create initial state
            initial_state = ReactAgentState(
                messages=[
                    SystemMessage(content=system_prompt), 
                    HumanMessage(content=task_prompt)
                ],
                definitions=definitions,
                current_case=case_data,
                domain_analysis=None,
                similarity_scores=None,
                confidence_data=None,
                match_result=None,
                processing_context=None,
                error_count=0,
                start_time=case_start_time.timestamp()
            )
            
            # Run the native ReAct agent with timeout and error handling
            logger.info(f"🤖 Executing native ReAct agent for case {case_data.id}")
            
            try:
                result = await asyncio.wait_for(
                    self.graph.ainvoke(initial_state, config=config),
                    timeout=DEFAULT_TIMEOUT
                )
            except asyncio.TimeoutError:
                raise TimeoutError(f"Processing timeout after {DEFAULT_TIMEOUT} seconds")
            except GraphRecursionError as e:
                raise RuntimeError(f"Graph recursion limit exceeded: {str(e)}")
            
            # Check if we have a final result stored
            result_key = f"{case_data.id}_final_result"
            if result_key in self.processing_context:
                final_result = self.processing_context[result_key]
                
                processing_time = (datetime.now() - case_start_time).total_seconds()
                final_result["total_processing_time"] = processing_time
                
                logger.info(f"✅ Successfully processed case {case_data.id} in {processing_time:.2f}s with confidence {final_result.get('confidence_score', 0):.3f}")
                
                # Cleanup case-specific context to save memory
                self._cleanup_case_context(case_data.id)
                
                return final_result
            else:
                # Fallback: extract information from agent messages
                logger.warning(f"⚠️ No final result found for case {case_data.id}, extracting from agent response")
                
                final_message = result["messages"][-1].content if result["messages"] else "No response"
                processing_time = (datetime.now() - case_start_time).total_seconds()
                
                return {
                    "case_id": case_data.id,
                    "case_description": case_data.case,
                    "status": "partial_completion",
                    "agent_response": final_message,
                    "reasoning": "Native ReAct agent completed but final result not properly stored",
                    "confidence_score": 0.3,  # Low confidence for incomplete processing
                    "confidence_level": "low",
                    "alternative_matches": [],
                    "analysis_steps": ["Partial ReAct agent execution"],
                    "processing_time": processing_time,
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            processing_time = (datetime.now() - case_start_time).total_seconds()
            error_msg = f"Error processing case {case_data.id}: {str(e)}"
            logger.error(f"❌ {error_msg}\n{traceback.format_exc()}")
            
            # Cleanup on error
            self._cleanup_case_context(case_data.id)
            
            return {
                "case_id": case_data.id,
                "case_description": case_data.case,
                "status": "error",
                "error": error_msg,
                "error_type": type(e).__name__,
                "processing_time": processing_time,
                "timestamp": datetime.now().isoformat()
            }
    
    def _cleanup_case_context(self, case_id: str):
        """Clean up case-specific context to save memory"""
        keys_to_remove = [
            f"{case_id}_domain_analysis",
            f"{case_id}_similarity_scores", 
            f"{case_id}_semantic_analysis",
            f"{case_id}_confidence_analysis",
            f"{case_id}_start_time",
            f"{case_id}_case_data"
        ]
        
        for key in keys_to_remove:
            self.processing_context.pop(key, None)
    
    async def process_all_cases(
        self, 
        json_file_path: str, 
        csv_file_path: str,
        output_file_path: Optional[str] = None,
        max_concurrent: int = 1  # Sequential processing to avoid rate limits
    ) -> List[Dict[str, Any]]:
        """Process all cases using native ReAct agent with comprehensive management"""
        
        start_time = datetime.now()
        logger.info("🚀 Starting Complete Native ReAct Contextual Case Mapping process...")
        
        try:
            # Load and validate data
            logger.info("📂 Loading input data...")
            definitions = self.load_json_data(json_file_path)
            cases = self.load_csv_data(csv_file_path)
            
            logger.info(f"📊 Processing {len(cases)} cases with {len(definitions)} definitions using native ReAct")
            logger.info(f"🎯 Model: {self.llm.model_name}, Max concurrent: {max_concurrent}")
            
            # Process cases
            results = []
            successful_count = 0
            error_count = 0
            
            if max_concurrent == 1:
                # Sequential processing (recommended for API rate limits)
                for i, case in enumerate(cases, 1):
                    logger.info(f"🔍 Processing case {i}/{len(cases)}: {case.id}")
                    
                    result = await self.process_single_case_with_native_react(case, definitions)
                    results.append(result)
                    
                    if result.get("status") == "success":
                        successful_count += 1
                    else:
                        error_count += 1
                    
                    # Rate limiting
                    if i < len(cases):  # Don't delay after last case
                        await asyncio.sleep(RATE_LIMIT_DELAY)
            
            else:
                # Concurrent processing (use with caution due to API limits)
                semaphore = asyncio.Semaphore(max_concurrent)
                
                async def process_with_semaphore(case: CaseData) -> Dict[str, Any]:
                    async with semaphore:
                        return await self.process_single_case_with_native_react(case, definitions)
                
                logger.info(f"🔄 Starting concurrent processing with {max_concurrent} workers...")
                tasks = [process_with_semaphore(case) for case in cases]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Handle exceptions in results
                processed_results = []
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"❌ Exception in case {cases[i].id}: {str(result)}")
                        processed_results.append({
                            "case_id": cases[i].id,
                            "case_description": cases[i].case,
                            "status": "error",
                            "error": str(result),
                            "timestamp": datetime.now().isoformat()
                        })
                        error_count += 1
                    else:
                        processed_results.append(result)
                        if result.get("status") == "success":
                            successful_count += 1
                        else:
                            error_count += 1
                
                results = processed_results
            
            # Calculate summary statistics
            total_time = (datetime.now() - start_time).total_seconds()
            avg_time = total_time / len(cases) if cases else 0
            
            logger.info(f"✅ Processing complete! {successful_count} successful, {error_count} errors in {total_time:.2f}s")
            
            # Save results if output path provided
            if output_file_path:
                self.save_results(results, output_file_path, total_time, avg_time)
            
            return results
            
        except Exception as e:
            total_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"❌ Fatal error in process_all_cases: {str(e)}\n{traceback.format_exc()}")
            raise RuntimeError(f"Processing failed after {total_time:.2f}s: {str(e)}")
    
    def save_results(self, results: List[Dict[str, Any]], output_file_path: str, 
                    total_time: float, avg_time: float):
        """Save comprehensive results to JSON file with metadata"""
        try:
            logger.info(f"💾 Saving results to {output_file_path}...")
            
            # Calculate statistics
            successful = [r for r in results if r.get("status") == "success"]
            errors = [r for r in results if r.get("status") == "error"]
            
            confidence_scores = [
                r.get("confidence_score", 0) for r in successful 
                if isinstance(r.get("confidence_score"), (int, float))
            ]
            
            # Create comprehensive metadata
            output_data = {
                "metadata": {
                    "timestamp": datetime.now().isoformat(),
                    "model_used": self.llm.model_name,
                    "agent_type": "Native ReAct (StateGraph + ToolNode)",
                    "framework_version": "LangGraph 0.4.7+",
                    "total_cases": len(results),
                    "successful_matches": len(successful),
                    "errors": len(errors),
                    "success_rate": len(successful) / len(results) if results else 0,
                    "processing_stats": {
                        "total_time_seconds": round(total_time, 2),
                        "average_time_per_case": round(avg_time, 2),
                        "cases_per_minute": round(len(results) / (total_time / 60), 2) if total_time > 0 else 0
                    },
                    "confidence_stats": {
                        "average_confidence": round(sum(confidence_scores) / len(confidence_scores), 3) if confidence_scores else 0,
                        "min_confidence": round(min(confidence_scores), 3) if confidence_scores else 0,
                        "max_confidence": round(max(confidence_scores), 3) if confidence_scores else 0,
                        "high_confidence_count": len([s for s in confidence_scores if s >= 0.8]),
                        "low_confidence_count": len([s for s in confidence_scores if s < 0.5])
                    },
                    "system_info": {
                        "max_retries": MAX_RETRIES,
                        "timeout_seconds": DEFAULT_TIMEOUT,
                        "rate_limit_delay": RATE_LIMIT_DELAY,
                        "recursion_limit": MAX_RECURSION_LIMIT
                    }
                },
                "results": results
            }
            
            # Save with proper formatting
            output_path = Path(output_file_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as file:
                json.dump(output_data, file, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"✅ Results successfully saved to {output_file_path}")
            logger.info(f"📊 Summary: {len(successful)} successful, {len(errors)} errors")
            
            if confidence_scores:
                avg_conf = sum(confidence_scores) / len(confidence_scores)
                logger.info(f"📈 Average confidence: {avg_conf:.3f}")
            
        except Exception as e:
            logger.error(f"❌ Error saving results: {str(e)}\n{traceback.format_exc()}")
            raise
    
    def print_comprehensive_summary(self, results: List[Dict[str, Any]]):
        """Print a comprehensive summary of the native ReAct agent results"""
        
        successful = [r for r in results if r.get("status") == "success"]
        errors = [r for r in results if r.get("status") == "error"]
        partial = [r for r in results if r.get("status") == "partial_completion"]
        
        print(f"\n{'='*80}")
        print("🤖 COMPLETE NATIVE REACT CONTEXTUAL CASE MAPPING SUMMARY")
        print(f"{'='*80}")
        
        # Basic statistics
        print(f"📊 PROCESSING STATISTICS:")
        print(f"   • Total cases processed: {len(results)}")
        print(f"   • ✅ Successful matches: {len(successful)}")
        print(f"   • ⚠️ Partial completions: {len(partial)}")
        print(f"   • ❌ Errors: {len(errors)}")
        print(f"   • 📈 Success rate: {len(successful) / len(results) * 100:.1f}%")
        
        # Performance statistics
        processing_times = [r.get("processing_time", 0) for r in results if r.get("processing_time")]
        if processing_times:
            print(f"\n⏱️ PERFORMANCE METRICS:")
            print(f"   • Average processing time: {sum(processing_times) / len(processing_times):.2f}s")
            print(f"   • Fastest case: {min(processing_times):.2f}s")
            print(f"   • Slowest case: {max(processing_times):.2f}s")
        
        # Confidence statistics
        confidence_scores = [
            r.get("confidence_score", 0) for r in successful 
            if isinstance(r.get("confidence_score"), (int, float))
        ]
        
        if confidence_scores:
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            print(f"\n📈 CONFIDENCE ANALYSIS:")
            print(f"   • Average confidence score: {avg_confidence:.3f}")
            print(f"   • Highest confidence: {max(confidence_scores):.3f}")
            print(f"   • Lowest confidence: {min(confidence_scores):.3f}")
            
            # Confidence distribution
            very_high = len([s for s in confidence_scores if s >= 0.9])
            high = len([s for s in confidence_scores if 0.7 <= s < 0.9])
            moderate = len([s for s in confidence_scores if 0.5 <= s < 0.7])
            low = len([s for s in confidence_scores if s < 0.5])
            
            print(f"   • Very High (≥0.9): {very_high} cases")
            print(f"   • High (0.7-0.9): {high} cases")
            print(f"   • Moderate (0.5-0.7): {moderate} cases")
            print(f"   • Low (<0.5): {low} cases")
            
            # Top matches by confidence
            print(f"\n🏆 TOP MATCHES BY CONFIDENCE:")
            top_matches = sorted(
                [r for r in successful if isinstance(r.get("confidence_score"), (int, float))], 
                key=lambda x: x.get("confidence_score", 0), 
                reverse=True
            )[:5]
            
            for i, match in enumerate(top_matches, 1):
                print(f"   {i}. 📋 Case {match['case_id']}: {match.get('matched_name', 'Unknown')} "
                      f"(confidence: {match.get('confidence_score', 0):.3f})")
        
        # Error analysis
        if errors:
            print(f"\n⚠️ ERROR ANALYSIS:")
            error_types = {}
            for error in errors:
                error_type = error.get('error_type', 'Unknown')
                error_types[error_type] = error_types.get(error_type, 0) + 1
            
            for error_type, count in error_types.items():
                print(f"   • {error_type}: {count} cases")
            
            print(f"\n   Recent errors:")
            for error in errors[:3]:
                print(f"   ❌ Case {error['case_id']}: {error.get('error', 'Unknown error')[:60]}...")
        
        # Native ReAct advantages
        print(f"\n🎯 NATIVE REACT AGENT ADVANTAGES:")
        print(f"   • ⚙️ Full control over ReAct reasoning process")
        print(f"   • 🛠️ Custom specialized analysis tools")
        print(f"   • 🔍 Transparent step-by-step execution")
        print(f"   • 🎛️ Configurable StateGraph with ToolNode")
        print(f"   • 📊 Multi-dimensional confidence scoring")
        print(f"   • 🧠 Advanced semantic similarity analysis")
        print(f"   • 🏆 Comprehensive competitive analysis")
        print(f"   • 📈 Domain-specific pattern recognition")
        print(f"   • ⚡ Optimized performance with error handling")
        
        print(f"\n{'='*80}")
        print("🎉 Analysis complete! Native ReAct agent demonstrates superior")
        print("   contextual understanding with full transparency and control.")
        print(f"{'='*80}")


# Main execution and CLI
async def main():
    """Main function to run the complete native ReAct contextual case mapper"""
    
    # Setup argument parser
    parser = argparse.ArgumentParser(
        description="Complete Native ReAct Contextual Case Mapping System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python complete_react_mapper.py --json-file definitions.json --csv-file cases.csv
  python complete_react_mapper.py --json-file defs.json --csv-file cases.csv --output-file results.json --model gpt-4o
  python complete_react_mapper.py --help

Environment Variables:
  OPENAI_API_KEY      Your OpenAI API key (required)
  LANGSMITH_API_KEY   LangSmith API key for tracing (optional)
        """
    )
    
    parser.add_argument("--json-file", required=True, 
                       help="Path to JSON file with name definitions")
    parser.add_argument("--csv-file", required=True, 
                       help="Path to CSV file with cases")
    parser.add_argument("--output-file", 
                       help="Path to save results JSON file (optional)")
    parser.add_argument("--api-key", 
                       help="OpenAI API key (or set OPENAI_API_KEY env var)")
    parser.add_argument("--model", default="o3-mini", 
                       help="Model name (default: o3-mini)")
    parser.add_argument("--temperature", type=float, default=0.1,
                       help="Model temperature (default: 0.1)")
    parser.add_argument("--max-tokens", type=int, default=2000,
                       help="Maximum tokens per response (default: 2000)")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT,
                       help=f"Request timeout in seconds (default: {DEFAULT_TIMEOUT})")
    parser.add_argument("--max-concurrent", type=int, default=1,
                       help="Maximum concurrent processing (default: 1)")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Configure logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("🔧 Verbose logging enabled")
    
    # Get and validate API key
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("❌ OpenAI API key is required. Set OPENAI_API_KEY environment variable or use --api-key")
        sys.exit(1)
    
    # Validate input files
    if not Path(args.json_file).exists():
        logger.error(f"❌ JSON file not found: {args.json_file}")
        sys.exit
