#!/usr/bin/env python3
"""
TTL to CSV to FalkorDB Ultra-Fast Pipeline
Converts TTL to CSV format then uses FalkorDB's native bulk loader for maximum speed
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
import subprocess
from typing import Dict, Set, List, Tuple, Any
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TTLToCSVConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to CSV converter"""
        self.output_dir = output_dir
        self.nodes_by_type = {}  # type -> list of nodes
        self.edges_by_type = {}  # relationship -> list of edges
        self.node_id_map = {}    # URI -> unique_id
        self.next_id = 1
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
    def clean_identifier(self, uri_or_literal: str) -> str:
        """Clean and create valid identifier for labels/properties"""
        if isinstance(uri_or_literal, URIRef):
            parsed = urlparse(str(uri_or_literal))
            if parsed.fragment:
                return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.fragment)
            elif parsed.path:
                local_name = parsed.path.split('/')[-1]
                if local_name:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', local_name)
                else:
                    return re.sub(r'[^a-zA-Z0-9_]', '_', parsed.netloc)
            else:
                return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
        else:
            return re.sub(r'[^a-zA-Z0-9_]', '_', str(uri_or_literal))
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique sequential ID for a resource"""
        uri = str(resource)
        if uri not in self.node_id_map:
            self.node_id_map[uri] = str(self.next_id)
            self.next_id += 1
        return self.node_id_map[uri]
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, Any]:
        """Extract meaningful properties from URI"""
        properties = {}
        uri_str = str(uri)
        
        properties['uri'] = uri_str
        
        # Extract namespace and local name
        parsed = urlparse(uri_str)
        if parsed.fragment:
            properties['local_name'] = parsed.fragment
            properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
        elif parsed.path:
            parts = parsed.path.strip('/').split('/')
            if parts and parts[-1]:
                properties['local_name'] = parts[-1]
                properties['namespace'] = uri_str.replace(parts[-1], '').rstrip('/')
        
        return properties
    
    def process_literal_value(self, literal: Literal) -> Tuple[Any, str]:
        """Process literal value and return (value, datatype)"""
        if literal.datatype:
            datatype = str(literal.datatype)
            # Handle common datatypes
            if 'integer' in datatype or 'int' in datatype:
                try:
                    return int(literal), 'integer'
                except:
                    return str(literal), 'string'
            elif 'decimal' in datatype or 'double' in datatype or 'float' in datatype:
                try:
                    return float(literal), 'float'
                except:
                    return str(literal), 'string'
            elif 'boolean' in datatype:
                return str(literal).lower() in ('true', '1'), 'boolean'
            elif 'date' in datatype:
                return str(literal), 'date'
            else:
                return str(literal), 'string'
        else:
            return str(literal), 'string'
    
    def convert_ttl_to_csv(self, ttl_file_path: str):
        """Convert TTL file to CSV format optimized for FalkorDB bulk loader"""
        logger.info(f"Converting {ttl_file_path} to CSV format...")
        
        # Parse TTL file
        graph = Graph()
        try:
            graph.parse(ttl_file_path, format='turtle')
            logger.info(f"Successfully parsed TTL file. Found {len(graph)} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        # Track properties for each node type
        node_properties = {}  # node_type -> set of property names
        edge_properties = {}  # edge_type -> set of property names
        
        total_triples = len(graph)
        processed = 0
        
        logger.info("First pass: Analyzing schema and collecting data...")
        with tqdm(total=total_triples, desc="Analyzing triples") as pbar:
            for subject, predicate, obj in graph:
                processed += 1
                pbar.update(1)
                
                subject_id = self.get_or_create_node_id(subject)
                subject_type = self.clean_identifier(subject)
                predicate_clean = self.clean_identifier(predicate)
                
                # Initialize node type tracking
                if subject_type not in self.nodes_by_type:
                    self.nodes_by_type[subject_type] = {}
                    node_properties[subject_type] = set()
                
                # Initialize node if not exists
                if subject_id not in self.nodes_by_type[subject_type]:
                    if isinstance(subject, URIRef):
                        base_props = self.extract_properties_from_uri(subject)
                    else:  # BNode
                        base_props = {'uri': str(subject), 'blank_node_id': str(subject)}
                    
                    self.nodes_by_type[subject_type][subject_id] = base_props
                    
                    # Track all base properties
                    for prop_name in base_props.keys():
                        node_properties[subject_type].add(prop_name)
                
                # Handle object
                if isinstance(obj, Literal):
                    # Add as property to subject node
                    value, datatype = self.process_literal_value(obj)
                    self.nodes_by_type[subject_type][subject_id][predicate_clean] = value
                    
                    # Track property
                    node_properties[subject_type].add(predicate_clean)
                    
                    # Also store language if present
                    if obj.language:
                        lang_prop = f"{predicate_clean}_lang"
                        self.nodes_by_type[subject_type][subject_id][lang_prop] = obj.language
                        node_properties[subject_type].add(lang_prop)
                
                else:
                    # Object is a resource - create edge
                    object_id = self.get_or_create_node_id(obj)
                    object_type = self.clean_identifier(obj)
                    
                    # Initialize object node if not exists
                    if object_type not in self.nodes_by_type:
                        self.nodes_by_type[object_type] = {}
                        node_properties[object_type] = set()
                    
                    if object_id not in self.nodes_by_type[object_type]:
                        if isinstance(obj, URIRef):
                            base_props = self.extract_properties_from_uri(obj)
                        else:  # BNode
                            base_props = {'uri': str(obj), 'blank_node_id': str(obj)}
                        
                        self.nodes_by_type[object_type][object_id] = base_props
                        
                        # Track all base properties
                        for prop_name in base_props.keys():
                            node_properties[object_type].add(prop_name)
                    
                    # Create edge
                    if predicate_clean not in self.edges_by_type:
                        self.edges_by_type[predicate_clean] = []
                        edge_properties[predicate_clean] = set(['predicate_uri'])
                    
                    edge = {
                        'source_id': subject_id,
                        'target_id': object_id,
                        'predicate_uri': str(predicate)
                    }
                    self.edges_by_type[predicate_clean].append(edge)
        
        logger.info(f"Schema analysis complete:")
        logger.info(f"  Node types: {list(self.nodes_by_type.keys())}")
        logger.info(f"  Edge types: {list(self.edges_by_type.keys())}")
        
        # Write CSV files
        csv_files = []
        
        # Write node CSV files
        logger.info("Writing node CSV files...")
        for node_type, nodes in self.nodes_by_type.items():
            if not nodes:
                continue
                
            filename = f"{self.output_dir}/{node_type}.csv"
            csv_files.append(('nodes', filename))
            
            # Get all possible properties for this node type
            all_props = node_properties[node_type]
            # Ensure 'id' is first column for bulk loader
            headers = ['id'] + [prop for prop in sorted(all_props) if prop != 'id']
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(headers)
                
                for node_id, node_data in nodes.items():
                    row = [node_id]  # ID first
                    for prop in headers[1:]:  # Skip 'id'
                        value = node_data.get(prop, '')
                        # Handle different data types for CSV
                        if isinstance(value, (list, dict)):
                            value = json.dumps(value)
                        elif value is None:
                            value = ''
                        row.append(str(value))
                    writer.writerow(row)
            
            logger.info(f"  Written {len(nodes)} {node_type} nodes to {filename}")
        
        # Write edge CSV files
        logger.info("Writing edge CSV files...")
        for edge_type, edges in self.edges_by_type.items():
            if not edges:
                continue
                
            filename = f"{self.output_dir}/{edge_type}.csv"
            csv_files.append(('relationships', filename))
            
            # Get all possible properties for this edge type
            all_props = edge_properties[edge_type]
            # Source and target must be first two columns
            headers = ['source_id', 'target_id'] + [prop for prop in sorted(all_props) 
                                                   if prop not in ['source_id', 'target_id']]
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(headers)
                
                for edge in edges:
                    row = [edge['source_id'], edge['target_id']]
                    for prop in headers[2:]:  # Skip source_id, target_id
                        value = edge.get(prop, '')
                        if isinstance(value, (list, dict)):
                            value = json.dumps(value)
                        elif value is None:
                            value = ''
                        row.append(str(value))
                    writer.writerow(row)
            
            logger.info(f"  Written {len(edges)} {edge_type} edges to {filename}")
        
        logger.info(f"CSV conversion completed! Files written to {self.output_dir}/")
        return csv_files
    
    def run_falkor_bulk_loader(self, csv_files: List[Tuple[str, str]], graph_name: str, 
                              host: str = 'localhost', port: int = 6379, password: str = None):
        """Run FalkorDB bulk loader on the generated CSV files"""
        logger.info("Installing falkordb-bulk-loader...")
        try:
            subprocess.run(['pip', 'install', 'falkordb-bulk-loader'], 
                         check=True, capture_output=True, text=True)
            logger.info("falkordb-bulk-loader installed successfully")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to install falkordb-bulk-loader: {e}")
            return False
        
        # Build command
        cmd = ['falkordb-bulk-insert', graph_name]
        
        # Add connection parameters
        if host != 'localhost':
            cmd.extend(['--host', host])
        if port != 6379:
            cmd.extend(['--port', str(port)])
        if password:
            cmd.extend(['--password', password])
        
        # Add CSV files
        for file_type, filename in csv_files:
            if file_type == 'nodes':
                cmd.extend(['--nodes', filename])
            elif file_type == 'relationships':
                cmd.extend(['--relationships', filename])
        
        logger.info(f"Running FalkorDB bulk loader command:")
        logger.info(f"  {' '.join(cmd)}")
        
        try:
            start_time = time.time()
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            end_time = time.time()
            
            logger.info(f"âœ… Bulk loading completed in {end_time - start_time:.2f} seconds!")
            logger.info("Bulk loader output:")
            if result.stdout:
                logger.info(result.stdout)
            
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Bulk loading failed: {e}")
            if e.stdout:
                logger.error(f"STDOUT: {e.stdout}")
            if e.stderr:
                logger.error(f"STDERR: {e.stderr}")
            return False

def main():
    """Main function for TTL to CSV to FalkorDB pipeline"""
    parser = argparse.ArgumentParser(description='Convert TTL to CSV and bulk load into FalkorDB')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='localhost', help='FalkorDB host')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port')
    parser.add_argument('--password', help='FalkorDB password')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--keep_csv', action='store_true', help='Keep CSV files after loading')
    
    args = parser.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        return
    
    file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
    logger.info(f"Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    
    # Create converter
    converter = TTLToCSVConverter(args.output_dir)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        csv_files = converter.convert_ttl_to_csv(args.ttl_file)
        conversion_time = time.time() - start_time
        
        logger.info(f"âœ… TTL to CSV conversion completed in {conversion_time:.2f} seconds")
        logger.info(f"   Performance: {file_size_mb/conversion_time:.1f} MB/second")
        
        # Run bulk loader
        success = converter.run_falkor_bulk_loader(
            csv_files, args.graph_name, args.host, args.port, args.password
        )
        
        if success:
            total_time = time.time() - start_time
            logger.info(f"ðŸš€ COMPLETE PIPELINE SUCCESS in {total_time:.2f} seconds!")
            logger.info(f"   Overall performance: {file_size_mb/total_time:.1f} MB/second")
            
            # Cleanup CSV files if requested
            if not args.keep_csv:
                import shutil
                shutil.rmtree(args.output_dir)
                logger.info(f"Cleaned up temporary CSV files in {args.output_dir}")
        else:
            logger.error("Pipeline failed during bulk loading")
            
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    main()
