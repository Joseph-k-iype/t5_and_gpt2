#!/usr/bin/env python3
"""
TTL to FalkorDB Pipeline - Direct Load from csv_output/
Converts TTL to CSV format then loads directly via HTTP server
"""

import rdflib
from rdflib import Graph, URIRef, Literal, BNode
import csv
import hashlib
import json
import time
import logging
import os
from typing import Dict, Set, List, Tuple, Any, Optional
from urllib.parse import urlparse
import re
from tqdm import tqdm
import argparse
import sys
import shutil
from pathlib import Path
import platform
import threading
import http.server
import socketserver
import socket

# Setup logging with UTF-8 encoding support
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('pipeline.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class HTTPServerThread(threading.Thread):
    """HTTP server thread for serving CSV files"""
    def __init__(self, directory, port=8000):
        super().__init__(daemon=True)
        self.directory = Path(directory)
        self.port = port
        self.httpd = None
        self.running = False
        
    def find_free_port(self):
        """Find a free port starting from self.port"""
        for port in range(self.port, self.port + 100):
            try:
                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                    s.bind(('', port))
                    return port
            except OSError:
                continue
        raise OSError("No free ports available")
    
    def run(self):
        """Start the HTTP server"""
        os.chdir(self.directory)
        self.port = self.find_free_port()
        
        handler = http.server.SimpleHTTPRequestHandler
        # Suppress HTTP server logs
        handler.log_message = lambda self, format, *args: None
        
        with socketserver.TCPServer(("", self.port), handler) as httpd:
            self.httpd = httpd
            self.running = True
            logger.info(f"HTTP server started on port {self.port} serving {self.directory}")
            httpd.serve_forever()
    
    def stop(self):
        """Stop the HTTP server"""
        if self.httpd and self.running:
            self.httpd.shutdown()
            self.running = False
            logger.info("HTTP server stopped")

class TTLToFalkorDBConverter:
    def __init__(self, output_dir='csv_output'):
        """Initialize the TTL to FalkorDB converter"""
        self.output_dir = Path(output_dir)
        self.nodes = {}  # node_id -> {node_type, properties}
        self.edges = []  # list of {source_id, target_id, edge_type, properties}
        self.node_id_map = {}  # URI -> unique_id
        self.next_id = 1
        self.is_windows = platform.system() == 'Windows'
        self.http_server = None
        
        # Create output directory
        try:
            logger.debug(f"Creating output directory: {self.output_dir}")
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Output directory created/verified: {self.output_dir}")
            logger.debug(f"Directory exists: {self.output_dir.exists()}")
            logger.debug(f"Directory is writable: {os.access(self.output_dir, os.W_OK)}")
        except Exception as e:
            logger.error(f"Failed to create output directory {self.output_dir}: {e}")
            raise
        
        # CSV writing parameters
        self.csv_params = {
            'quoting': csv.QUOTE_MINIMAL,
            'quotechar': '"',
            'delimiter': ',',
            'lineterminator': '\n',
            'escapechar': None
        }
        
    def clean_identifier(self, uri_or_literal: Any) -> str:
        """Clean and create valid identifier for labels/properties"""
        try:
            if isinstance(uri_or_literal, URIRef):
                uri_str = str(uri_or_literal)
                parsed = urlparse(uri_str)
                
                # Try fragment first (after #)
                if parsed.fragment:
                    name = parsed.fragment
                # Then try last path component
                elif parsed.path and parsed.path != '/':
                    name = parsed.path.split('/')[-1] or parsed.path.split('/')[-2]
                # Fall back to netloc
                elif parsed.netloc:
                    name = parsed.netloc.replace('.', '_')
                else:
                    # Use hash of full URI as fallback
                    name = f"uri_{hashlib.md5(uri_str.encode()).hexdigest()[:8]}"
                
                # Clean the name - only alphanumeric and underscore
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                # Ensure it starts with letter or underscore
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"n_{cleaned}"
                # Ensure it's not empty and has reasonable length
                if not cleaned or len(cleaned) < 1:
                    cleaned = f"node_{hashlib.md5(uri_str.encode()).hexdigest()[:6]}"
                return cleaned[:50]  # Limit length
            else:
                # Handle literals or other types
                name = str(uri_or_literal)
                cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', name)
                if cleaned and not (cleaned[0].isalpha() or cleaned[0] == '_'):
                    cleaned = f"l_{cleaned}"
                return (cleaned or "literal")[:50]
        except Exception as e:
            logger.warning(f"Error cleaning identifier for {uri_or_literal}: {e}")
            return f"error_{hashlib.md5(str(uri_or_literal).encode()).hexdigest()[:8]}"
    
    def get_or_create_node_id(self, resource) -> str:
        """Get or create a unique ID for a resource"""
        try:
            uri = str(resource)
            if uri not in self.node_id_map:
                # Use hash-based IDs to avoid integer overflow issues
                hash_obj = hashlib.md5(uri.encode('utf-8'))
                node_id = f"n_{hash_obj.hexdigest()[:8]}"
                
                # Ensure uniqueness
                original_id = node_id
                counter = 1
                while node_id in [node['id'] for node in self.nodes.values()]:
                    node_id = f"{original_id}_{counter}"
                    counter += 1
                
                self.node_id_map[uri] = node_id
            return self.node_id_map[uri]
        except Exception as e:
            logger.error(f"Error creating node ID for {resource}: {e}")
            # Fallback to simple hash
            fallback_id = f"error_{hashlib.md5(str(resource).encode()).hexdigest()[:8]}"
            return fallback_id
    
    def extract_properties_from_uri(self, uri: URIRef) -> Dict[str, str]:
        """Extract meaningful properties from URI"""
        properties = {}
        try:
            uri_str = str(uri)
            properties['uri'] = uri_str
            
            # Extract namespace and local name
            parsed = urlparse(uri_str)
            if parsed.fragment:
                properties['local_name'] = parsed.fragment
                properties['namespace'] = uri_str.replace('#' + parsed.fragment, '')
            elif parsed.path and parsed.path != '/':
                parts = [p for p in parsed.path.strip('/').split('/') if p]
                if parts:
                    properties['local_name'] = parts[-1]
                    properties['namespace'] = f"{parsed.scheme}://{parsed.netloc}/" + '/'.join(parts[:-1])
        except Exception as e:
            logger.warning(f"Error extracting properties from URI {uri}: {e}")
            properties['uri'] = str(uri)
        
        return properties
    
    def sanitize_csv_value(self, value: Any) -> str:
        """Sanitize value for CSV output"""
        try:
            if value is None:
                return ''
            
            if isinstance(value, bool):
                return 'true' if value else 'false'
            
            if isinstance(value, (list, dict)):
                # Convert to JSON string for complex data
                return json.dumps(value, ensure_ascii=False)
            
            # Convert to string and clean
            str_value = str(value).strip()
            
            # Replace problematic characters for CSV
            str_value = str_value.replace('\r\n', ' ').replace('\r', ' ').replace('\n', ' ')
            str_value = re.sub(r'\s+', ' ', str_value)  # Normalize whitespace
            
            # Escape quotes by doubling them (CSV standard)
            if '"' in str_value:
                str_value = str_value.replace('"', '""')
            
            # Limit string length to prevent memory issues
            if len(str_value) > 500:
                logger.warning(f"Very long string truncated (length: {len(str_value)})")
                str_value = str_value[:500] + "..."
            
            return str_value
        except Exception as e:
            logger.warning(f"Error sanitizing value {value}: {e}")
            return str(value) if value is not None else ''
    
    def process_literal_value(self, literal: Literal) -> str:
        """Process literal value and return cleaned string"""
        try:
            if literal.datatype:
                datatype_str = str(literal.datatype)
                
                # Handle common XSD datatypes
                if any(x in datatype_str.lower() for x in ['integer', 'int', 'long']):
                    try:
                        int_val = int(literal)
                        # Keep reasonable sized integers
                        if abs(int_val) <= 2147483647:
                            return str(int_val)
                        else:
                            return str(literal)  # Keep as string if too large
                    except ValueError:
                        return str(literal)
                        
                elif any(x in datatype_str.lower() for x in ['decimal', 'double', 'float']):
                    try:
                        float_val = float(literal)
                        if abs(float_val) <= 1e15:
                            return str(float_val)
                        else:
                            return str(literal)
                    except ValueError:
                        return str(literal)
                        
                elif 'boolean' in datatype_str.lower():
                    return str(literal).lower()
                    
                else:
                    return str(literal)
            else:
                return str(literal)
                
        except Exception as e:
            logger.warning(f"Error processing literal {literal}: {e}")
            return str(literal)
    
    def convert_ttl_to_csv(self, ttl_file_path: str, max_triples: Optional[int] = None):
        """Convert TTL file to CSV format for FalkorDB LOAD CSV"""
        logger.info(f"Converting {ttl_file_path} to CSV format for direct loading...")
        
        # Parse TTL file
        graph = Graph()
        try:
            logger.info("Parsing TTL file...")
            graph.parse(ttl_file_path, format='turtle')
            total_triples = len(graph)
            logger.info(f"Successfully parsed TTL file. Found {total_triples:,} triples.")
        except Exception as e:
            logger.error(f"Error parsing TTL file: {e}")
            raise
        
        if total_triples == 0:
            logger.warning("No triples found in the TTL file!")
            return [], None
        
        # Apply limit if specified
        if max_triples and max_triples < total_triples:
            logger.info(f"Processing only first {max_triples} of {total_triples} triples")
            total_triples = max_triples
        
        logger.info("Processing triples and building data structures...")
        processed_count = 0
        try:
            with tqdm(total=total_triples, desc="Processing triples") as pbar:
                for subject, predicate, obj in graph:
                    if max_triples and processed_count >= max_triples:
                        break
                    
                    try:
                        pbar.update(1)
                        processed_count += 1
                        
                        # Get or create subject node
                        subject_id = self.get_or_create_node_id(subject)
                        subject_type = self.clean_identifier(subject)
                        predicate_clean = self.clean_identifier(predicate)
                        
                        # Initialize subject node if not exists
                        if subject_id not in self.nodes:
                            if isinstance(subject, URIRef):
                                base_props = self.extract_properties_from_uri(subject)
                            else:  # BNode
                                base_props = {
                                    'uri': str(subject),
                                    'resource_type': 'blank_node'
                                }
                            
                            self.nodes[subject_id] = {
                                'id': subject_id,
                                'node_type': subject_type,
                                'properties': base_props
                            }
                        
                        # Handle object
                        if isinstance(obj, Literal):
                            # Add as property to subject node
                            value = self.process_literal_value(obj)
                            prop_name = predicate_clean
                            
                            # Store the value as a property
                            self.nodes[subject_id]['properties'][prop_name] = value
                            
                            # Store language if present
                            if obj.language:
                                lang_prop = f"{prop_name}_lang"
                                self.nodes[subject_id]['properties'][lang_prop] = obj.language
                        
                        else:
                            # Object is a resource - create edge
                            object_id = self.get_or_create_node_id(obj)
                            object_type = self.clean_identifier(obj)
                            
                            # Initialize object node if not exists
                            if object_id not in self.nodes:
                                if isinstance(obj, URIRef):
                                    base_props = self.extract_properties_from_uri(obj)
                                else:  # BNode
                                    base_props = {
                                        'uri': str(obj),
                                        'resource_type': 'blank_node'
                                    }
                                
                                self.nodes[object_id] = {
                                    'id': object_id,
                                    'node_type': object_type,
                                    'properties': base_props
                                }
                            
                            # Create edge
                            edge = {
                                'source_id': subject_id,
                                'target_id': object_id,
                                'edge_type': predicate_clean,
                                'properties': {
                                    'predicate_uri': str(predicate)
                                }
                            }
                            self.edges.append(edge)
                    except Exception as e:
                        logger.warning(f"Error processing triple {processed_count}: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error during triple processing: {e}")
            raise
        
        logger.info(f"Data processing complete:")
        logger.info(f"  Nodes: {len(self.nodes):,}")
        logger.info(f"  Edges: {len(self.edges):,}")
        
        # Write CSV files
        try:
            csv_files = self.write_csv_files()
            logger.info(f"CSV files written successfully: {[f[1] for f in csv_files]}")
        except Exception as e:
            logger.error(f"Error writing CSV files: {e}")
            raise
        
        return csv_files
    
    def write_csv_files(self):
        """Write nodes and edges to CSV files"""
        csv_files = []
        
        # Write nodes CSV
        nodes_file = self.output_dir / "nodes.csv"
        csv_files.append(('nodes', str(nodes_file)))
        
        # Get all possible property names from all nodes
        all_properties = set()
        for node in self.nodes.values():
            all_properties.update(node['properties'].keys())
        
        all_properties = sorted(all_properties)
        
        try:
            with open(nodes_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: id, node_type, then all properties
                headers = ['id', 'node_type'] + all_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for node in self.nodes.values():
                    row = [
                        node['id'],
                        node['node_type']
                    ]
                    
                    # Add property values
                    for prop in all_properties:
                        value = node['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.nodes):,} nodes to {nodes_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing nodes file {nodes_file}: {e}")
            raise
        
        # Write edges CSV
        edges_file = self.output_dir / "edges.csv"
        csv_files.append(('edges', str(edges_file)))
        
        # Get all possible edge property names
        all_edge_properties = set()
        for edge in self.edges:
            all_edge_properties.update(edge['properties'].keys())
        
        all_edge_properties = sorted(all_edge_properties)
        
        try:
            with open(edges_file, 'w', newline='', encoding='utf-8') as csvfile:
                # Headers: source_id, target_id, edge_type, then all properties
                headers = ['source_id', 'target_id', 'edge_type'] + all_edge_properties
                writer = csv.writer(csvfile, **self.csv_params)
                writer.writerow(headers)
                
                for edge in self.edges:
                    row = [
                        edge['source_id'],
                        edge['target_id'],
                        edge['edge_type']
                    ]
                    
                    # Add property values
                    for prop in all_edge_properties:
                        value = edge['properties'].get(prop, '')
                        row.append(self.sanitize_csv_value(value))
                    
                    writer.writerow(row)
            
            logger.info(f"Written {len(self.edges):,} edges to {edges_file.name}")
            
        except Exception as e:
            logger.error(f"Error writing edges file {edges_file}: {e}")
            raise
        
        return csv_files
    
    def start_http_server(self, port=8000):
        """Start HTTP server to serve CSV files directly"""
        try:
            self.http_server = HTTPServerThread(self.output_dir, port)
            self.http_server.start()
            
            # Wait a moment for server to start
            time.sleep(1)
            
            if self.http_server.running:
                logger.info(f"HTTP server serving CSV files from {self.output_dir}")
                return f"http://localhost:{self.http_server.port}"
            else:
                logger.error("Failed to start HTTP server")
                return None
                
        except Exception as e:
            logger.error(f"Error starting HTTP server: {e}")
            return None
    
    def stop_http_server(self):
        """Stop the HTTP server"""
        if self.http_server:
            self.http_server.stop()
            self.http_server = None
    
    def generate_cypher_commands_direct(self, base_url: str):
        """Generate Cypher commands that load directly from HTTP URLs"""
        # Ensure output directory exists
        try:
            logger.debug(f"Ensuring output directory exists: {self.output_dir}")
            self.output_dir.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Output directory confirmed: {self.output_dir.exists()}")
        except Exception as e:
            logger.error(f"Failed to create output directory {self.output_dir}: {e}")
            raise
        
        cypher_file = self.output_dir / "load_data_direct.cypher"
        logger.debug(f"Will write Cypher file to: {cypher_file}")
        
        nodes_url = f"{base_url}/nodes.csv"
        edges_url = f"{base_url}/edges.csv"
        
        try:
            logger.debug(f"Opening file for writing: {cypher_file}")
            with open(cypher_file, 'w', encoding='utf-8') as f:
                f.write("// FalkorDB LOAD CSV Commands - Direct Loading via HTTP\n")
                f.write("// Loading directly from csv_output/ folder via HTTP server\n")
                f.write(f"// Files served from: {self.output_dir}\n")
                f.write(f"// Base URL: {base_url}\n\n")
                
                f.write("// Clear existing data (optional)\n")
                f.write("// MATCH (n) DETACH DELETE n;\n\n")
                
                f.write("// DIRECT LOAD METHOD - Load from local HTTP server\n")
                f.write("// Load all nodes with generic Entity label\n")
                f.write(f"LOAD CSV WITH HEADERS FROM '{nodes_url}' AS row\n")
                f.write("CREATE (n:Entity)\n")
                f.write("SET n = row;\n\n")
                
                f.write("// Load all edges with generic CONNECTED_TO relationship\n")
                f.write(f"LOAD CSV WITH HEADERS FROM '{edges_url}' AS row\n")
                f.write("MATCH (source:Entity {id: row.source_id})\n")
                f.write("MATCH (target:Entity {id: row.target_id})\n")
                f.write("CREATE (source)-[r:CONNECTED_TO]->(target)\n")
                f.write("SET r = row\n")
                f.write("REMOVE r.source_id, r.target_id, r.edge_type;\n\n")
                
                f.write("// BATCHED LOAD METHOD (for large datasets)\n")
                f.write(f"// LOAD CSV WITH HEADERS FROM '{nodes_url}' AS row\n")
                f.write("// CALL {\n")
                f.write("//   WITH row\n")
                f.write("//   CREATE (n:Entity)\n")
                f.write("//   SET n = row\n")
                f.write("//   RETURN n\n")
                f.write("// } IN TRANSACTIONS OF 1000 ROWS;\n\n")
                
                f.write("// Verify data loaded\n")
                f.write("MATCH (n) RETURN count(n) AS node_count;\n")
                f.write("MATCH ()-[r]->() RETURN count(r) AS edge_count;\n")
                f.write("MATCH (n) RETURN n.node_type, count(*) AS count ORDER BY count DESC LIMIT 10;\n\n")
                
                f.write("// NOTE: HTTP server must be running for these commands to work\n")
                f.write("// Use --execute_cypher flag to automatically start server and load data\n")
            
            logger.info(f"Generated direct loading Cypher commands in {cypher_file.name}")
            logger.debug(f"Cypher file written successfully to: {cypher_file}")
            
        except Exception as e:
            logger.error(f"Error writing Cypher file {cypher_file}: {e}")
            logger.error(f"Directory exists: {self.output_dir.exists()}")
            logger.error(f"Directory writable: {os.access(self.output_dir, os.W_OK) if self.output_dir.exists() else 'N/A'}")
            raise
        
        return str(cypher_file)
    
    def test_falkordb_connection(self, host: str, port: int, password: Optional[str] = None):
        """Test connection to FalkorDB"""
        logger.info(f"Testing connection to FalkorDB at {host}:{port}...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test basic connection
            response = r.ping()
            if response:
                logger.info("Redis connection successful")
            else:
                logger.error("Redis ping failed")
                return False
            
            # Test FalkorDB module
            try:
                modules = r.module_list()
                falkor_loaded = any('graph' in str(module).lower() for module in modules)
                if falkor_loaded:
                    logger.info("FalkorDB module is loaded")
                else:
                    logger.warning("FalkorDB module not detected in module list")
                    logger.info(f"Available modules: {modules}")
            except Exception as e:
                logger.warning(f"Could not check modules: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"Connection test failed: {e}")
            return False
    
    def execute_cypher_commands_direct(self, base_url: str, graph_name: str, 
                                     host: str = '127.0.0.1', port: int = 6379, 
                                     password: Optional[str] = None):
        """Execute Cypher commands loading directly via HTTP"""
        logger.info(f"Executing Cypher commands on graph '{graph_name}' with direct loading...")
        
        try:
            import redis
        except ImportError:
            logger.error("redis-py not installed. Install with: pip install redis")
            return False
        
        try:
            # Create Redis connection
            if password:
                r = redis.Redis(host=host, port=port, password=password, decode_responses=True)
            else:
                r = redis.Redis(host=host, port=port, decode_responses=True)
            
            # Test connection
            r.ping()
            logger.info("Connected to FalkorDB")
            
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            return False
        
        # Use HTTP URLs to load directly from csv_output folder
        nodes_url = f"{base_url}/nodes.csv"
        edges_url = f"{base_url}/edges.csv"
        
        logger.info(f"Loading nodes from: {nodes_url}")
        logger.info(f"Loading edges from: {edges_url}")
        
        try:
            # Load nodes
            logger.info("Loading nodes...")
            nodes_query = f"""
            LOAD CSV WITH HEADERS FROM '{nodes_url}' AS row
            CREATE (n:Entity)
            SET n = row
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, nodes_query)
                logger.info(f"Nodes loaded: {result}")
            except Exception as e:
                logger.error(f"Failed to load nodes: {e}")
                if "error opening csv uri" in str(e).lower():
                    logger.error("Could not access CSV files via HTTP.")
                    logger.error("Solutions:")
                    logger.error("  1. Ensure HTTP server is running")
                    logger.error("  2. Check firewall/antivirus blocking local HTTP server")
                    logger.error("  3. Try different port with --http_port option")
                return False
            
            # Load edges
            logger.info("Loading edges...")
            edges_query = f"""
            LOAD CSV WITH HEADERS FROM '{edges_url}' AS row
            MATCH (source:Entity {{id: row.source_id}})
            MATCH (target:Entity {{id: row.target_id}})
            CREATE (source)-[r:CONNECTED_TO]->(target)
            SET r = row
            REMOVE r.source_id, r.target_id
            """
            
            try:
                result = r.execute_command("GRAPH.QUERY", graph_name, edges_query)
                logger.info(f"Edges loaded: {result}")
            except Exception as e:
                logger.error(f"Failed to load edges: {e}")
                return False
            
            # Verify data
            try:
                count_query = "MATCH (n) RETURN count(n) AS node_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, count_query)
                logger.info(f"Total nodes in graph: {result}")
                
                edge_count_query = "MATCH ()-[r]->() RETURN count(r) AS edge_count"
                result = r.execute_command("GRAPH.QUERY", graph_name, edge_count_query)
                logger.info(f"Total edges in graph: {result}")
            except Exception as e:
                logger.warning(f"Could not verify data counts: {e}")
            
            logger.info("Cypher commands executed successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to execute Cypher commands: {e}")
            return False

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Convert TTL to FalkorDB - Direct Load from csv_output/')
    parser.add_argument('ttl_file', help='Path to TTL file')
    parser.add_argument('--graph_name', default='knowledge_graph', help='FalkorDB graph name')
    parser.add_argument('--host', default='127.0.0.1', help='FalkorDB host (default: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=6379, help='FalkorDB port (default: 6379)')
    parser.add_argument('--password', help='FalkorDB password (default: none)')
    parser.add_argument('--output_dir', default='csv_output', help='Output directory for CSV files')
    parser.add_argument('--csv_only', action='store_true', help='Only convert to CSV, skip loading')
    parser.add_argument('--test_connection', action='store_true', help='Test FalkorDB connection only')
    parser.add_argument('--max_triples', type=int, help='Limit number of triples to process for testing')
    parser.add_argument('--execute_cypher', action='store_true', help='Execute Cypher commands automatically with direct loading')
    parser.add_argument('--http_port', type=int, default=8000, help='Port for HTTP server (default: 8000)')
    parser.add_argument('--keep_server', action='store_true', help='Keep HTTP server running after loading')
    parser.add_argument('--debug', action='store_true', help='Enable debug logging')
    
    args = parser.parse_args()
    
    # Set debug logging if requested
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.debug("Debug logging enabled")
    
    # Validate input
    if not os.path.exists(args.ttl_file):
        logger.error(f"TTL file not found: {args.ttl_file}")
        sys.exit(1)
    
    try:
        file_size_mb = os.path.getsize(args.ttl_file) / (1024 * 1024)
        logger.info(f"Processing TTL file: {args.ttl_file} ({file_size_mb:.1f}MB)")
    except Exception as e:
        logger.error(f"Cannot access TTL file: {e}")
        sys.exit(1)
    
    # Create converter
    try:
        converter = TTLToFalkorDBConverter(args.output_dir)
    except Exception as e:
        logger.error(f"Failed to initialize converter: {e}")
        sys.exit(1)
    
    # Test connection only if requested
    if args.test_connection:
        logger.info("Connection test mode")
        try:
            success = converter.test_falkordb_connection(args.host, args.port, args.password)
            if success:
                logger.info("Connection test passed!")
                sys.exit(0)
            else:
                logger.error("Connection test failed!")
                sys.exit(1)
        except Exception as e:
            logger.error(f"Connection test error: {e}")
            sys.exit(1)
    
    try:
        # Convert TTL to CSV
        start_time = time.time()
        try:
            logger.info("Starting TTL to CSV conversion...")
            csv_files = converter.convert_ttl_to_csv(args.ttl_file, args.max_triples)
            
            if not csv_files:
                logger.error("No CSV files were generated")
                sys.exit(1)
                
            logger.info(f"CSV conversion completed successfully. Generated files: {len(csv_files)}")
            
        except Exception as e:
            logger.error(f"TTL to CSV conversion failed: {e}")
            logger.exception("Detailed error:")
            sys.exit(1)
        
        conversion_time = time.time() - start_time
        logger.info(f"TTL->CSV conversion completed in {conversion_time:.1f}s ({file_size_mb/conversion_time:.1f} MB/s)")
        
        if args.csv_only:
            logger.info("CSV-only mode: Files generated successfully")
            logger.info(f"CSV files in: {args.output_dir}/")
            logger.info("To load directly:")
            logger.info("  python script.py data.ttl --execute_cypher")
            return
        
        # Execute Cypher commands with direct loading
        if args.execute_cypher:
            try:
                # Verify CSV files exist before starting server
                nodes_file = converter.output_dir / "nodes.csv"
                edges_file = converter.output_dir / "edges.csv"
                
                logger.debug(f"Checking for nodes file: {nodes_file}")
                logger.debug(f"Checking for edges file: {edges_file}")
                logger.debug(f"Output directory contents: {list(converter.output_dir.iterdir()) if converter.output_dir.exists() else 'Directory does not exist'}")
                
                if not nodes_file.exists():
                    logger.error(f"Nodes CSV file not found: {nodes_file}")
                    sys.exit(1)
                    
                if not edges_file.exists():
                    logger.error(f"Edges CSV file not found: {edges_file}")
                    sys.exit(1)
                
                logger.info(f"CSV files verified: {nodes_file}, {edges_file}")
                
                # Start HTTP server
                logger.info("Starting HTTP server for direct CSV loading...")
                base_url = converter.start_http_server(args.http_port)
                
                if not base_url:
                    logger.error("Failed to start HTTP server")
                    sys.exit(1)
                
                # Generate Cypher commands
                try:
                    cypher_file = converter.generate_cypher_commands_direct(base_url)
                    logger.info(f"Generated Cypher commands: {cypher_file}")
                except Exception as e:
                    logger.error(f"Failed to generate Cypher commands: {e}")
                    converter.stop_http_server()
                    sys.exit(1)
                
                # Execute commands
                success = converter.execute_cypher_commands_direct(
                    base_url, args.graph_name, args.host, args.port, args.password
                )
                
                total_time = time.time() - start_time
                
                if success:
                    logger.info(f"DIRECT LOAD SUCCESS! Total time: {total_time:.1f}s")
                    logger.info(f"Performance: {file_size_mb/total_time:.1f} MB/s")
                    logger.info(f"Data loaded into graph: {args.graph_name}")
                    logger.info(f"CSV files served from: {converter.output_dir}")
                    
                    if args.keep_server:
                        logger.info(f"HTTP server kept running at: {base_url}")
                        logger.info("Press Ctrl+C to stop the server")
                        try:
                            while True:
                                time.sleep(1)
                        except KeyboardInterrupt:
                            logger.info("Stopping HTTP server...")
                            converter.stop_http_server()
                    else:
                        converter.stop_http_server()
                else:
                    converter.stop_http_server()
                    logger.error("Failed to execute Cypher commands")
                    sys.exit(1)
                    
            except Exception as e:
                converter.stop_http_server()
                logger.error(f"Error in direct loading: {e}")
                sys.exit(1)
        else:
            # Start HTTP server for manual use
            try:
                # Verify CSV files exist before starting server
                nodes_file = converter.output_dir / "nodes.csv"
                edges_file = converter.output_dir / "edges.csv"
                
                logger.debug(f"Checking for nodes file: {nodes_file}")
                logger.debug(f"Checking for edges file: {edges_file}")
                logger.debug(f"Output directory contents: {list(converter.output_dir.iterdir()) if converter.output_dir.exists() else 'Directory does not exist'}")
                
                if not nodes_file.exists():
                    logger.error(f"Nodes CSV file not found: {nodes_file}")
                    logger.error("Run with --csv_only first to generate CSV files")
                    sys.exit(1)
                    
                if not edges_file.exists():
                    logger.error(f"Edges CSV file not found: {edges_file}")
                    logger.error("Run with --csv_only first to generate CSV files")
                    sys.exit(1)
                
                base_url = converter.start_http_server(args.http_port)
                if base_url:
                    try:
                        cypher_file = converter.generate_cypher_commands_direct(base_url)
                        logger.info("CSV files generated and HTTP server started")
                        logger.info(f"Files in: {args.output_dir}/")
                        logger.info(f"Server URL: {base_url}")
                        logger.info(f"Commands in: {cypher_file}")
                        logger.info("\nTo load data:")
                        logger.info("  1. Connect to FalkorDB")
                        logger.info("  2. Run the commands from the .cypher file")
                        logger.info("  3. Or use --execute_cypher flag for automatic loading")
                        logger.info("\nHTTP server running - Press Ctrl+C to stop")
                        
                        try:
                            while True:
                                time.sleep(1)
                        except KeyboardInterrupt:
                            logger.info("Stopping HTTP server...")
                            converter.stop_http_server()
                    except Exception as e:
                        logger.error(f"Error generating Cypher commands: {e}")
                        converter.stop_http_server()
                        sys.exit(1)
                else:
                    logger.error("Failed to start HTTP server")
            except Exception as e:
                logger.error(f"Error starting HTTP server: {e}")
            
    except KeyboardInterrupt:
        logger.error("Pipeline interrupted by user")
        if converter.http_server:
            converter.stop_http_server()
        sys.exit(1)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        if converter.http_server:
            converter.stop_http_server()
        logger.exception("Full error details:")
        sys.exit(1)

if __name__ == "__main__":
    main()
