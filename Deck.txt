import re
import gc
import time
import logging
import hashlib
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any, Iterator
from dataclasses import dataclass
from collections import defaultdict, Counter
import json

# Core libraries
import psutil
from tqdm import tqdm

# FalkorDB
import falkordb


@dataclass
class ProcessingStats:
    """Statistics for alternative processing approaches."""
    start_time: float = 0.0
    end_time: float = 0.0
    total_lines: int = 0
    processed_lines: int = 0
    error_lines: int = 0
    skipped_lines: int = 0
    nodes_created: int = 0
    edges_created: int = 0
    memory_peak_mb: float = 0.0
    
    @property
    def processing_time(self) -> float:
        return (self.end_time or time.time()) - self.start_time


class ManualTTLParser:
    """Manual TTL/N-Triples parser using regex - bypasses RDFLib completely."""
    
    def __init__(self):
        # Compiled regex patterns for efficiency
        self.ntriples_pattern = re.compile(
            r'^<([^>]+)>\s+<([^>]+)>\s+(?:<([^>]+)>|"([^"]*)"(?:\^\^<([^>]+)>)?(?:@([a-z-]+))?)\s*\.\s*$'
        )
        
        self.prefix_pattern = re.compile(r'@prefix\s+([^:]*):?\s+<([^>]+)>\s*\.')
        self.base_pattern = re.compile(r'@base\s+<([^>]+)>\s*\.')
        
        self.prefixes = {}
        self.base_uri = ""
        
    def parse_line_safe(self, line: str) -> Optional[Tuple[str, str, str, str]]:
        """Safely parse a single line using regex."""
        try:
            line = line.strip()
            if not line or line.startswith('#'):
                return None
            
            # Handle prefixes
            prefix_match = self.prefix_pattern.match(line)
            if prefix_match:
                prefix, uri = prefix_match.groups()
                self.prefixes[prefix] = uri
                return None
            
            # Handle base
            base_match = self.base_pattern.match(line)
            if base_match:
                self.base_uri = base_match.group(1)
                return None
            
            # Handle N-Triples format
            nt_match = self.ntriples_pattern.match(line)
            if nt_match:
                subject, predicate, obj_uri, obj_literal, obj_datatype, obj_lang = nt_match.groups()
                
                # Determine object value and type
                if obj_uri:
                    obj_value = obj_uri
                    obj_type = "uri"
                elif obj_literal:
                    obj_value = obj_literal
                    obj_type = "literal"
                    if obj_datatype:
                        obj_type = f"literal^^{obj_datatype}"
                    elif obj_lang:
                        obj_type = f"literal@{obj_lang}"
                else:
                    return None
                
                return subject, predicate, obj_value, obj_type
            
            # Try simple TTL triple pattern
            simple_match = re.match(r'([^<>\s]+:\w+|<[^>]+>)\s+([^<>\s]+:\w+|<[^>]+>)\s+(.+?)\s*\.', line)
            if simple_match:
                subject, predicate, obj = simple_match.groups()
                
                # Expand prefixed names
                subject = self._expand_prefixed_name(subject.strip())
                predicate = self._expand_prefixed_name(predicate.strip())
                
                # Handle object
                obj = obj.strip()
                if obj.startswith('<') and obj.endswith('>'):
                    obj_value = obj[1:-1]
                    obj_type = "uri"
                elif obj.startswith('"'):
                    # Simple quoted literal
                    obj_value = obj.strip('"')
                    obj_type = "literal"
                else:
                    obj_value = self._expand_prefixed_name(obj)
                    obj_type = "uri"
                
                return subject, predicate, obj_value, obj_type
            
            return None
            
        except Exception as e:
            logging.debug(f"Error parsing line: {e}")
            return None
    
    def _expand_prefixed_name(self, name: str) -> str:
        """Expand prefixed name to full URI."""
        try:
            if ':' in name and not name.startswith('<'):
                prefix, local = name.split(':', 1)
                if prefix in self.prefixes:
                    return f"{self.prefixes[prefix]}{local}"
            elif name.startswith('<') and name.endswith('>'):
                return name[1:-1]
            return name
        except Exception:
            return name


class FilePreprocessor:
    """Preprocesses large files to identify and handle problematic lines."""
    
    def __init__(self, input_file: str, output_file: str = None):
        self.input_file = Path(input_file)
        self.output_file = Path(output_file) if output_file else self.input_file.with_suffix('.clean.nt')
        self.stats = {
            'total_lines': 0,
            'valid_lines': 0,
            'invalid_lines': 0,
            'empty_lines': 0,
            'comment_lines': 0
        }
    
    def preprocess_file(self) -> str:
        """Preprocess file to clean and validate lines."""
        logging.info(f"Preprocessing {self.input_file} -> {self.output_file}")
        
        with open(self.input_file, 'r', encoding='utf-8', errors='ignore') as infile, \
             open(self.output_file, 'w', encoding='utf-8') as outfile:
            
            for line_num, line in enumerate(tqdm(infile, desc="Preprocessing"), 1):
                self.stats['total_lines'] += 1
                
                try:
                    line = line.strip()
                    
                    # Skip empty lines
                    if not line:
                        self.stats['empty_lines'] += 1
                        continue
                    
                    # Skip comments
                    if line.startswith('#'):
                        self.stats['comment_lines'] += 1
                        continue
                    
                    # Basic validation
                    if self._is_valid_line(line):
                        outfile.write(line + '\n')
                        self.stats['valid_lines'] += 1
                    else:
                        self.stats['invalid_lines'] += 1
                        logging.debug(f"Invalid line {line_num}: {line[:100]}...")
                
                except Exception as e:
                    self.stats['invalid_lines'] += 1
                    logging.debug(f"Error processing line {line_num}: {e}")
        
        logging.info(f"Preprocessing complete: {self.stats}")
        return str(self.output_file)
    
    def _is_valid_line(self, line: str) -> bool:
        """Basic validation of RDF line."""
        try:
            # Check for basic structure
            if not line.endswith('.'):
                return False
            
            # Check for reasonable length
            if len(line) > 50000:  # Skip extremely long lines
                return False
            
            # Check for basic RDF patterns
            if not any(pattern in line for pattern in ['<', '"', 'prefix', '@']):
                return False
            
            return True
            
        except Exception:
            return False


class ExternalToolProcessor:
    """Uses external RDF tools for conversion."""
    
    def __init__(self):
        self.available_tools = self._check_available_tools()
    
    def _check_available_tools(self) -> Dict[str, bool]:
        """Check which external RDF tools are available."""
        tools = {}
        
        # Check for rapper (Redland)
        try:
            subprocess.run(['rapper', '--version'], capture_output=True, check=True)
            tools['rapper'] = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            tools['rapper'] = False
        
        # Check for rdf4j
        try:
            subprocess.run(['rdf4j'], capture_output=True)
            tools['rdf4j'] = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            tools['rdf4j'] = False
        
        # Check for riot (Apache Jena)
        try:
            subprocess.run(['riot', '--version'], capture_output=True, check=True)
            tools['riot'] = True
        except (subprocess.CalledProcessError, FileNotFoundError):
            tools['riot'] = False
        
        logging.info(f"Available external tools: {tools}")
        return tools
    
    def convert_to_ntriples(self, input_file: str, output_file: str = None) -> str:
        """Convert TTL to N-Triples using external tools."""
        if not output_file:
            output_file = Path(input_file).with_suffix('.nt')
        
        if self.available_tools.get('rapper'):
            return self._convert_with_rapper(input_file, output_file)
        elif self.available_tools.get('riot'):
            return self._convert_with_riot(input_file, output_file)
        else:
            raise RuntimeError("No external RDF tools available. Install rapper or riot.")
    
    def _convert_with_rapper(self, input_file: str, output_file: str) -> str:
        """Convert using rapper."""
        logging.info(f"Converting {input_file} to N-Triples using rapper")
        
        cmd = [
            'rapper',
            '-i', 'turtle',
            '-o', 'ntriples',
            input_file
        ]
        
        with open(output_file, 'w') as f:
            result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)
        
        if result.returncode != 0:
            logging.error(f"Rapper conversion failed: {result.stderr}")
            raise RuntimeError(f"Rapper conversion failed")
        
        logging.info(f"Conversion complete: {output_file}")
        return output_file
    
    def _convert_with_riot(self, input_file: str, output_file: str) -> str:
        """Convert using Apache Jena riot."""
        logging.info(f"Converting {input_file} to N-Triples using riot")
        
        cmd = [
            'riot',
            '--output=NT',
            input_file
        ]
        
        with open(output_file, 'w') as f:
            result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True)
        
        if result.returncode != 0:
            logging.error(f"Riot conversion failed: {result.stderr}")
            raise RuntimeError(f"Riot conversion failed")
        
        logging.info(f"Conversion complete: {output_file}")
        return output_file


class ChunkBasedProcessor:
    """Processes large files in isolated chunks to contain errors."""
    
    def __init__(self, chunk_size_mb: int = 100):
        self.chunk_size_bytes = chunk_size_mb * 1024 * 1024
        self.parser = ManualTTLParser()
    
    def process_file_in_chunks(self, file_path: str) -> Iterator[Tuple[str, str, str, str]]:
        """Process file in chunks with error isolation."""
        file_path = Path(file_path)
        file_size = file_path.stat().st_size
        
        logging.info(f"Processing {file_path} ({file_size / 1024**3:.2f} GB) in chunks")
        
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            chunk_num = 0
            while True:
                chunk_num += 1
                chunk_lines = []
                chunk_size = 0
                
                # Read chunk
                while chunk_size < self.chunk_size_bytes:
                    try:
                        line = f.readline()
                        if not line:  # EOF
                            break
                        chunk_lines.append(line)
                        chunk_size += len(line.encode('utf-8'))
                    except Exception as e:
                        logging.warning(f"Error reading line in chunk {chunk_num}: {e}")
                        continue
                
                if not chunk_lines:
                    break
                
                # Process chunk
                logging.info(f"Processing chunk {chunk_num} ({len(chunk_lines):,} lines)")
                
                for line_num, line in enumerate(chunk_lines):
                    try:
                        parsed = self.parser.parse_line_safe(line)
                        if parsed:
                            yield parsed
                    except Exception as e:
                        logging.debug(f"Error in chunk {chunk_num}, line {line_num}: {e}")
                        continue
                
                # Memory cleanup after each chunk
                del chunk_lines
                gc.collect()


class SimpleFalkorDBLoader:
    """Simplified FalkorDB loader with minimal string operations."""
    
    def __init__(self, graph_name: str = "knowledge_graph", host: str = "localhost", port: int = 6379):
        self.graph_name = graph_name
        self.host = host
        self.port = port
        self.db = None
        self.graph = None
        
        # Statistics
        self.nodes_created = 0
        self.edges_created = 0
        
        # Simple ID mapping
        self.resource_to_id = {}
        self.next_id = 0
    
    def connect(self):
        """Connect to FalkorDB."""
        self.db = falkordb.FalkorDB(host=self.host, port=self.port)
        self.graph = self.db.select_graph(self.graph_name)
        logging.info(f"Connected to FalkorDB: {self.graph_name}")
    
    def get_safe_id(self, resource: str) -> str:
        """Get safe ID for resource with minimal string operations."""
        try:
            if resource in self.resource_to_id:
                return self.resource_to_id[resource]
            
            # Create simple hash-based ID
            resource_hash = hashlib.md5(resource.encode('utf-8', errors='ignore')).hexdigest()[:12]
            safe_id = f"n_{resource_hash}"
            
            self.resource_to_id[resource] = safe_id
            return safe_id
            
        except Exception:
            # Fallback to sequential ID
            safe_id = f"n_{self.next_id}"
            self.next_id += 1
            self.resource_to_id[resource] = safe_id
            return safe_id
    
    def load_triple_simple(self, subject: str, predicate: str, obj: str, obj_type: str):
        """Load triple with minimal processing."""
        try:
            subj_id = self.get_safe_id(subject)
            
            # Create subject node if needed
            self._ensure_node_exists(subj_id, subject)
            
            if obj_type == "literal":
                # Add as property
                self._add_property_safe(subj_id, predicate, obj)
            else:
                # Create object node and edge
                obj_id = self.get_safe_id(obj)
                self._ensure_node_exists(obj_id, obj)
                self._create_edge_safe(subj_id, obj_id, predicate)
            
        except Exception as e:
            logging.debug(f"Error loading triple: {e}")
    
    def _ensure_node_exists(self, node_id: str, uri: str):
        """Ensure node exists with minimal operations."""
        try:
            query = "MERGE (n {id: $id}) SET n.uri = $uri"
            self.graph.query(query, {'id': node_id, 'uri': uri})
            self.nodes_created += 1
        except Exception as e:
            logging.debug(f"Error creating node: {e}")
    
    def _add_property_safe(self, node_id: str, predicate: str, value: str):
        """Add property safely."""
        try:
            # Clean property name
            prop_name = predicate.split('/')[-1].split('#')[-1]
            prop_name = re.sub(r'[^a-zA-Z0-9_]', '_', prop_name)[:50]
            
            if not prop_name:
                prop_name = "property"
            
            query = f"MATCH (n {{id: $id}}) SET n.{prop_name} = $value"
            self.graph.query(query, {'id': node_id, 'value': value})
            
        except Exception as e:
            logging.debug(f"Error adding property: {e}")
    
    def _create_edge_safe(self, source_id: str, target_id: str, predicate: str):
        """Create edge safely."""
        try:
            # Clean edge type
            edge_type = predicate.split('/')[-1].split('#')[-1]
            edge_type = re.sub(r'[^a-zA-Z0-9_]', '_', edge_type)[:50]
            
            if not edge_type:
                edge_type = "RELATED"
            
            query = f"""
            MATCH (a {{id: $source}}), (b {{id: $target}})
            MERGE (a)-[r:{edge_type}]->(b)
            """
            
            self.graph.query(query, {'source': source_id, 'target': target_id})
            self.edges_created += 1
            
        except Exception as e:
            logging.debug(f"Error creating edge: {e}")


class AlternativeConverter:
    """Main converter using alternative approaches."""
    
    def __init__(self, approach: str = "manual"):
        self.approach = approach
        self.stats = ProcessingStats()
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('alternative_conversion.log')
            ]
        )
    
    def convert_with_alternative_approach(self, input_file: str, graph_name: str = "knowledge_graph") -> ProcessingStats:
        """Convert using specified alternative approach."""
        self.stats.start_time = time.time()
        
        try:
            if self.approach == "manual":
                return self._convert_with_manual_parser(input_file, graph_name)
            elif self.approach == "external":
                return self._convert_with_external_tools(input_file, graph_name)
            elif self.approach == "preprocess":
                return self._convert_with_preprocessing(input_file, graph_name)
            elif self.approach == "chunks":
                return self._convert_with_chunks(input_file, graph_name)
            else:
                raise ValueError(f"Unknown approach: {self.approach}")
                
        except Exception as e:
            logging.error(f"Conversion failed: {e}")
            raise
        finally:
            self.stats.end_time = time.time()
    
    def _convert_with_manual_parser(self, input_file: str, graph_name: str) -> ProcessingStats:
        """Convert using manual regex-based parser."""
        logging.info("Using manual regex-based parser approach")
        
        parser = ManualTTLParser()
        loader = SimpleFalkorDBLoader(graph_name)
        loader.connect()
        
        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(tqdm(f, desc="Processing with manual parser"), 1):
                self.stats.total_lines = line_num
                
                try:
                    parsed = parser.parse_line_safe(line)
                    if parsed:
                        subject, predicate, obj, obj_type = parsed
                        loader.load_triple_simple(subject, predicate, obj, obj_type)
                        self.stats.processed_lines += 1
                    else:
                        self.stats.skipped_lines += 1
                
                except Exception as e:
                    self.stats.error_lines += 1
                    if self.stats.error_lines % 10000 == 0:
                        logging.warning(f"Errors so far: {self.stats.error_lines}")
                
                # Memory management
                if line_num % 50000 == 0:
                    gc.collect()
                    memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                    self.stats.memory_peak_mb = max(self.stats.memory_peak_mb, memory_mb)
        
        self.stats.nodes_created = loader.nodes_created
        self.stats.edges_created = loader.edges_created
        
        return self.stats
    
    def _convert_with_external_tools(self, input_file: str, graph_name: str) -> ProcessingStats:
        """Convert using external RDF tools."""
        logging.info("Using external tools approach")
        
        # Convert to N-Triples first
        external_processor = ExternalToolProcessor()
        nt_file = external_processor.convert_to_ntriples(input_file)
        
        # Process N-Triples
        return self._convert_with_manual_parser(nt_file, graph_name)
    
    def _convert_with_preprocessing(self, input_file: str, graph_name: str) -> ProcessingStats:
        """Convert with file preprocessing."""
        logging.info("Using preprocessing approach")
        
        # Preprocess file
        preprocessor = FilePreprocessor(input_file)
        clean_file = preprocessor.preprocess_file()
        
        # Process cleaned file
        return self._convert_with_manual_parser(clean_file, graph_name)
    
    def _convert_with_chunks(self, input_file: str, graph_name: str) -> ProcessingStats:
        """Convert using chunk-based processing."""
        logging.info("Using chunk-based processing approach")
        
        chunk_processor = ChunkBasedProcessor()
        loader = SimpleFalkorDBLoader(graph_name)
        loader.connect()
        
        processed_count = 0
        for subject, predicate, obj, obj_type in chunk_processor.process_file_in_chunks(input_file):
            try:
                loader.load_triple_simple(subject, predicate, obj, obj_type)
                processed_count += 1
                
                if processed_count % 10000 == 0:
                    logging.info(f"Processed {processed_count:,} triples")
                    memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                    self.stats.memory_peak_mb = max(self.stats.memory_peak_mb, memory_mb)
                
            except Exception as e:
                self.stats.error_lines += 1
        
        self.stats.processed_lines = processed_count
        self.stats.nodes_created = loader.nodes_created
        self.stats.edges_created = loader.edges_created
        
        return self.stats


def main():
    """Main function with multiple approach options."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Alternative RDF to FalkorDB conversion approaches")
    parser.add_argument('input_file', help='Input TTL/RDF file')
    parser.add_argument('--approach', choices=['manual', 'external', 'preprocess', 'chunks'], 
                       default='manual', help='Conversion approach')
    parser.add_argument('--graph-name', default='knowledge_graph', help='FalkorDB graph name')
    
    args = parser.parse_args()
    
    if not Path(args.input_file).exists():
        print(f"File not found: {args.input_file}")
        return
    
    print(f"Converting {args.input_file} using {args.approach} approach")
    print("This may take a long time for large files...")
    
    try:
        converter = AlternativeConverter(args.approach)
        stats = converter.convert_with_alternative_approach(args.input_file, args.graph_name)
        
        print(f"\n✅ Conversion completed:")
        print(f"   Processing time: {stats.processing_time:.1f} seconds")
        print(f"   Lines processed: {stats.processed_lines:,}")
        print(f"   Nodes created: {stats.nodes_created:,}")
        print(f"   Edges created: {stats.edges_created:,}")
        print(f"   Peak memory: {stats.memory_peak_mb:.1f} MB")
        
        if stats.error_lines > 0:
            error_rate = stats.error_lines / stats.total_lines if stats.total_lines > 0 else 0
            print(f"   Error rate: {error_rate:.2%}")
        
    except Exception as e:
        print(f"❌ Conversion failed: {e}")
        print("Check alternative_conversion.log for details")


if __name__ == "__main__":
    main()
