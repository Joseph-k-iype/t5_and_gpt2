import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
import time
import datetime
from typing import Optional, Any, Dict, List, Union
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI, BadRequestError
from pydantic import BaseModel, Field
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.docstore import Document as LC_DOCUMENT
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
from pydantic import BaseModel, ValidationError, field_validator

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class (enhanced from original)
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        # Set credential for Azure OpenAI
        self.credential = self._get_credential()
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"), 
                client_id=self.get("AZURE_CLIENT_ID"), 
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if is_file_readable(path):
                self.set("REQUESTS_CA_BUNDLE", path)
                self.set("SSL_CERT_FILE", path)
                self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.warning(f"Certificate path not found, skipping: {e}")
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if is_file_readable(dotenvfile):
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
                del temp_dict
        except Exception as e:
            logger.warning(f"Environment file not found, skipping: {dotenvfile}")
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## ISO/IEC 11179 Compliance Response Models
class ISO11179ValidationResult(BaseModel):
    """Response model for ISO/IEC 11179 compliance validation"""
    bde_name: str
    bde_description: str
    compliance_status: str = Field(..., description="Either 'good' or 'poor'")
    overall_score: float = Field(..., description="Overall compliance score from 0.0 to 1.0")
    
    # Detailed scoring for different ISO/IEC 11179 criteria
    naming_score: float = Field(..., description="Score for naming conventions (0.0-1.0)")
    definition_score: float = Field(..., description="Score for definition quality (0.0-1.0)")
    semantic_clarity_score: float = Field(..., description="Score for semantic clarity (0.0-1.0)")
    business_relevance_score: float = Field(..., description="Score for business relevance (0.0-1.0)")
    
    # Detailed compliance issues
    naming_issues: List[str] = Field(default=[], description="List of naming convention issues")
    definition_issues: List[str] = Field(default=[], description="List of definition issues")
    semantic_issues: List[str] = Field(default=[], description="List of semantic clarity issues")
    business_issues: List[str] = Field(default=[], description="List of business relevance issues")
    
    # Enhanced alternatives (if status is 'poor')
    enhanced_name: Optional[str] = Field(None, description="Enhanced BDE name complying with ISO/IEC 11179")
    enhanced_description: Optional[str] = Field(None, description="Enhanced BDE description complying with ISO/IEC 11179")
    enhancement_rationale: Optional[str] = Field(None, description="Explanation of enhancements made")
    
    # ISO/IEC 11179 specific recommendations
    object_class_recommendation: Optional[str] = Field(None, description="Recommended object class")
    property_term_recommendation: Optional[str] = Field(None, description="Recommended property term")
    representation_term_recommendation: Optional[str] = Field(None, description="Recommended representation term")


## ISO/IEC 11179 BDE Validator Class
class ISO11179BDEValidator:
    def __init__(self, config_file: str = CONFIG_PATH, creds_file: str = CREDS_PATH, cert_file: str = CERT_PATH):
        """Initialize the ISO/IEC 11179 BDE Validator"""
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.client = self._setup_azure_client()
        self.deployment_name = self.env.get("BATCH_DEPLOYMENT_NAME", "gpt-4o-mini-global-batch")
        
        # ISO/IEC 11179 compliance criteria
        self.iso_11179_guidelines = self._load_iso_guidelines()
        
    def _setup_azure_client(self) -> AzureOpenAI:
        """Setup Azure OpenAI client for batch processing"""
        try:
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            
            return AzureOpenAI(
                azure_endpoint=self.env.get("AZURE_OPENAI_ENDPOINT"),
                azure_ad_token_provider=token_provider,
                api_version="2025-03-01-preview"  # Latest API version supporting batch
            )
        except Exception as e:
            logger.error(f"Error setting up Azure OpenAI client: {e}")
            raise
    
    def _load_iso_guidelines(self) -> str:
        """Load comprehensive ISO/IEC 11179 guidelines for validation"""
        return """
ISO/IEC 11179 METADATA REGISTRY STANDARDS - COMPLIANCE CRITERIA:

PART 4 - DATA DEFINITION FORMULATION RULES:
1. DEFINITION COMPLETENESS: Definitions must be complete, precise, and unambiguous
2. CONTEXT INDEPENDENCE: Definitions should be understandable without external context
3. NON-CIRCULAR: Definitions must not be circular or self-referencing
4. POSITIVE STATEMENTS: State what something IS, not what it is NOT
5. SINGULAR FORM: Use singular form for object classes and properties
6. BUSINESS TERMINOLOGY: Use business terms rather than technical jargon
7. AVOID SYNONYMS: Do not use synonyms within the same definition
8. PROPER GRAMMAR: Use correct grammar, spelling, and punctuation

PART 5 - NAMING CONVENTIONS:
1. OBJECT CLASS + PROPERTY STRUCTURE: Names should follow Object_Class + Property_Term format
   - Object Class: The thing being described (e.g., Person, Customer, Product)
   - Property Term: The characteristic being measured (e.g., Name, Identifier, Date, Amount)
   
2. REPRESENTATION TERMS: Use standardized representation terms:
   - Identifier: Unique reference (ID, Code, Number)
   - Name: Human-readable designation
   - Date: Calendar date
   - Time: Time of day
   - DateTime: Combined date and time
   - Amount: Monetary value
   - Quantity: Numeric count or measurement
   - Rate: Ratio or percentage
   - Indicator: Boolean or flag values
   - Text: Free-form textual data
   - Code: Coded values from controlled vocabulary

3. NAMING QUALITY CRITERIA:
   - UNIQUENESS: Names must be unique within the registry
   - MEANINGFULNESS: Names should be self-explanatory
   - BREVITY: Concise while maintaining clarity
   - STANDARDIZATION: Use standard terminology and avoid abbreviations
   - CONSISTENCY: Follow consistent patterns across similar elements
   - AVOID TECHNICAL TERMS: Use business language, not system-specific terms

SEMANTIC CLARITY REQUIREMENTS:
1. UNAMBIGUOUS: Only one possible interpretation
2. PRECISE: Exact boundaries and scope defined
3. COMPLETE: All necessary information included
4. VERIFIABLE: Can be tested or validated
5. ATOMIC: Represents a single concept

BUSINESS RELEVANCE CRITERIA:
1. BUSINESS VALUE: Clear business purpose and usage
2. STAKEHOLDER UNDERSTANDING: Understandable by business users
3. OPERATIONAL RELEVANCE: Relates to business operations
4. DECISION SUPPORT: Supports business decision-making

COMMON VIOLATIONS TO AVOID:
- Abbreviations without explanation (e.g., "cust_id" instead of "Customer_Identifier")
- Technical system names (e.g., "field1", "col_a", "temp_var")
- Ambiguous terms (e.g., "data", "info", "value")
- Inconsistent naming patterns
- Missing object class or property terms
- Circular definitions
- Vague or incomplete descriptions
- Use of "and/or" or multiple concepts in single element

ENHANCEMENT RECOMMENDATIONS:
- Convert abbreviations to full terms
- Add missing object class or property components
- Clarify vague terms with specific business meaning
- Standardize representation terms
- Remove technical jargon and system-specific references
- Ensure proper grammatical structure
- Add context for better understanding
"""

    def read_bde_csv(self, csv_path: str) -> pd.DataFrame:
        """Read and validate BDE CSV file"""
        try:
            df = pd.read_csv(csv_path)
            required_columns = ['bde_name', 'bde_description']
            
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"CSV must contain columns: {required_columns}")
            
            # Handle missing values
            df = df.dropna(subset=required_columns)
            
            logger.info(f"Loaded {len(df)} BDE records for validation")
            return df
            
        except Exception as e:
            logger.error(f"Error reading BDE CSV: {e}")
            raise
    
    def create_validation_batch_jsonl(self, bde_df: pd.DataFrame, output_file: str = "iso_validation_batch.jsonl") -> str:
        """Create JSONL file for ISO/IEC 11179 compliance validation"""
        try:
            system_prompt = f"""You are an expert data governance analyst specializing in ISO/IEC 11179 metadata registry standards. Your task is to validate Business Data Element (BDE) names and descriptions for compliance with ISO/IEC 11179 standards.

{self.iso_11179_guidelines}

VALIDATION PROCESS:
1. Analyze the BDE name and description against all ISO/IEC 11179 criteria
2. Score each category (naming, definition, semantic clarity, business relevance) from 0.0 to 1.0
3. Calculate overall compliance score
4. Determine if status is "good" (score ≥ 0.75) or "poor" (score < 0.75)
5. If "poor", provide enhanced alternatives that fully comply with standards
6. Identify specific issues and provide actionable recommendations

SCORING CRITERIA:
- Naming Score (0.0-1.0): Object class + property structure, standard representation terms, no abbreviations
- Definition Score (0.0-1.0): Completeness, clarity, non-circular, positive statements
- Semantic Clarity Score (0.0-1.0): Unambiguous, precise, atomic concept
- Business Relevance Score (0.0-1.0): Business value, stakeholder understanding, operational relevance

Respond in valid JSON format matching the exact structure:
{{
    "bde_name": "string",
    "bde_description": "string",
    "compliance_status": "good|poor",
    "overall_score": 0.0,
    "naming_score": 0.0,
    "definition_score": 0.0,
    "semantic_clarity_score": 0.0,
    "business_relevance_score": 0.0,
    "naming_issues": ["issue1", "issue2"],
    "definition_issues": ["issue1", "issue2"],
    "semantic_issues": ["issue1", "issue2"],
    "business_issues": ["issue1", "issue2"],
    "enhanced_name": "string or null",
    "enhanced_description": "string or null",
    "enhancement_rationale": "string or null",
    "object_class_recommendation": "string or null",
    "property_term_recommendation": "string or null",
    "representation_term_recommendation": "string or null"
}}"""

            # Create JSONL entries
            batch_requests = []
            for idx, row in bde_df.iterrows():
                custom_id = f"iso-validation-{idx}"
                
                user_prompt = f"""Validate this BDE for ISO/IEC 11179 compliance:

BDE Name: {row['bde_name']}
BDE Description: {row['bde_description']}

Provide detailed analysis and scoring according to ISO/IEC 11179 standards."""
                
                request = {
                    "custom_id": custom_id,
                    "method": "POST", 
                    "url": "/chat/completions",
                    "body": {
                        "model": self.deployment_name,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "response_format": {"type": "json_object"},
                        "temperature": 0.1,
                        "max_tokens": 1200
                    }
                }
                batch_requests.append(request)
            
            # Write to JSONL file
            with open(output_file, 'w', encoding='utf-8') as f:
                for request in batch_requests:
                    f.write(json.dumps(request, ensure_ascii=False) + '\n')
            
            logger.info(f"Created ISO/IEC 11179 validation JSONL: {output_file} with {len(batch_requests)} requests")
            return output_file
            
        except Exception as e:
            logger.error(f"Error creating validation JSONL: {e}")
            raise
    
    def upload_batch_file(self, jsonl_file: str) -> str:
        """Upload JSONL file for batch processing"""
        try:
            with open(jsonl_file, "rb") as f:
                file_response = self.client.files.create(
                    file=f,
                    purpose="batch",
                    extra_body={
                        "expires_after": {"seconds": 1209600, "anchor": "created_at"}  # 14 days
                    }
                )
            
            logger.info(f"Uploaded file: {file_response.id}")
            return file_response.id
            
        except Exception as e:
            logger.error(f"Error uploading batch file: {e}")
            raise
    
    def create_batch_job(self, file_id: str) -> str:
        """Create and submit batch job"""
        try:
            batch_response = self.client.batches.create(
                input_file_id=file_id,
                endpoint="/chat/completions",
                completion_window="24h",
                extra_body={
                    "output_expires_after": {"seconds": 1209600, "anchor": "created_at"}  # 14 days
                }
            )
            
            logger.info(f"Created batch job: {batch_response.id}")
            return batch_response.id
            
        except BadRequestError as e:
            if 'token_limit_exceeded' in str(e):
                logger.warning("Token limit exceeded. Consider splitting your batch into smaller files.")
            raise
        except Exception as e:
            logger.error(f"Error creating batch job: {e}")
            raise
    
    def monitor_batch_job(self, batch_id: str, check_interval: int = 60) -> dict:
        """Monitor batch job progress"""
        try:
            status = "validating"
            while status not in ("completed", "failed", "canceled", "expired"):
                time.sleep(check_interval)
                
                batch_response = self.client.batches.retrieve(batch_id)
                status = batch_response.status
                
                logger.info(f"{datetime.datetime.now()} Batch ID: {batch_id}, Status: {status}")
                
                if batch_response.status == "failed":
                    if batch_response.errors:
                        for error in batch_response.errors.data:
                            logger.error(f"Error code {error.code}: {error.message}")
                    break
            
            return batch_response
            
        except Exception as e:
            logger.error(f"Error monitoring batch job: {e}")
            raise
    
    def download_results(self, batch_response, output_file: str = "iso_validation_results.jsonl") -> str:
        """Download and save batch results"""
        try:
            output_file_id = batch_response.output_file_id
            if not output_file_id:
                output_file_id = batch_response.error_file_id
                if not output_file_id:
                    raise ValueError("No output or error file available")
            
            file_response = self.client.files.content(output_file_id)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(file_response.text)
            
            logger.info(f"Downloaded results to: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"Error downloading results: {e}")
            raise
    
    def parse_validation_results(self, results_file: str, output_csv: str = "iso_11179_compliance_report.csv") -> pd.DataFrame:
        """Parse JSONL validation results and create compliance report"""
        try:
            validations = []
            
            with open(results_file, 'r', encoding='utf-8') as f:
                for line in f:
                    result = json.loads(line.strip())
                    
                    if result.get('response') and result['response'].get('body'):
                        response_body = result['response']['body']
                        if response_body.get('choices'):
                            content = response_body['choices'][0]['message']['content']
                            
                            try:
                                validation_data = json.loads(content)
                                
                                # Flatten the validation result for CSV
                                row = {
                                    'custom_id': result['custom_id'],
                                    'bde_name': validation_data.get('bde_name', ''),
                                    'bde_description': validation_data.get('bde_description', ''),
                                    'compliance_status': validation_data.get('compliance_status', ''),
                                    'overall_score': validation_data.get('overall_score', 0.0),
                                    'naming_score': validation_data.get('naming_score', 0.0),
                                    'definition_score': validation_data.get('definition_score', 0.0),
                                    'semantic_clarity_score': validation_data.get('semantic_clarity_score', 0.0),
                                    'business_relevance_score': validation_data.get('business_relevance_score', 0.0),
                                    'naming_issues': '; '.join(validation_data.get('naming_issues', [])),
                                    'definition_issues': '; '.join(validation_data.get('definition_issues', [])),
                                    'semantic_issues': '; '.join(validation_data.get('semantic_issues', [])),
                                    'business_issues': '; '.join(validation_data.get('business_issues', [])),
                                    'enhanced_name': validation_data.get('enhanced_name', ''),
                                    'enhanced_description': validation_data.get('enhanced_description', ''),
                                    'enhancement_rationale': validation_data.get('enhancement_rationale', ''),
                                    'object_class_recommendation': validation_data.get('object_class_recommendation', ''),
                                    'property_term_recommendation': validation_data.get('property_term_recommendation', ''),
                                    'representation_term_recommendation': validation_data.get('representation_term_recommendation', '')
                                }
                                validations.append(row)
                                
                            except json.JSONDecodeError as je:
                                logger.warning(f"Could not parse validation content for {result['custom_id']}: {je}")
                    
                    elif result.get('error'):
                        logger.error(f"Error in result {result['custom_id']}: {result['error']}")
            
            # Convert to DataFrame and save
            df = pd.DataFrame(validations)
            df.to_csv(output_csv, index=False)
            
            # Generate summary statistics
            if not df.empty:
                logger.info(f"ISO/IEC 11179 Compliance Summary:")
                logger.info(f"  Total BDEs Validated: {len(df)}")
                logger.info(f"  Good Compliance: {len(df[df['compliance_status'] == 'good'])}")
                logger.info(f"  Poor Compliance: {len(df[df['compliance_status'] == 'poor'])}")
                logger.info(f"  Average Overall Score: {df['overall_score'].mean():.2f}")
                logger.info(f"  Average Naming Score: {df['naming_score'].mean():.2f}")
                logger.info(f"  Average Definition Score: {df['definition_score'].mean():.2f}")
            
            logger.info(f"Parsed {len(validations)} validation results to: {output_csv}")
            return df
            
        except Exception as e:
            logger.error(f"Error parsing validation results: {e}")
            raise
    
    def generate_compliance_summary(self, df: pd.DataFrame, output_file: str = "iso_11179_summary_report.txt") -> str:
        """Generate a detailed compliance summary report"""
        try:
            if df.empty:
                return "No data available for summary report"
            
            report = []
            report.append("="*80)
            report.append("ISO/IEC 11179 METADATA REGISTRY COMPLIANCE REPORT")
            report.append("="*80)
            report.append(f"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report.append("")
            
            # Overall Statistics
            total_bdes = len(df)
            good_compliance = len(df[df['compliance_status'] == 'good'])
            poor_compliance = len(df[df['compliance_status'] == 'poor'])
            
            report.append("OVERALL COMPLIANCE STATISTICS:")
            report.append("-" * 40)
            report.append(f"Total BDEs Evaluated: {total_bdes}")
            report.append(f"Good Compliance: {good_compliance} ({good_compliance/total_bdes*100:.1f}%)")
            report.append(f"Poor Compliance: {poor_compliance} ({poor_compliance/total_bdes*100:.1f}%)")
            report.append("")
            
            # Score Averages
            report.append("AVERAGE SCORES BY CATEGORY:")
            report.append("-" * 40)
            report.append(f"Overall Score: {df['overall_score'].mean():.3f}")
            report.append(f"Naming Score: {df['naming_score'].mean():.3f}")
            report.append(f"Definition Score: {df['definition_score'].mean():.3f}")
            report.append(f"Semantic Clarity Score: {df['semantic_clarity_score'].mean():.3f}")
            report.append(f"Business Relevance Score: {df['business_relevance_score'].mean():.3f}")
            report.append("")
            
            # Most Common Issues
            all_naming_issues = []
            all_definition_issues = []
            all_semantic_issues = []
            all_business_issues = []
            
            for _, row in df.iterrows():
                if row['naming_issues']:
                    all_naming_issues.extend([issue.strip() for issue in row['naming_issues'].split(';') if issue.strip()])
                if row['definition_issues']:
                    all_definition_issues.extend([issue.strip() for issue in row['definition_issues'].split(';') if issue.strip()])
                if row['semantic_issues']:
                    all_semantic_issues.extend([issue.strip() for issue in row['semantic_issues'].split(';') if issue.strip()])
                if row['business_issues']:
                    all_business_issues.extend([issue.strip() for issue in row['business_issues'].split(';') if issue.strip()])
            
            def top_issues(issues_list, category_name):
                if not issues_list:
                    return f"No {category_name.lower()} issues found"
                
                from collections import Counter
                issue_counts = Counter(issues_list)
                top_5 = issue_counts.most_common(5)
                
                result = [f"TOP {category_name.upper()} ISSUES:"]
                for i, (issue, count) in enumerate(top_5, 1):
                    result.append(f"  {i}. {issue} ({count} occurrences)")
                return "\n".join(result)
            
            report.append(top_issues(all_naming_issues, "Naming"))
            report.append("")
            report.append(top_issues(all_definition_issues, "Definition"))
            report.append("")
            report.append(top_issues(all_semantic_issues, "Semantic"))
            report.append("")
            report.append(top_issues(all_business_issues, "Business"))
            report.append("")
            
            # Worst Performing BDEs
            report.append("LOWEST SCORING BDEs (Require Immediate Attention):")
            report.append("-" * 40)
            worst_bdes = df.nsmallest(5, 'overall_score')[['bde_name', 'overall_score', 'compliance_status']]
            for _, row in worst_bdes.iterrows():
                report.append(f"  • {row['bde_name']} (Score: {row['overall_score']:.3f}, Status: {row['compliance_status']})")
            report.append("")
            
            # Recommendations
            report.append("RECOMMENDED ACTIONS:")
            report.append("-" * 40)
            report.append("1. Prioritize BDEs with 'poor' compliance status for immediate review")
            report.append("2. Focus on naming convention standardization (Object_Class + Property_Term)")
            report.append("3. Enhance definitions to be more complete and business-focused")
            report.append("4. Implement ISO/IEC 11179 training for data stewards")
            report.append("5. Establish governance processes for ongoing compliance monitoring")
            report.append("")
            
            report_text = "\n".join(report)
            
            # Save to file
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            
            logger.info(f"Generated compliance summary report: {output_file}")
            return report_text
            
        except Exception as e:
            logger.error(f"Error generating compliance summary: {e}")
            raise
    
    def run_complete_validation(self, bde_csv_path: str, output_dir: str = "iso_validation_output") -> Dict[str, str]:
        """Run the complete ISO/IEC 11179 compliance validation process"""
        try:
            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            
            # Step 1: Read BDE CSV file
            logger.info("Step 1: Reading BDE CSV file...")
            bde_df = self.read_bde_csv(bde_csv_path)
            
            # Step 2: Create validation batch JSONL
            logger.info("Step 2: Creating ISO/IEC 11179 validation batch...")
            jsonl_file = os.path.join(output_dir, "iso_validation_batch.jsonl")
            self.create_validation_batch_jsonl(bde_df, jsonl_file)
            
            # Step 3: Upload file
            logger.info("Step 3: Uploading batch file...")
            file_id = self.upload_batch_file(jsonl_file)
            
            # Step 4: Create batch job
            logger.info("Step 4: Creating batch job...")
            batch_id = self.create_batch_job(file_id)
            
            # Step 5: Monitor progress
            logger.info("Step 5: Monitoring batch job progress...")
            batch_response = self.monitor_batch_job(batch_id)
            
            # Step 6: Download results
            logger.info("Step 6: Downloading validation results...")
            results_file = os.path.join(output_dir, "iso_validation_results.jsonl")
            self.download_results(batch_response, results_file)
            
            # Step 7: Parse results to CSV
            logger.info("Step 7: Generating compliance report...")
            compliance_csv = os.path.join(output_dir, "iso_11179_compliance_report.csv")
            compliance_df = self.parse_validation_results(results_file, compliance_csv)
            
            # Step 8: Generate summary report
            logger.info("Step 8: Creating summary report...")
            summary_file = os.path.join(output_dir, "iso_11179_summary_report.txt")
            summary_text = self.generate_compliance_summary(compliance_df, summary_file)
            
            result_files = {
                'compliance_report_csv': compliance_csv,
                'summary_report_txt': summary_file,
                'detailed_results_jsonl': results_file,
                'batch_input_jsonl': jsonl_file
            }
            
            logger.info("✅ ISO/IEC 11179 compliance validation completed successfully!")
            logger.info(f"Results available in: {output_dir}")
            
            return result_files
            
        except Exception as e:
            logger.error(f"Error in complete validation process: {e}")
            raise


# Example usage and configuration
def main():
    """Example usage of the ISO/IEC 11179 BDE Validator"""
    try:
        # Initialize the validator
        validator = ISO11179BDEValidator()
        
        # Example BDE CSV file path (update with your actual file path)
        bde_csv_path = "bde_data.csv"  # Contains columns: bde_name, bde_description
        
        # Run complete validation process
        result_files = validator.run_complete_validation(bde_csv_path)
        
        print("\n" + "="*60)
        print("ISO/IEC 11179 COMPLIANCE VALIDATION COMPLETED")
        print("="*60)
        for file_type, file_path in result_files.items():
            print(f"{file_type.replace('_', ' ').title()}: {file_path}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        raise


if __name__ == "__main__":
    main()
