#!/usr/bin/env python3
"""
Query-Driven RDF to FalkorDB Property Graph Converter

This module provides a flexible solution for converting RDF data from SPARQL endpoints
to FalkorDB property graphs using custom SPARQL queries for nodes and edges.

Features:
- Custom SPARQL queries for nodes and edges
- Consistent labeling between nodes and relationships
- Streaming conversion with configurable batching
- Authentication support for secured endpoints
- Comprehensive error handling and monitoring

Dependencies:
    pip install rdflib falkordb urllib3 requests
"""

import logging
import json
import time
from typing import Dict, List, Set, Tuple, Optional, Any, Iterator
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from urllib.parse import urlparse
import re
from datetime import datetime

import rdflib
from rdflib import ConjunctiveGraph, Graph, URIRef, Literal, BNode
from rdflib.plugins.stores import sparqlstore
from rdflib.namespace import RDF, RDFS, OWL, XSD
import falkordb

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class QueryConfig:
    """Configuration for SPARQL queries"""
    nodes_query: str
    edges_query: str
    sparql_endpoint: str
    username: Optional[str] = None
    password: Optional[str] = None
    
    # FalkorDB settings
    falkordb_host: str = 'localhost'
    falkordb_port: int = 6379
    falkordb_password: Optional[str] = None
    graph_name: str = 'rdf_graph'
    
    # Processing settings
    batch_size: int = 1000
    max_retries: int = 3
    timeout: int = 30
    
    # Transformation settings
    use_shortened_uris: bool = True
    preserve_uri_properties: bool = True
    create_indexes: bool = True
    
    # Monitoring
    export_stats: bool = True
    validate_conversion: bool = True

@dataclass
class ConversionStats:
    """Statistics tracking for conversion process"""
    start_time: datetime
    end_time: Optional[datetime] = None
    
    # Query execution stats
    nodes_query_time: float = 0.0
    edges_query_time: float = 0.0
    
    # Processing stats
    total_node_triples: int = 0
    total_edge_triples: int = 0
    processed_nodes: int = 0
    processed_edges: int = 0
    created_nodes: int = 0
    created_relationships: int = 0
    
    # Error tracking
    node_errors: int = 0
    edge_errors: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert stats to dictionary for JSON serialization"""
        result = asdict(self)
        result['start_time'] = self.start_time.isoformat()
        if self.end_time:
            result['end_time'] = self.end_time.isoformat()
            result['duration_seconds'] = (self.end_time - self.start_time).total_seconds()
        return result

class URIProcessor:
    """Handles URI processing and shortening"""
    
    def __init__(self, use_shortened_uris: bool = True):
        self.use_shortened_uris = use_shortened_uris
        self.namespace_map: Dict[str, str] = {}
        self.uri_cache: Dict[str, str] = {}
        self._setup_common_namespaces()
    
    def _setup_common_namespaces(self):
        """Setup common namespace mappings"""
        common_namespaces = {
            'http://www.w3.org/1999/02/22-rdf-syntax-ns#': 'rdf',
            'http://www.w3.org/2000/01/rdf-schema#': 'rdfs',
            'http://www.w3.org/2002/07/owl#': 'owl',
            'http://www.w3.org/2001/XMLSchema#': 'xsd',
            'http://xmlns.com/foaf/0.1/': 'foaf',
            'http://purl.org/dc/elements/1.1/': 'dc',
            'http://purl.org/dc/terms/': 'dct',
            'http://schema.org/': 'schema',
            'http://dbpedia.org/resource/': 'dbr',
            'http://dbpedia.org/ontology/': 'dbo',
            'http://www.w3.org/2004/02/skos/core#': 'skos'
        }
        
        for namespace, prefix in common_namespaces.items():
            self.namespace_map[namespace] = prefix
    
    def add_namespace(self, namespace: str, prefix: str):
        """Add custom namespace mapping"""
        self.namespace_map[namespace] = prefix
    
    def process_uri(self, uri_str: str) -> str:
        """Process URI for use as identifier or label"""
        if not self.use_shortened_uris:
            return uri_str
        
        # Check cache first
        if uri_str in self.uri_cache:
            return self.uri_cache[uri_str]
        
        # Try namespace mapping
        for namespace, prefix in self.namespace_map.items():
            if uri_str.startswith(namespace):
                local_name = uri_str[len(namespace):]
                local_name = self._clean_identifier(local_name)
                result = f"{prefix}_{local_name}" if local_name else prefix
                self.uri_cache[uri_str] = result
                return result
        
        # Fallback: extract from URI structure
        result = self._extract_local_name(uri_str)
        self.uri_cache[uri_str] = result
        return result
    
    def _clean_identifier(self, name: str) -> str:
        """Clean string to be valid identifier"""
        # Replace non-alphanumeric characters with underscores
        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        # Remove leading/trailing underscores
        clean_name = clean_name.strip('_')
        # Ensure it starts with letter or underscore
        if clean_name and clean_name[0].isdigit():
            clean_name = f"_{clean_name}"
        return clean_name or 'unknown'
    
    def _extract_local_name(self, uri: str) -> str:
        """Extract local name from URI"""
        parsed = urlparse(uri)
        
        if parsed.fragment:
            local_name = parsed.fragment
        elif '/' in parsed.path:
            local_name = parsed.path.split('/')[-1]
        elif '#' in uri:
            local_name = uri.split('#')[-1]
        else:
            local_name = uri.split('/')[-1] if '/' in uri else uri
        
        return self._clean_identifier(local_name)

class FalkorDBManager:
    """Manages FalkorDB connection and operations"""
    
    def __init__(self, config: QueryConfig):
        self.config = config
        self.db = None
        self.graph = None
        self._connect()
    
    def _connect(self):
        """Establish connection to FalkorDB"""
        try:
            connect_kwargs = {
                'host': self.config.falkordb_host,
                'port': self.config.falkordb_port
            }
            
            if self.config.falkordb_password:
                connect_kwargs['password'] = self.config.falkordb_password
            
            self.db = falkordb.FalkorDB(**connect_kwargs)
            self.graph = self.db.select_graph(self.config.graph_name)
            logger.info(f"Connected to FalkorDB at {self.config.falkordb_host}:{self.config.falkordb_port}")
        except Exception as e:
            logger.error(f"Failed to connect to FalkorDB: {e}")
            raise
    
    def clear_graph(self):
        """Clear existing graph data"""
        try:
            self.graph.query("MATCH (n) DETACH DELETE n")
            logger.info("Cleared existing graph data")
        except Exception as e:
            logger.error(f"Error clearing graph: {e}")
            raise
    
    def create_indexes(self):
        """Create performance indexes"""
        if not self.config.create_indexes:
            return
        
        index_queries = [
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.uri)",
            "CREATE INDEX IF NOT EXISTS FOR (n) ON (n.id)",
        ]
        
        for query in index_queries:
            try:
                self.graph.query(query)
                logger.debug(f"Created index: {query}")
            except Exception as e:
                logger.warning(f"Could not create index '{query}': {e}")
    
    def execute_query(self, query: str, params: Optional[Dict] = None) -> Any:
        """Execute a Cypher query with error handling"""
        max_retries = self.config.max_retries
        
        for attempt in range(max_retries):
            try:
                return self.graph.query(query, params or {})
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"Query failed after {max_retries} attempts: {e}")
                    raise
                logger.warning(f"Query attempt {attempt + 1} failed, retrying: {e}")
                time.sleep(2 ** attempt)  # Exponential backoff
    
    def get_graph_stats(self) -> Dict[str, int]:
        """Get current graph statistics"""
        try:
            node_result = self.graph.query("MATCH (n) RETURN count(n) as count")
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            rel_result = self.graph.query("MATCH ()-[r]->() RETURN count(r) as count")
            rel_count = rel_result.result_set[0][0] if rel_result.result_set else 0
            
            return {'nodes': node_count, 'relationships': rel_count}
        except Exception as e:
            logger.error(f"Error getting statistics: {e}")
            return {'nodes': 0, 'relationships': 0}

class QueryDrivenConverter:
    """Main converter class using custom SPARQL queries"""
    
    def __init__(self, config: QueryConfig):
        self.config = config
        self.stats = ConversionStats(start_time=datetime.now())
        self.uri_processor = URIProcessor(config.use_shortened_uris)
        self.falkordb_manager = FalkorDBManager(config)
        self.rdf_graph = None
        
        # Processing caches
        self.node_cache: Dict[str, Dict[str, Any]] = {}
        self.predicate_types: Set[str] = set()  # Track predicates for consistent edge types
        
        self._setup_rdf_connection()
    
    def _setup_rdf_connection(self):
        """Setup RDF graph connection to SPARQL endpoint"""
        try:
            auth = None
            if self.config.username and self.config.password:
                auth = (self.config.username, self.config.password)
            
            store = sparqlstore.SPARQLUpdateStore(
                self.config.sparql_endpoint,
                auth=auth,
                timeout=self.config.timeout
            )
            
            self.rdf_graph = ConjunctiveGraph(store=store)
            logger.info(f"Connected to SPARQL endpoint: {self.config.sparql_endpoint}")
            
        except Exception as e:
            logger.error(f"Failed to connect to SPARQL endpoint: {e}")
            raise
    
    def convert(self) -> ConversionStats:
        """Main conversion method"""
        try:
            logger.info("Starting query-driven RDF to FalkorDB conversion...")
            
            # Clear existing data
            self.falkordb_manager.clear_graph()
            
            # Step 1: Process nodes using nodes query
            logger.info("Processing nodes...")
            self._process_nodes()
            
            # Step 2: Process edges using edges query  
            logger.info("Processing edges...")
            self._process_edges()
            
            # Step 3: Create indexes
            self.falkordb_manager.create_indexes()
            
            # Step 4: Validate if requested
            if self.config.validate_conversion:
                self._validate_conversion()
            
            # Step 5: Finalize statistics
            self._finalize_stats()
            
            logger.info("Conversion completed successfully!")
            return self.stats
            
        except Exception as e:
            logger.error(f"Conversion failed: {e}")
            raise
    
    def _process_nodes(self):
        """Process nodes using the custom nodes query"""
        start_time = time.time()
        
        try:
            logger.info(f"Executing nodes query...")
            results = list(self.rdf_graph.query(self.config.nodes_query))
            self.stats.total_node_triples = len(results)
            self.stats.nodes_query_time = time.time() - start_time
            
            logger.info(f"Retrieved {len(results)} node triples, processing in batches...")
            
            # Process in batches
            batch_size = self.config.batch_size
            for i in range(0, len(results), batch_size):
                batch = results[i:i+batch_size]
                self._process_node_batch(batch)
                
                progress = (i + len(batch)) / len(results) * 100
                logger.info(f"Node processing progress: {progress:.1f}%")
            
            # Commit all nodes to FalkorDB
            self._commit_nodes()
            
        except Exception as e:
            logger.error(f"Error processing nodes: {e}")
            raise
    
    def _process_node_batch(self, batch: List[Tuple]):
        """Process a batch of node triples"""
        for triple in batch:
            try:
                if len(triple) >= 3:
                    subject, predicate, obj = triple[0], triple[1], triple[2]
                    self._process_node_triple(subject, predicate, obj)
                    self.stats.processed_nodes += 1
                else:
                    logger.warning(f"Invalid triple format: {triple}")
            except Exception as e:
                logger.warning(f"Error processing node triple {triple}: {e}")
                self.stats.node_errors += 1
    
    def _process_node_triple(self, subject, predicate, obj):
        """Process individual node triple (subject, predicate, object)"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        
        # Ensure node exists in cache
        if subject_uri not in self.node_cache:
            self._create_node_entry(subject_uri, subject)
        
        # Handle the predicate-object relationship
        if predicate_uri == str(RDF.type):
            # rdf:type becomes a node label
            type_label = self.uri_processor.process_uri(str(obj))
            self.node_cache[subject_uri]['labels'].add(type_label)
        else:
            # Regular property
            prop_name = self.uri_processor.process_uri(predicate_uri)
            prop_value = self._convert_value(obj)
            
            # Handle multiple values for the same property
            if prop_name in self.node_cache[subject_uri]['properties']:
                existing = self.node_cache[subject_uri]['properties'][prop_name]
                if isinstance(existing, list):
                    existing.append(prop_value)
                else:
                    self.node_cache[subject_uri]['properties'][prop_name] = [existing, prop_value]
            else:
                self.node_cache[subject_uri]['properties'][prop_name] = prop_value
    
    def _create_node_entry(self, uri: str, rdf_node):
        """Create a new node entry in cache"""
        node_data = {
            'labels': {'Resource'},  # Default label
            'properties': {}
        }
        
        # Always preserve original URI if requested
        if self.config.preserve_uri_properties:
            node_data['properties']['uri'] = uri
        
        # Handle blank nodes
        if isinstance(rdf_node, BNode):
            node_data['labels'].add('BlankNode')
            node_data['properties']['blank_node_id'] = str(rdf_node)
        
        # Add a processed identifier for easier queries
        node_data['properties']['id'] = self.uri_processor.process_uri(uri)
        
        self.node_cache[uri] = node_data
    
    def _convert_value(self, obj) -> Any:
        """Convert RDF value to appropriate Python type"""
        if isinstance(obj, Literal):
            try:
                if obj.datatype:
                    if obj.datatype == XSD.integer:
                        return int(obj)
                    elif obj.datatype in (XSD.decimal, XSD.float, XSD.double):
                        return float(obj)
                    elif obj.datatype == XSD.boolean:
                        return str(obj).lower() in ('true', '1')
                    elif obj.datatype in (XSD.dateTime, XSD.date, XSD.time):
                        return str(obj)  # Keep as string for FalkorDB
                    else:
                        return str(obj)
                else:
                    # Handle language tags
                    if obj.language:
                        return f"{obj}@{obj.language}"
                    return str(obj)
            except Exception as e:
                logger.warning(f"Error converting literal {obj}: {e}")
                return str(obj)
        else:
            # URI or BNode reference
            return str(obj)
    
    def _commit_nodes(self):
        """Commit all cached nodes to FalkorDB"""
        logger.info(f"Committing {len(self.node_cache)} nodes to FalkorDB...")
        
        for uri, node_data in self.node_cache.items():
            try:
                self._create_falkordb_node(uri, node_data)
                self.stats.created_nodes += 1
            except Exception as e:
                logger.error(f"Error creating node {uri}: {e}")
                self.stats.node_errors += 1
        
        logger.info(f"Successfully created {self.stats.created_nodes} nodes")
    
    def _create_falkordb_node(self, uri: str, node_data: Dict[str, Any]):
        """Create individual node in FalkorDB"""
        # Prepare labels - clean and join
        labels = [self.uri_processor._clean_identifier(label) for label in node_data['labels']]
        labels_str = ':'.join(sorted(labels))
        
        # Prepare properties
        properties = dict(node_data['properties'])
        
        # Build Cypher query
        if properties:
            props_clause = ', '.join([f"{k}: ${k}" for k in properties.keys()])
            query = f"CREATE (n:{labels_str} {{{props_clause}}})"
            self.falkordb_manager.execute_query(query, properties)
        else:
            query = f"CREATE (n:{labels_str})"
            self.falkordb_manager.execute_query(query)
    
    def _process_edges(self):
        """Process edges using the custom edges query"""
        start_time = time.time()
        
        try:
            logger.info(f"Executing edges query...")
            results = list(self.rdf_graph.query(self.config.edges_query))
            self.stats.total_edge_triples = len(results)
            self.stats.edges_query_time = time.time() - start_time
            
            logger.info(f"Retrieved {len(results)} edge triples, processing in batches...")
            
            # Process in batches
            batch_size = self.config.batch_size
            for i in range(0, len(results), batch_size):
                batch = results[i:i+batch_size]
                self._process_edge_batch(batch)
                
                progress = (i + len(batch)) / len(results) * 100
                logger.info(f"Edge processing progress: {progress:.1f}%")
            
        except Exception as e:
            logger.error(f"Error processing edges: {e}")
            raise
    
    def _process_edge_batch(self, batch: List[Tuple]):
        """Process a batch of edge triples"""
        for triple in batch:
            try:
                if len(triple) >= 3:
                    subject, predicate, obj = triple[0], triple[1], triple[2]
                    self._process_edge_triple(subject, predicate, obj)
                    self.stats.processed_edges += 1
                else:
                    logger.warning(f"Invalid edge triple format: {triple}")
            except Exception as e:
                logger.warning(f"Error processing edge triple {triple}: {e}")
                self.stats.edge_errors += 1
    
    def _process_edge_triple(self, subject, predicate, obj):
        """Process individual edge triple (subject, predicate, object)"""
        subject_uri = str(subject)
        predicate_uri = str(predicate)
        object_uri = str(obj)
        
        # Create relationship using the predicate as the relationship type
        rel_type = self.uri_processor.process_uri(predicate_uri)
        self.predicate_types.add(rel_type)
        
        # Create the relationship in FalkorDB
        try:
            self._create_falkordb_relationship(subject_uri, rel_type, object_uri, predicate_uri)
            self.stats.created_relationships += 1
        except Exception as e:
            logger.error(f"Error creating relationship {subject_uri} -> {object_uri}: {e}")
            self.stats.edge_errors += 1
    
    def _create_falkordb_relationship(self, subject_uri: str, rel_type: str, object_uri: str, predicate_uri: str):
        """Create relationship in FalkorDB"""
        # Clean relationship type
        clean_rel_type = self.uri_processor._clean_identifier(rel_type)
        
        # Build relationship properties
        rel_props = {}
        if self.config.preserve_uri_properties:
            rel_props['predicate_uri'] = predicate_uri
        
        # Build Cypher query
        if rel_props:
            props_clause = '{' + ', '.join([f"{k}: ${k}" for k in rel_props.keys()]) + '}'
            query = f"""
            MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
            CREATE (s)-[:{clean_rel_type} {props_clause}]->(o)
            """
        else:
            query = f"""
            MATCH (s {{uri: $subject_uri}}), (o {{uri: $object_uri}})
            CREATE (s)-[:{clean_rel_type}]->(o)
            """
        
        params = {
            'subject_uri': subject_uri,
            'object_uri': object_uri,
            **rel_props
        }
        
        self.falkordb_manager.execute_query(query, params)
    
    def _validate_conversion(self):
        """Validate the conversion results"""
        logger.info("Validating conversion results...")
        
        falkor_stats = self.falkordb_manager.get_graph_stats()
        
        if falkor_stats['nodes'] == 0:
            logger.warning("No nodes were created in FalkorDB")
        
        if self.stats.processed_edges > 0 and falkor_stats['relationships'] == 0:
            logger.warning("No relationships were created despite processing edge triples")
        
        logger.info(f"Validation complete: {falkor_stats['nodes']} nodes, {falkor_stats['relationships']} relationships")
    
    def _finalize_stats(self):
        """Finalize conversion statistics"""
        self.stats.end_time = datetime.now()
        
        # Get final counts from FalkorDB
        falkor_stats = self.falkordb_manager.get_graph_stats()
        self.stats.created_nodes = falkor_stats['nodes']
        self.stats.created_relationships = falkor_stats['relationships']
        
        if self.config.export_stats:
            self.export_stats()
    
    def export_stats(self, filename: Optional[str] = None):
        """Export conversion statistics"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"conversion_stats_{self.config.graph_name}_{timestamp}.json"
        
        try:
            stats_data = self.stats.to_dict()
            stats_data['predicate_types_used'] = list(self.predicate_types)
            
            with open(filename, 'w') as f:
                json.dump(stats_data, f, indent=2)
            logger.info(f"Statistics exported to {filename}")
        except Exception as e:
            logger.warning(f"Could not export statistics: {e}")
    
    def get_sample_queries(self) -> List[str]:
        """Generate sample Cypher queries for the converted graph"""
        return [
            "MATCH (n) RETURN labels(n) as node_labels, count(n) as count ORDER BY count DESC LIMIT 10",
            "MATCH ()-[r]->() RETURN type(r) as relationship_type, count(r) as count ORDER BY count DESC LIMIT 10",
            "MATCH (n) RETURN n.id, n.uri LIMIT 5",
            "MATCH (s)-[r]->(o) RETURN s.id, type(r), o.id LIMIT 10",
            "MATCH (n) WHERE size((n)--()) > 0 RETURN n.id, size((n)--()) as degree ORDER BY degree DESC LIMIT 10"
        ]

# Utility functions
def validate_sparql_endpoint(endpoint: str, username: str = None, password: str = None) -> bool:
    """Validate SPARQL endpoint connectivity"""
    try:
        auth = (username, password) if username and password else None
        store = sparqlstore.SPARQLStore(endpoint, auth=auth)
        
        g = Graph(store=store)
        list(g.query("SELECT * WHERE { ?s ?p ?o } LIMIT 1"))
        return True
    except Exception as e:
        logger.error(f"SPARQL endpoint validation failed: {e}")
        return False

def create_sample_queries():
    """Create sample SPARQL queries for common patterns"""
    return {
        'nodes_query_example': """
        # Example nodes query - extracts entities and their properties
        SELECT ?subject ?predicate ?object WHERE {
            ?subject ?predicate ?object .
            # Filter for specific entity types or patterns
            ?subject a ?type .
            FILTER(?type IN (
                <http://xmlns.com/foaf/0.1/Person>,
                <http://schema.org/Organization>,
                <http://dbpedia.org/ontology/Place>
            ))
            # Exclude relationship predicates (those will be in edges query)
            FILTER(?predicate NOT IN (
                <http://xmlns.com/foaf/0.1/knows>,
                <http://schema.org/memberOf>,
                <http://dbpedia.org/ontology/locatedIn>
            ))
        }
        """,
        
        'edges_query_example': """
        # Example edges query - extracts relationships between entities
        SELECT ?subject ?predicate ?object WHERE {
            ?subject ?predicate ?object .
            # Only include relationship predicates
            FILTER(?predicate IN (
                <http://xmlns.com/foaf/0.1/knows>,
                <http://schema.org/memberOf>,
                <http://dbpedia.org/ontology/locatedIn>,
                <http://purl.org/dc/terms/partOf>
            ))
            # Ensure both subject and object are entities (not literals)
            FILTER(isURI(?object))
        }
        """
    }

def main():
    """Example usage"""
    
    # Sample queries (replace with your actual queries)
    sample_queries = create_sample_queries()
    
    config = QueryConfig(
        # Your custom SPARQL queries
        nodes_query=sample_queries['nodes_query_example'],
        edges_query=sample_queries['edges_query_example'],
        
        # SPARQL endpoint configuration
        sparql_endpoint="https://dbpedia.org/sparql",
        username=None,  # Set if authentication required
        password=None,  # Set if authentication required
        
        # FalkorDB configuration
        falkordb_host='localhost',
        falkordb_port=6379,
        graph_name='custom_rdf_graph',
        
        # Processing configuration
        batch_size=1000,
        use_shortened_uris=True,
        preserve_uri_properties=True,
        create_indexes=True,
        validate_conversion=True,
        export_stats=True
    )
    
    try:
        # Validate endpoint
        if not validate_sparql_endpoint(config.sparql_endpoint, config.username, config.password):
            logger.error("Cannot connect to SPARQL endpoint")
            return
        
        # Run conversion
        converter = QueryDrivenConverter(config)
        stats = converter.convert()
        
        # Print results
        print("\n" + "="*50)
        print("CONVERSION COMPLETED")
        print("="*50)
        print(f"Node triples processed: {stats.processed_nodes}")
        print(f"Edge triples processed: {stats.processed_edges}")
        print(f"Nodes created: {stats.created_nodes}")
        print(f"Relationships created: {stats.created_relationships}")
        print(f"Node query time: {stats.nodes_query_time:.2f}s")
        print(f"Edge query time: {stats.edges_query_time:.2f}s")
        
        if stats.end_time:
            duration = (stats.end_time - stats.start_time).total_seconds()
            print(f"Total duration: {duration:.2f}s")
        
        print(f"\nPredicate types used as relationships: {len(converter.predicate_types)}")
        
        print("\nSample Cypher queries to explore your data:")
        for i, query in enumerate(converter.get_sample_queries(), 1):
            print(f"{i}. {query}")
        
    except Exception as e:
        logger.error(f"Conversion failed: {e}")
        raise

if __name__ == "__main__":
    main()
