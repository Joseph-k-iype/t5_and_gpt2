import PyPDF2
import re
import nltk
import os
import uuid
from abc import ABC, abstractmethod
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize.punkt import PunktSentenceTokenizer
from tqdm import tqdm
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter, SentenceTransformersTokenTextSplitter
from langchain.schema import Document as LC_DOCUMENT
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from fastapi import FastAPI, Request, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel as PydanticBaseModel, field_validator
import logging
import uvicorn
import glob
from typing import List, Dict, Any, Optional

# Set up logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Set NLTK data path to custom location
NLTK_DATA_PATH = os.path.abspath("code/library/nltk")
nltk.data.path.insert(0, NLTK_DATA_PATH)

# Configuration paths
CONFIG_PATH = "config.json"
CREDS_PATH = "credentials.json"
CERT_PATH = "cert.pem"

# Utility function for file validation
def is_file_readable(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"File not found: {file_path}")
    if not os.path.isfile(file_path):
        raise ValueError(f"Not a file: {file_path}")
    return True

# Simple document model for embedding
class MyDocument:
    def __init__(self, id, text):
        self.id = id
        self.text = text

# Document processing classes
class PDFProcessor:
    """Class for processing PDF documents into text."""
    
    def __init__(self, directory_path: str = None):
        """Initialize with directory containing PDF files."""
        self.directory_path = directory_path
        
    def set_directory(self, directory_path: str):
        """Set the directory path containing PDF files."""
        self.directory_path = directory_path
        
    def get_pdf_files(self) -> List[str]:
        """Get list of PDF files in the directory."""
        if not self.directory_path:
            raise ValueError("Directory path not set")
        
        pdf_files = glob.glob(f"{self.directory_path}/**/*.pdf", recursive=True)
        if not pdf_files:
            logger.warning(f"No PDF files found in {self.directory_path}")
        
        return pdf_files
        
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from a single PDF file."""
        try:
            is_file_readable(pdf_path)
            text = ""
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            
            if not text.strip():
                logger.warning(f"No text extracted from {pdf_path}")
                
            return text
        except Exception as e:
            logger.error(f"Error extracting text from {pdf_path}: {e}")
            return ""
    
    def batch_process_pdfs(self) -> Dict[str, str]:
        """Process all PDFs in the directory and return a dictionary mapping filenames to texts."""
        pdf_files = self.get_pdf_files()
        result = {}
        
        for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
            text = self.extract_text_from_pdf(pdf_file)
            if text:
                result[os.path.basename(pdf_file)] = text
        
        return result


# Tokenization and Chunking Architecture
class BaseTokenizer(ABC):
    """Abstract base class for text tokenization."""
    
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        """Tokenize text into units (words, sentences, etc.)"""
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """Count the number of tokens in text"""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Return tokenizer name"""
        pass


class BaseChunker(ABC):
    """Abstract base class for text chunking."""
    
    @abstractmethod
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        """Split text into chunks"""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Return chunker name"""
        pass


class WordTokenizer(BaseTokenizer):
    """Simple word-based tokenizer using NLTK."""
    
    def tokenize(self, text: str) -> List[str]:
        return word_tokenize(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "word_tokenizer"


class SentenceTokenizer(BaseTokenizer):
    """Sentence-based tokenizer using NLTK."""
    
    def __init__(self):
        self.tokenizer = PunktSentenceTokenizer()
    
    def tokenize(self, text: str) -> List[str]:
        return self.tokenizer.tokenize(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "sentence_tokenizer"


class RegexTokenizer(BaseTokenizer):
    """Simple regex-based tokenizer."""
    
    def __init__(self, pattern: str = r"\b\w+\b"):
        self.pattern = re.compile(pattern)
    
    def tokenize(self, text: str) -> List[str]:
        return self.pattern.findall(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "regex_tokenizer"


class CharacterChunker(BaseChunker):
    """Character-based chunker using RecursiveCharacterTextSplitter."""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        return self.text_splitter.create_documents([text], metadatas=[metadata])
    
    @property
    def name(self) -> str:
        return "character_chunker"


class SentenceChunker(BaseChunker):
    """Sentence-based chunker using sentence tokenization."""
    
    def __init__(self, chunk_size: int = 10, chunk_overlap: int = 2):
        self.chunk_size = chunk_size  # Number of sentences per chunk
        self.chunk_overlap = chunk_overlap  # Number of overlapping sentences
        self.tokenizer = SentenceTokenizer()
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        sentences = self.tokenizer.tokenize(text)
        chunks = []
        
        for i in range(0, len(sentences), self.chunk_size - self.chunk_overlap):
            # Ensure we don't go beyond the array bounds
            end_idx = min(i + self.chunk_size, len(sentences))
            # Create chunk from sentences
            chunk_text = " ".join(sentences[i:end_idx])
            chunks.append(LC_DOCUMENT(page_content=chunk_text, metadata=metadata.copy()))
            
            # Break if we've reached the end
            if end_idx == len(sentences):
                break
        
        return chunks
    
    @property
    def name(self) -> str:
        return "sentence_chunker"


class TokenChunker(BaseChunker):
    """Token-based chunker using custom tokenizers."""
    
    def __init__(self, tokenizer: BaseTokenizer, chunk_size: int = 500, chunk_overlap: int = 50):
        self.tokenizer = tokenizer
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        tokens = self.tokenizer.tokenize(text)
        chunks = []
        
        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):
            # Ensure we don't go beyond the array bounds
            end_idx = min(i + self.chunk_size, len(tokens))
            # Create chunk from tokens
            chunk_text = " ".join(tokens[i:end_idx])
            chunks.append(LC_DOCUMENT(page_content=chunk_text, metadata=metadata.copy()))
            
            # Break if we've reached the end
            if end_idx == len(tokens):
                break
        
        return chunks
    
    @property
    def name(self) -> str:
        return f"token_chunker_{self.tokenizer.name}"


class ChunkingStrategy:
    """Factory class for creating optimal chunking strategies."""
    
    STRATEGIES = {
        "character": lambda chunk_size, chunk_overlap: CharacterChunker(chunk_size, chunk_overlap),
        "sentence": lambda chunk_size, chunk_overlap: SentenceChunker(chunk_size, chunk_overlap),
        "word": lambda chunk_size, chunk_overlap: TokenChunker(WordTokenizer(), chunk_size, chunk_overlap),
        "regex": lambda chunk_size, chunk_overlap: TokenChunker(RegexTokenizer(), chunk_size, chunk_overlap),
    }
    
    @staticmethod
    def get_chunker(strategy: str = "character", chunk_size: int = 1000, chunk_overlap: int = 200) -> BaseChunker:
        """Get a chunker based on the specified strategy."""
        if strategy not in ChunkingStrategy.STRATEGIES:
            logger.warning(f"Unknown chunking strategy: {strategy}. Using character chunking.")
            strategy = "character"
        
        return ChunkingStrategy.STRATEGIES[strategy](chunk_size, chunk_overlap)
    
    @staticmethod
    def get_optimal_chunker_for_pdf() -> BaseChunker:
        """Get the optimal chunker for PDF documents."""
        # For PDFs, sentence-based chunking often works best due to layout challenges
        return SentenceChunker(chunk_size=10, chunk_overlap=2)


class TextChunker:
    """Enhanced class for chunking document text with multiple strategies."""
    
    def __init__(self, strategy: str = "character", chunk_size: int = 1000, chunk_overlap: int = 200):
        """Initialize with chunking strategy and parameters."""
        self.strategy = strategy
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunker = ChunkingStrategy.get_chunker(strategy, chunk_size, chunk_overlap)
    
    def set_strategy(self, strategy: str):
        """Change the chunking strategy."""
        self.strategy = strategy
        self.chunker = ChunkingStrategy.get_chunker(strategy, self.chunk_size, self.chunk_overlap)
        logger.info(f"Chunking strategy changed to: {strategy}")
    
    def auto_select_strategy(self, text: str):
        """Auto-select optimal chunking strategy based on text characteristics."""
        text_length = len(text)
        
        if text_length < 5000:
            # For short texts, character chunking is fine
            self.set_strategy("character")
        elif re.search(r"^\s*\d+\.\s+", text, re.MULTILINE) is not None:
            # Text with numbered lists or sections, sentence chunking works well
            self.set_strategy("sentence")
        elif len(sent_tokenize(text[:5000])) > 30:
            # Text with many short sentences
            self.set_strategy("sentence")
        else:
            # Default to word tokenization for most texts
            self.set_strategy("word")
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        """Split text into chunks and return as LangChain documents."""
        if not metadata:
            metadata = {}
            
        try:
            return self.chunker.chunk_text(text, metadata)
        except Exception as e:
            logger.error(f"Error chunking text with {self.strategy} strategy: {e}")
            # Fallback to character chunking on failure
            fallback_chunker = CharacterChunker(self.chunk_size, self.chunk_overlap)
            logger.info(f"Falling back to character chunking")
            return fallback_chunker.chunk_text(text, metadata)
    
    def batch_chunk_texts(self, texts_dict: Dict[str, str]) -> List[LC_DOCUMENT]:
        """Process multiple texts and return chunked documents."""
        all_chunks = []
        
        for filename, text in tqdm(texts_dict.items(), desc="Chunking documents"):
            metadata = {"source": filename}
            
            # Auto-select optimal strategy for this text
            self.auto_select_strategy(text)
            logger.info(f"Selected {self.strategy} chunking strategy for {filename}")
            
            chunks = self.chunk_text(text, metadata)
            all_chunks.extend(chunks)
            
        logger.info(f"Created {len(all_chunks)} chunks from {len(texts_dict)} documents")
        return all_chunks


# Embedding client class
class EmbeddingClient:
    """Client for generating embeddings from text."""
    
    def __init__(self, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        """Initialize with Azure API version and embedding model."""
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        
        # In a real implementation, you would initialize the OpenAI/Azure client here
        # For this example, we'll use a dummy implementation
        logger.info(f"Initialized EmbeddingClient with model: {embeddings_model}")
    
    def generate_embeddings(self, document: MyDocument):
        """Generate embeddings for a document.
        
        In a real implementation, this would call the OpenAI/Azure API.
        For this example, we'll return a dummy embedding.
        """
        # Dummy implementation - in real code, call the API
        embedding_size = 3072 if "3-large" in self.embeddings_model else 1536
        embedding = [0.0] * embedding_size  # Dummy embedding
        
        return type('obj', (object,), {
            'id': document.id,
            'embedding': embedding
        })


class ChromaManager:
    """Class for managing ChromaDB vector store."""
    
    def __init__(self, persist_directory: str, embedding_client: EmbeddingClient):
        """Initialize with persistence directory and embedding client."""
        self.persist_directory = persist_directory
        self.embedding_client = embedding_client
        self.embedding_function = self._create_embedding_function()
        self.vectorstore = None
        
        # Disable telemetry as requested
        os.environ["ANONYMIZED_TELEMETRY"] = "False"
    
    def _create_embedding_function(self):
        """Create a custom embedding function that uses our embedding client."""
        return lambda texts: [
            self.embedding_client.generate_embeddings(
                MyDocument(id=str(uuid.uuid4()), text=text)
            ).embedding 
            for text in texts
        ]
    
    def _get_embedding_dims(self) -> int:
        """Get the dimensionality of the embeddings."""
        # text-embedding-3-large has 3072 dimensions
        if "3-large" in self.embedding_client.embeddings_model:
            return 3072
        # text-embedding-3-small has 1536 dimensions
        elif "3-small" in self.embedding_client.embeddings_model:
            return 1536
        # Default for older models
        else:
            return 1536
    
    def init_vectorstore(self):
        """Initialize the vector store."""
        try:
            os.makedirs(self.persist_directory, exist_ok=True)
            
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_function,
                collection_name="pdf_knowledge_base",
                client_settings=Settings(
                    anonymized_telemetry=False
                )
            )
            
            logger.info(f"Vector store initialized at {self.persist_directory}")
            return self.vectorstore
        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            raise
    
    def add_documents(self, documents: List[LC_DOCUMENT]):
        """Add documents to the vector store."""
        if not self.vectorstore:
            self.init_vectorstore()
            
        try:
            self.vectorstore.add_documents(documents)
            self.vectorstore.persist()
            logger.info(f"Added {len(documents)} documents to vector store")
        except Exception as e:
            logger.error(f"Error adding documents to vector store: {e}")
            raise
    
    def similarity_search(self, query: str, k: int = 5) -> List[LC_DOCUMENT]:
        """Search for similar documents."""
        if not self.vectorstore:
            raise ValueError("Vector store not initialized")
            
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            logger.error(f"Error searching vector store: {e}")
            return []
    
    def get_retriever(self, search_kwargs: Dict[str, Any] = None):
        """Get a retriever for the vector store."""
        if not self.vectorstore:
            raise ValueError("Vector store not initialized")
            
        if not search_kwargs:
            search_kwargs = {"k": 5}
            
        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)
    
    def has_documents(self) -> bool:
        """Check if the vector store has documents."""
        if not self.vectorstore:
            return False
            
        try:
            # Try to get collection info to check if it exists and has documents
            collection_info = self.vectorstore._collection.count()
            return collection_info > 0
        except Exception as e:
            logger.error(f"Error checking collection count: {e}")
            return False


class ChatbotConfig(PydanticBaseModel):
    """Configuration for the RAG chatbot."""
    model_name: str = "gpt-4o-mini"
    temperature: float = 0.7
    max_tokens: int = 800
    chunk_size: int = 1000
    chunk_overlap: int = 200
    chunking_strategy: str = "auto"  # auto, character, sentence, word, regex
    k_documents: int = 5
    pdf_directory: str = "./knowledge_base"
    vector_store_dir: str = "./vector_db"
    embeddings_model: str = "text-embedding-3-large"
    
    @field_validator('chunking_strategy')
    def validate_chunking_strategy(cls, v):
        valid_strategies = ['auto', 'character', 'sentence', 'word', 'regex']
        if v not in valid_strategies:
            raise ValueError(f"Chunking strategy must be one of {valid_strategies}")
        return v


# Simple Azure Chatbot mock for testing
class AzureChatbot:
    """Mock Azure Chatbot class."""
    
    def __init__(self, config_file, creds_file, cert_file):
        """Initialize with configuration files."""
        self.config_file = config_file
        self.creds_file = creds_file
        self.cert_file = cert_file
        self.env = {
            "API_VERSION": "2023-05-15"
        }
        
        # Mock LLM for testing
        self.llm = type('obj', (object,), {
            '__call__': lambda self, *args, **kwargs: {"text": "This is a mock response from the LLM."}
        })
        
        logger.info("Initialized AzureChatbot mock")


class RAGChatbot(AzureChatbot):
    """Extension of AzureChatbot to support Retrieval Augmented Generation (RAG)."""
    
    def __init__(self, config_file: str, creds_file: str, cert_file: str, chatbot_config: ChatbotConfig = None):
        """Initialize with configuration files and chatbot config."""
        super().__init__(config_file, creds_file, cert_file)
        
        self.config = chatbot_config or ChatbotConfig()
        
        # Initialize components
        self.pdf_processor = PDFProcessor(self.config.pdf_directory)
        
        # Initialize text chunker with appropriate strategy
        if self.config.chunking_strategy == 'auto':
            self.text_chunker = TextChunker(
                strategy="character",  # Initial strategy, will be auto-selected per document
                chunk_size=self.config.chunk_size, 
                chunk_overlap=self.config.chunk_overlap
            )
        else:
            self.text_chunker = TextChunker(
                strategy=self.config.chunking_strategy,
                chunk_size=self.config.chunk_size, 
                chunk_overlap=self.config.chunk_overlap
            )
        
        # Create embedding client
        self.embedding_client = EmbeddingClient(
            azure_api_version=self.env.get("API_VERSION", "2023-05-15"),
            embeddings_model=self.config.embeddings_model
        )
        
        # Initialize vector store
        self.chroma_manager = ChromaManager(
            persist_directory=self.config.vector_store_dir,
            embedding_client=self.embedding_client
        )
        
        # Flag to track if the vector store has been properly initialized with documents
        self.vector_store_initialized = False
        
        # Initialize QA chain, but don't throw error if vector store is empty
        try:
            self._setup_qa_chain()
        except Exception as e:
            logger.warning(f"Could not initialize QA chain: {e}. You need to process knowledge base first.")
    
    def _setup_qa_chain(self):
        """Set up the QA chain for answering questions."""
        try:
            # Initialize vector store if needed
            if not self.chroma_manager.vectorstore:
                self.chroma_manager.init_vectorstore()
            
            # Check if the vector store has documents
            if not self.chroma_manager.has_documents():
                logger.warning("Vector store has no documents. Process knowledge base before answering questions.")
                self.vector_store_initialized = False
                return
                
            # Get retriever with configured k
            retriever = self.chroma_manager.get_retriever(
                search_kwargs={"k": self.config.k_documents}
            )
            
            # Define RAG prompt template
            template = """You are an intelligent assistant answering questions based on a knowledge base. 
Use the following pieces of context to answer the user's question. 
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}
Answer:"""
            
            qa_prompt = PromptTemplate(
                template=template,
                input_variables=["context", "question"]
            )
            
            # Create QA chain
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": qa_prompt}
            )
            
            self.vector_store_initialized = True
            logger.info("QA chain initialized successfully")
        except Exception as e:
            self.vector_store_initialized = False
            logger.error(f"Error setting up QA chain: {e}")
            raise
    
    def process_knowledge_base(self):
        """Process all PDFs in the knowledge base and add to vector store."""
        try:
            # Process PDFs to extract text
            texts_dict = self.pdf_processor.batch_process_pdfs()
            
            if not texts_dict:
                logger.warning("No text extracted from PDFs")
                return False
            
            # Chunk the texts
            chunked_docs = self.text_chunker.batch_chunk_texts(texts_dict)
            
            if not chunked_docs:
                logger.warning("No chunks created from documents")
                return False
            
            # Add to vector store
            self.chroma_manager.add_documents(chunked_docs)
            
            # Re-initialize QA chain with updated vector store
            self._setup_qa_chain()
            
            return True
        except Exception as e:
            logger.error(f"Error processing knowledge base: {e}")
            return False
    
    def check_vector_store_ready(self):
        """Check if vector store is initialized and has documents."""
        if not self.chroma_manager.vectorstore:
            return False
        
        return self.chroma_manager.has_documents()
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """Answer a question using the RAG approach."""
        try:
            # Check if vector store is ready
            if not self.vector_store_initialized:
                if not self.check_vector_store_ready():
                    return {
                        "question": question,
                        "answer": "Knowledge base not processed yet. Please process the PDF knowledge base first.",
                        "sources": []
                    }
                else:
                    # Re-initialize QA chain if vector store has documents but chain wasn't initialized
                    self._setup_qa_chain()
            
            # Get answer from QA chain
            result = self.qa_chain({"query": question})
            
            # Format sources
            sources = []
            for doc in result.get("source_documents", []):
                sources.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source", "Unknown")
                })
            
            return {
                "question": question,
                "answer": result["result"],
                "sources": sources
            }
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return {
                "question": question,
                "answer": f"Error processing your question: {str(e)}",
                "sources": []
            }


## CLI Interface

def run_cli(config_file: str, creds_file: str, cert_file: str):
    """Run the chatbot in CLI mode."""
    print("Initializing Knowledge Base Chatbot...")
    
    # Configure chatbot
    chatbot_config = ChatbotConfig()
    
    # Allow user to customize configuration
    print("\nCurrent Configuration:")
    for key, value in chatbot_config.dict().items():
        print(f"  {key}: {value}")
    
    customize = input("\nWould you like to customize this configuration? (y/n): ").lower() == 'y'
    
    if customize:
        chatbot_config.pdf_directory = input(f"PDF Directory [{chatbot_config.pdf_directory}]: ") or chatbot_config.pdf_directory
        chatbot_config.vector_store_dir = input(f"Vector Store Directory [{chatbot_config.vector_store_dir}]: ") or chatbot_config.vector_store_dir
        chatbot_config.k_documents = int(input(f"Number of documents to retrieve [{chatbot_config.k_documents}]: ") or chatbot_config.k_documents)
        
        # Chunking strategy selection
        print("\nChunking Strategy Options:")
        print("  auto     - Automatically select optimal strategy per document")
        print("  character - Character-based chunking")
        print("  sentence - Sentence-based chunking")
        print("  word     - Word-based chunking")
        print("  regex    - Regex pattern-based chunking")
        
        strategy = input(f"Chunking Strategy [{chatbot_config.chunking_strategy}]: ") or chatbot_config.chunking_strategy
        if strategy in ['auto', 'character', 'sentence', 'word', 'regex']:
            chatbot_config.chunking_strategy = strategy
        else:
            print(f"Invalid strategy '{strategy}'. Using '{chatbot_config.chunking_strategy}' instead.")
    
    # Initialize chatbot
    chatbot = RAGChatbot(config_file, creds_file, cert_file, chatbot_config)
    
    # Process knowledge base
    process_kb = input("\nProcess knowledge base PDFs? (y/n): ").lower() == 'y'
    if process_kb:
        print("Processing knowledge base... This may take a while.")
        success = chatbot.process_knowledge_base()
        if success:
            print("Knowledge base processed successfully!")
        else:
            print("Failed to process knowledge base.")
    
    # Chat loop
    print("\nChatbot initialized! Type 'exit' to quit.")
    while True:
        question = input("\nQuestion: ")
        if question.lower() in ['exit', 'quit', 'q']:
            break
            
        result = chatbot.answer_question(question)
        
        print(f"\nAnswer: {result['answer']}")
        
        if result['sources']:
            print("\nSources:")
            for i, source in enumerate(result['sources'], 1):
                print(f"  {i}. {source['source']}")


## API Interface

class QuestionRequest(PydanticBaseModel):
    """Request model for question answering API."""
    question: str


class QuestionResponse(PydanticBaseModel):
    """Response model for question answering API."""
    question: str
    answer: str
    sources: List[Dict[str, str]] = []


class ProcessKBRequest(PydanticBaseModel):
    """Request model for processing knowledge base API."""
    pdf_directory: Optional[str] = None


class ProcessKBResponse(PydanticBaseModel):
    """Response model for processing knowledge base API."""
    success: bool
    message: str


class StatusResponse(PydanticBaseModel):
    """Response model for status API."""
    kb_processed: bool
    doc_count: int
    vector_store_path: str


def create_api(config_file: str, creds_file: str, cert_file: str):
    """Create FastAPI application for the chatbot."""
    app = FastAPI(
        title="Knowledge Base PDF Chatbot API",
        description="API for answering questions based on PDF knowledge base",
        version="1.0.0"
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Create chatbot instance
    chatbot = RAGChatbot(config_file, creds_file, cert_file)
    
    @app.post("/answer", response_model=QuestionResponse)
    async def answer_question(request: QuestionRequest):
        """Answer a question using the knowledge base."""
        result = chatbot.answer_question(request.question)
        return result
    
    @app.post("/process-kb", response_model=ProcessKBResponse)
    async def process_knowledge_base(
        request: ProcessKBRequest, 
        background_tasks: BackgroundTasks
    ):
        """Process the knowledge base PDFs."""
        if request.pdf_directory:
            chatbot.pdf_processor.set_directory(request.pdf_directory)
        
        # Process in background
        background_tasks.add_task(chatbot.process_knowledge_base)
        
        return {
            "success": True,
            "message": "Knowledge base processing started in the background"
        }
    
    @app.get("/status", response_model=StatusResponse)
    async def get_status():
        """Get the status of the knowledge base."""
        doc_count = 0
        if chatbot.chroma_manager.vectorstore:
            try:
                doc_count = chatbot.chroma_manager.vectorstore._collection.count()
            except Exception as e:
                logger.error(f"Error getting collection count: {e}")
        
        return {
            "kb_processed": chatbot.vector_store_initialized,
            "doc_count": doc_count,
            "vector_store_path": chatbot.config.vector_store_dir
        }
    
    @app.get("/health")
    async def health_check():
        """Check if the API is healthy."""
        return {"status": "healthy"}
    
    return app


## Main function

def main():
    """Main function to run the chatbot."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Run the Knowledge Base PDF Chatbot")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to certificate file")
    parser.add_argument("--mode", choices=["cli", "api"], default="cli", help="Run mode (cli or api)")
    parser.add_argument("--host", default="127.0.0.1", help="Host for API server")
    parser.add_argument("--port", type=int, default=8000, help="Port for API server")
    
    args = parser.parse_args()
    
    if args.mode == "cli":
        run_cli(args.config, args.creds, args.cert)
    else:
        app = create_api(args.config, args.creds, args.cert)
        uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()
