import PyPDF2
import re
import nltk
import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union
from pathlib import Path
from abc import ABC, abstractmethod
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.tokenize.punkt import PunktSentenceTokenizer
from tqdm import tqdm
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter, SentenceTransformersTokenTextSplitter
from langchain.schema import Document as LC_DOCUMENT
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain, RetrievalQA
from langchain.docstore import Document
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
from fastapi import FastAPI, Request, HTTPException, Depends, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel as PydanticBaseModel, ValidationError, field_validator
import uvicorn
import glob
import ssl
import certifi

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Set NLTK data path to custom location
NLTK_DATA_PATH = os.path.abspath("code/library/nltk")
nltk.data.path.insert(0, NLTK_DATA_PATH)

# Verify NLTK data path is correctly set up
logger.info(f"Using NLTK data from: {NLTK_DATA_PATH}")
if not os.path.exists(os.path.join(NLTK_DATA_PATH, "tokenizers/punkt")):
    logger.warning(f"Punkt tokenizer not found in {NLTK_DATA_PATH}. Some tokenization features may not work.")

# Define paths
ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

# === UTILITY FUNCTIONS ===
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

# === PROVIDED CLASSES ===

class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(tenant_id=self.get("AZURE_TENANT_ID"), client_id=self.get("AZURE_CLIENT_ID"), client_secret=self.get("AZURE_CLIENT_SECRET"))
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                raise FileNotFoundError(f"The file '{dotenvfile}' does not exist or is not readable")
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:  # Fixed var_name to key
                self.var_list.append(key)  # Fixed var_name to key
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


class MyDocument(PydanticBaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}


class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        return AzureOpenAI(token_provider=token_provider, api_version=self.azure_api_version)
    
    def generate_embeddings(self, doc: MyDocument)->MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc


# Document processing classes
class PDFProcessor:
    """Class for processing PDF documents into text."""
    
    def __init__(self, directory_path: str = None):
        """Initialize with directory containing PDF files."""
        self.directory_path = directory_path
        
    def set_directory(self, directory_path: str):
        """Set the directory path containing PDF files."""
        self.directory_path = directory_path
        
    def get_pdf_files(self) -> List[str]:
        """Get list of PDF files in the directory."""
        if not self.directory_path:
            raise ValueError("Directory path not set")
        
        pdf_files = glob.glob(f"{self.directory_path}/**/*.pdf", recursive=True)
        if not pdf_files:
            logger.warning(f"No PDF files found in {self.directory_path}")
        
        return pdf_files
        
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from a single PDF file."""
        try:
            is_file_readable(pdf_path)
            text = ""
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                for page in reader.pages:
                    text += page.extract_text() + "\n"
            
            if not text.strip():
                logger.warning(f"No text extracted from {pdf_path}")
                
            return text
        except Exception as e:
            logger.error(f"Error extracting text from {pdf_path}: {e}")
            return ""
    
    def batch_process_pdfs(self) -> Dict[str, str]:
        """Process all PDFs in the directory and return a dictionary mapping filenames to texts."""
        pdf_files = self.get_pdf_files()
        result = {}
        
        for pdf_file in tqdm(pdf_files, desc="Processing PDFs"):
            text = self.extract_text_from_pdf(pdf_file)
            if text:
                result[os.path.basename(pdf_file)] = text
        
        return result


# Tokenization and Chunking Architecture
class BaseTokenizer(ABC):
    """Abstract base class for text tokenization."""
    
    @abstractmethod
    def tokenize(self, text: str) -> List[str]:
        """Tokenize text into units (words, sentences, etc.)"""
        pass
    
    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """Count the number of tokens in text"""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Return tokenizer name"""
        pass


class BaseChunker(ABC):
    """Abstract base class for text chunking."""
    
    @abstractmethod
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        """Split text into chunks"""
        pass
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Return chunker name"""
        pass


class WordTokenizer(BaseTokenizer):
    """Simple word-based tokenizer using NLTK."""
    
    def tokenize(self, text: str) -> List[str]:
        return word_tokenize(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "word_tokenizer"


class SentenceTokenizer(BaseTokenizer):
    """Sentence-based tokenizer using NLTK."""
    
    def __init__(self):
        self.tokenizer = PunktSentenceTokenizer()
    
    def tokenize(self, text: str) -> List[str]:
        return self.tokenizer.tokenize(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "sentence_tokenizer"


class RegexTokenizer(BaseTokenizer):
    """Simple regex-based tokenizer."""
    
    def __init__(self, pattern: str = r"\b\w+\b"):
        self.pattern = re.compile(pattern)
    
    def tokenize(self, text: str) -> List[str]:
        return self.pattern.findall(text)
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenize(text))
    
    @property
    def name(self) -> str:
        return "regex_tokenizer"


class CharacterChunker(BaseChunker):
    """Character-based chunker using RecursiveCharacterTextSplitter."""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        return self.text_splitter.create_documents([text], metadatas=[metadata])
    
    @property
    def name(self) -> str:
        return "character_chunker"


class SentenceChunker(BaseChunker):
    """Sentence-based chunker using sentence tokenization."""
    
    def __init__(self, chunk_size: int = 10, chunk_overlap: int = 2):
        self.chunk_size = chunk_size  # Number of sentences per chunk
        self.chunk_overlap = chunk_overlap  # Number of overlapping sentences
        self.tokenizer = SentenceTokenizer()
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        sentences = self.tokenizer.tokenize(text)
        chunks = []
        
        for i in range(0, len(sentences), self.chunk_size - self.chunk_overlap):
            # Ensure we don't go beyond the array bounds
            end_idx = min(i + self.chunk_size, len(sentences))
            # Create chunk from sentences
            chunk_text = " ".join(sentences[i:end_idx])
            chunks.append(LC_DOCUMENT(page_content=chunk_text, metadata=metadata.copy()))
            
            # Break if we've reached the end
            if end_idx == len(sentences):
                break
        
        return chunks
    
    @property
    def name(self) -> str:
        return "sentence_chunker"


class TokenChunker(BaseChunker):
    """Token-based chunker using custom tokenizers."""
    
    def __init__(self, tokenizer: BaseTokenizer, chunk_size: int = 500, chunk_overlap: int = 50):
        self.tokenizer = tokenizer
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        if not metadata:
            metadata = {}
        
        tokens = self.tokenizer.tokenize(text)
        chunks = []
        
        for i in range(0, len(tokens), self.chunk_size - self.chunk_overlap):
            # Ensure we don't go beyond the array bounds
            end_idx = min(i + self.chunk_size, len(tokens))
            # Create chunk from tokens
            chunk_text = " ".join(tokens[i:end_idx])
            chunks.append(LC_DOCUMENT(page_content=chunk_text, metadata=metadata.copy()))
            
            # Break if we've reached the end
            if end_idx == len(tokens):
                break
        
        return chunks
    
    @property
    def name(self) -> str:
        return f"token_chunker_{self.tokenizer.name}"


class ChunkingStrategy:
    """Factory class for creating optimal chunking strategies."""
    
    STRATEGIES = {
        "character": lambda chunk_size, chunk_overlap: CharacterChunker(chunk_size, chunk_overlap),
        "sentence": lambda chunk_size, chunk_overlap: SentenceChunker(chunk_size, chunk_overlap),
        "word": lambda chunk_size, chunk_overlap: TokenChunker(WordTokenizer(), chunk_size, chunk_overlap),
        "regex": lambda chunk_size, chunk_overlap: TokenChunker(RegexTokenizer(), chunk_size, chunk_overlap),
    }
    
    @staticmethod
    def get_chunker(strategy: str = "character", chunk_size: int = 1000, chunk_overlap: int = 200) -> BaseChunker:
        """Get a chunker based on the specified strategy."""
        if strategy not in ChunkingStrategy.STRATEGIES:
            logger.warning(f"Unknown chunking strategy: {strategy}. Using character chunking.")
            strategy = "character"
        
        return ChunkingStrategy.STRATEGIES[strategy](chunk_size, chunk_overlap)
    
    @staticmethod
    def get_optimal_chunker_for_pdf() -> BaseChunker:
        """Get the optimal chunker for PDF documents."""
        # For PDFs, sentence-based chunking often works best due to layout challenges
        return SentenceChunker(chunk_size=10, chunk_overlap=2)


class TextChunker:
    """Enhanced class for chunking document text with multiple strategies."""
    
    def __init__(self, strategy: str = "character", chunk_size: int = 1000, chunk_overlap: int = 200):
        """Initialize with chunking strategy and parameters."""
        self.strategy = strategy
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.chunker = ChunkingStrategy.get_chunker(strategy, chunk_size, chunk_overlap)
    
    def set_strategy(self, strategy: str):
        """Change the chunking strategy."""
        self.strategy = strategy
        self.chunker = ChunkingStrategy.get_chunker(strategy, self.chunk_size, self.chunk_overlap)
        logger.info(f"Chunking strategy changed to: {strategy}")
    
    def auto_select_strategy(self, text: str):
        """Auto-select optimal chunking strategy based on text characteristics."""
        text_length = len(text)
        
        if text_length < 5000:
            # For short texts, character chunking is fine
            self.set_strategy("character")
        elif re.search(r"^\s*\d+\.\s+", text, re.MULTILINE) is not None:
            # Text with numbered lists or sections, sentence chunking works well
            self.set_strategy("sentence")
        elif len(sent_tokenize(text[:5000])) > 30:
            # Text with many short sentences
            self.set_strategy("sentence")
        else:
            # Default to word tokenization for most texts
            self.set_strategy("word")
    
    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[LC_DOCUMENT]:
        """Split text into chunks and return as LangChain documents."""
        if not metadata:
            metadata = {}
            
        try:
            return self.chunker.chunk_text(text, metadata)
        except Exception as e:
            logger.error(f"Error chunking text with {self.strategy} strategy: {e}")
            # Fallback to character chunking on failure
            fallback_chunker = CharacterChunker(self.chunk_size, self.chunk_overlap)
            logger.info(f"Falling back to character chunking")
            return fallback_chunker.chunk_text(text, metadata)
    
    def batch_chunk_texts(self, texts_dict: Dict[str, str]) -> List[LC_DOCUMENT]:
        """Process multiple texts and return chunked documents."""
        all_chunks = []
        
        for filename, text in tqdm(texts_dict.items(), desc="Chunking documents"):
            metadata = {"source": filename}
            
            # Auto-select optimal strategy for this text
            self.auto_select_strategy(text)
            logger.info(f"Selected {self.strategy} chunking strategy for {filename}")
            
            chunks = self.chunk_text(text, metadata)
            all_chunks.extend(chunks)
            
        logger.info(f"Created {len(all_chunks)} chunks from {len(texts_dict)} documents")
        return all_chunks


class ChromaManager:
    """Class for managing ChromaDB vector store."""
    
    def __init__(self, persist_directory: str, embedding_client: EmbeddingClient):
        """Initialize with persistence directory and embedding client."""
        self.persist_directory = persist_directory
        self.embedding_client = embedding_client
        self.embedding_function = self._create_embedding_function()
        self.vectorstore = None
        
        # Disable telemetry as requested
        os.environ["ANONYMIZED_TELEMETRY"] = "False"
        
        # Create an empty collection immediately
        try:
            # Force initialization during the constructor
            self.init_vectorstore()
        except Exception as e:
            logger.error(f"Failed to initialize vector store during constructor: {e}")
            # Don't raise here - allow the object to be created
    
    def _create_embedding_function(self):
        """Create a custom embedding function that uses our embedding client."""
        def _embedding_function(texts):
            try:
                # Create a list to store embeddings
                embeddings = []
                for text in texts:
                    # Create a MyDocument instance with the text
                    doc = MyDocument(id=str(uuid.uuid4()), text=text)
                    # Generate embedding and add to list
                    embedding_result = self.embedding_client.generate_embeddings(doc)
                    if not hasattr(embedding_result, 'embedding') or not embedding_result.embedding:
                        logger.error(f"No embedding generated for text: {text[:100]}...")
                        # Return a fake embedding as a fallback
                        dummy_embedding = [0.0] * 1536  # Default dimension
                        if "3-large" in self.embedding_client.embeddings_model:
                            dummy_embedding = [0.0] * 3072
                        embeddings.append(dummy_embedding)
                    else:
                        embeddings.append(embedding_result.embedding)
                return embeddings
            except Exception as e:
                logger.error(f"Error in embedding function: {e}")
                # Return dummy embeddings to avoid breaking initialization
                dimension = 1536  # Default dimension
                if "3-large" in self.embedding_client.embeddings_model:
                    dimension = 3072
                return [[0.0] * dimension for _ in range(len(texts))]
        
        return _embedding_function
    
    def _get_embedding_dims(self) -> int:
        """Get the dimensionality of the embeddings."""
        # text-embedding-3-large has 3072 dimensions
        if "3-large" in self.embedding_client.embeddings_model:
            return 3072
        # text-embedding-3-small has 1536 dimensions
        elif "3-small" in self.embedding_client.embeddings_model:
            return 1536
        # Default for older models
        else:
            return 1536
    
    def init_vectorstore(self):
        """Initialize the vector store."""
        try:
            logger.info(f"Initializing vector store at {self.persist_directory}")
            os.makedirs(self.persist_directory, exist_ok=True)
            
            # Direct function-based approach
            self.vectorstore = Chroma(
                persist_directory=self.persist_directory,
                embedding_function=self.embedding_function,
                collection_name="pdf_knowledge_base",
                client_settings=Settings(
                    anonymized_telemetry=False
                )
            )
            
            if self.vectorstore is None:
                logger.error("Vector store is still None after initialization")
                raise ValueError("Vector store initialization failed - returned None")
                
            logger.info(f"Vector store successfully initialized at {self.persist_directory}")
            return self.vectorstore
        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            # Create basic default vectorstore with a fake embedding function
            try:
                logger.info("Attempting to create a fallback vector store...")
                
                def dummy_embedding_function(texts):
                    dim = self._get_embedding_dims()
                    return [[0.0] * dim for _ in range(len(texts))]
                
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=dummy_embedding_function,
                    collection_name="pdf_knowledge_base_fallback",
                    client_settings=Settings(
                        anonymized_telemetry=False
                    )
                )
                logger.info("Created fallback vector store")
                return self.vectorstore
            except Exception as e2:
                logger.error(f"Fallback vector store creation also failed: {e2}")
                raise e  # Raise the original error
    
    def add_documents(self, documents: List[LC_DOCUMENT]):
        """Add documents to the vector store."""
        if not self.vectorstore:
            self.init_vectorstore()
            
        try:
            self.vectorstore.add_documents(documents)
            self.vectorstore.persist()
            logger.info(f"Added {len(documents)} documents to vector store")
        except Exception as e:
            logger.error(f"Error adding documents to vector store: {e}")
            raise
    
    def similarity_search(self, query: str, k: int = 5) -> List[LC_DOCUMENT]:
        """Search for similar documents."""
        if not self.vectorstore:
            logger.error("Vector store not initialized when attempting similarity search")
            raise ValueError("Vector store not initialized")
            
        try:
            return self.vectorstore.similarity_search(query, k=k)
        except Exception as e:
            logger.error(f"Error searching vector store: {e}")
            return []
    
    def get_retriever(self, search_kwargs: Dict[str, Any] = None):
        """Get a retriever for the vector store."""
        if not self.vectorstore:
            logger.error("Vector store not initialized when attempting to get retriever")
            raise ValueError("Vector store not initialized")
            
        if not search_kwargs:
            search_kwargs = {"k": 5}
            
        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)


# AzureChatbot Class
class AzureChatbot:
    """Base class for Azure OpenAI based chatbots."""
    
    def __init__(self, config_file: str, creds_file: str, cert_file: str):
        """Initialize with configuration and credential files."""
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        
        logger.info("Azure Chatbot initialized successfully")
    
    def _setup_chat_model(self):
        try:
            # Use the credential from OSEnv
            token_provider = get_bearer_token_provider(
                DefaultAzureCredential(),
                "https://cognitiveservices.azure.com/.default"
            )
            
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider
            )
            
            logger.info(f"LLM initialized with model: {model_name}")
        except Exception as e:
            logger.error(f"Error initializing LLM: {e}")
            raise
    
    def generate_response(self, prompt: str) -> str:
        """Generate a response from the LLM."""
        try:
            response = self.llm(prompt)
            return response
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"Error: {str(e)}"
    
    def chat(self, messages: List[Dict[str, str]]) -> str:
        """Conduct a chat with the LLM."""
        try:
            # Convert to prompt
            prompt = self._format_chat_messages(messages)
            return self.generate_response(prompt)
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            return f"Error: {str(e)}"
    
    def _format_chat_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format chat messages into a prompt."""
        prompt = ""
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                prompt += f"System: {content}\n\n"
            elif role == "user":
                prompt += f"User: {content}\n\n"
            elif role == "assistant":
                prompt += f"Assistant: {content}\n\n"
                
        prompt += "Assistant: "
        return prompt


## RAG Chatbot Extension
class ChatbotConfig(PydanticBaseModel):
    """Configuration for the RAG chatbot."""
    model_name: str = "gpt-4o-mini"
    temperature: float = 0.7
    max_tokens: int = 800
    chunk_size: int = 1000
    chunk_overlap: int = 200
    chunking_strategy: str = "auto"  # auto, character, sentence, word, regex
    k_documents: int = 5
    pdf_directory: str = "./knowledge_base"
    vector_store_dir: str = "./vector_db"
    embeddings_model: str = "text-embedding-3-large"
    
    @field_validator('chunking_strategy')
    def validate_chunking_strategy(cls, v):
        valid_strategies = ['auto', 'character', 'sentence', 'word', 'regex']
        if v not in valid_strategies:
            raise ValueError(f"Chunking strategy must be one of {valid_strategies}")
        return v


class RAGChatbot(AzureChatbot):
    """Extension of AzureChatbot to support Retrieval Augmented Generation (RAG)."""
    
    def __init__(self, config_file: str, creds_file: str, cert_file: str, chatbot_config: ChatbotConfig = None):
        """Initialize with configuration files and chatbot config."""
        super().__init__(config_file, creds_file, cert_file)
        
        self.config = chatbot_config or ChatbotConfig()
        
        # Initialize components
        self.pdf_processor = PDFProcessor(self.config.pdf_directory)
        
        # Initialize text chunker with appropriate strategy
        if self.config.chunking_strategy == 'auto':
            self.text_chunker = TextChunker(
                strategy="character",  # Initial strategy, will be auto-selected per document
                chunk_size=self.config.chunk_size, 
                chunk_overlap=self.config.chunk_overlap
            )
        else:
            self.text_chunker = TextChunker(
                strategy=self.config.chunking_strategy,
                chunk_size=self.config.chunk_size, 
                chunk_overlap=self.config.chunk_overlap
            )
        
        # Create embedding client
        logger.info("Creating embedding client...")
        self.embedding_client = EmbeddingClient(
            azure_api_version=self.env.get("API_VERSION", "2023-05-15"),
            embeddings_model=self.config.embeddings_model
        )
        
        # Create vector store directory if it doesn't exist
        os.makedirs(self.config.vector_store_dir, exist_ok=True)
        
        # Initialize vector store
        logger.info(f"Creating ChromaManager with vector store directory: {self.config.vector_store_dir}")
        self.chroma_manager = ChromaManager(
            persist_directory=self.config.vector_store_dir,
            embedding_client=self.embedding_client
        )
        
        # Double-check if vector store is initialized
        if self.chroma_manager.vectorstore is None:
            logger.warning("Vector store is None after ChromaManager initialization, explicitly initializing...")
            try:
                self.chroma_manager.init_vectorstore()
            except Exception as e:
                logger.error(f"Failed to initialize vector store: {e}")
        
        # Log the vector store status
        logger.info(f"Vector store initialized: {self.chroma_manager.vectorstore is not None}")
        
        # Initialize QA chain
        try:
            self._setup_qa_chain()
        except Exception as e:
            logger.error(f"Failed to set up QA chain: {e}")
            logger.warning("Continuing without QA chain - you'll need to process documents before answering questions")
    
    def _setup_qa_chain(self):
        """Set up the QA chain for answering questions."""
        try:
            # Check if vector store is initialized
            if self.chroma_manager.vectorstore is None:
                logger.error("Vector store is None, cannot set up QA chain")
                raise ValueError("Vector store not initialized")
            
            logger.info("Setting up QA chain...")
            
            # Get retriever with configured k, but handle error gracefully
            try:
                retriever = self.chroma_manager.get_retriever(
                    search_kwargs={"k": self.config.k_documents}
                )
            except Exception as e:
                logger.error(f"Failed to get retriever: {e}")
                # Try again with a smaller k value in case that's the issue
                try:
                    logger.info("Trying with a smaller k value (k=1)...")
                    retriever = self.chroma_manager.get_retriever(
                        search_kwargs={"k": 1}
                    )
                except Exception as e2:
                    logger.error(f"Failed to get retriever with k=1: {e2}")
                    raise ValueError(f"Could not create retriever: {e2}")
            
            # Define RAG prompt template
            template = """You are an intelligent assistant answering questions based on a knowledge base. 
Use the following pieces of context to answer the user's question. 
If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context:
{context}

Question: {question}
Answer:"""
            
            qa_prompt = PromptTemplate(
                template=template,
                input_variables=["context", "question"]
            )
            
            # Create QA chain
            logger.info("Creating RetrievalQA chain...")
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=retriever,
                return_source_documents=True,
                chain_type_kwargs={"prompt": qa_prompt}
            )
            
            logger.info("QA chain initialized successfully")
        except Exception as e:
            logger.error(f"Error setting up QA chain: {e}")
            # Set qa_chain to None to indicate it wasn't initialized
            self.qa_chain = None
            raise
    
    def process_knowledge_base(self):
        """Process all PDFs in the knowledge base and add to vector store."""
        # Process PDFs to extract text
        texts_dict = self.pdf_processor.batch_process_pdfs()
        
        if not texts_dict:
            logger.warning("No text extracted from PDFs")
            return False
        
        # Chunk the texts
        chunked_docs = self.text_chunker.batch_chunk_texts(texts_dict)
        
        if not chunked_docs:
            logger.warning("No chunks created from documents")
            return False
        
        # Add to vector store - this will initialize if needed
        self.chroma_manager.add_documents(chunked_docs)
        
        # Re-initialize QA chain with updated vector store
        self._setup_qa_chain()
        
        return True
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """Answer a question using the RAG approach."""
        try:
            # Check if vector store is initialized
            if not self.chroma_manager.vectorstore:
                logger.error("Cannot answer question - vector store not initialized")
                return {
                    "question": question,
                    "answer": "Error: Knowledge base not initialized. Please process documents first.",
                    "sources": []
                }
            
            # Check if qa_chain is initialized
            if not hasattr(self, 'qa_chain') or self.qa_chain is None:
                logger.error("QA chain not initialized")
                # Try to initialize QA chain if possible
                try:
                    logger.info("Attempting to initialize QA chain...")
                    self._setup_qa_chain()
                except Exception as e:
                    logger.error(f"Failed to initialize QA chain: {e}")
                    return {
                        "question": question,
                        "answer": "Error: QA system not initialized. Please process documents first.",
                        "sources": []
                    }
            
            # Get answer from QA chain
            try:
                result = self.qa_chain({"query": question})
            except Exception as e:
                logger.error(f"Error in QA chain: {e}")
                # Fallback to direct LLM response
                fallback_response = self.generate_response(
                    f"The following question was asked, but I couldn't find relevant information in the knowledge base: '{question}'. "
                    "Please respond that you don't have enough information in the knowledge base to answer this question."
                )
                return {
                    "question": question,
                    "answer": fallback_response,
                    "sources": []
                }
            
            # Format sources
            sources = []
            for doc in result.get("source_documents", []):
                sources.append({
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "source": doc.metadata.get("source", "Unknown")
                })
            
            return {
                "question": question,
                "answer": result["result"],
                "sources": sources
            }
        except Exception as e:
            logger.error(f"Error answering question: {e}")
            return {
                "question": question,
                "answer": f"Error processing your question: {str(e)}. Please try processing the knowledge base first.",
                "sources": []
            }


## CLI Interface

def run_cli(config_file: str, creds_file: str, cert_file: str):
    """Run the chatbot in CLI mode."""
    print("Initializing Knowledge Base Chatbot...")
    
    # Configure chatbot
    chatbot_config = ChatbotConfig()
    
    # Allow user to customize configuration
    print("\nCurrent Configuration:")
    for key, value in chatbot_config.model_dump().items():  # Changed dict() to model_dump() for newer Pydantic
        print(f"  {key}: {value}")
    
    customize = input("\nWould you like to customize this configuration? (y/n): ").lower() == 'y'
    
    if customize:
        chatbot_config.pdf_directory = input(f"PDF Directory [{chatbot_config.pdf_directory}]: ") or chatbot_config.pdf_directory
        chatbot_config.vector_store_dir = input(f"Vector Store Directory [{chatbot_config.vector_store_dir}]: ") or chatbot_config.vector_store_dir
        chatbot_config.k_documents = int(input(f"Number of documents to retrieve [{chatbot_config.k_documents}]: ") or chatbot_config.k_documents)
        
        # Chunking strategy selection
        print("\nChunking Strategy Options:")
        print("  auto     - Automatically select optimal strategy per document")
        print("  character - Character-based chunking")
        print("  sentence - Sentence-based chunking")
        print("  word     - Word-based chunking")
        print("  regex    - Regex pattern-based chunking")
        
        strategy = input(f"Chunking Strategy [{chatbot_config.chunking_strategy}]: ") or chatbot_config.chunking_strategy
        if strategy in ['auto', 'character', 'sentence', 'word', 'regex']:
            chatbot_config.chunking_strategy = strategy
        else:
            print(f"Invalid strategy '{strategy}'. Using '{chatbot_config.chunking_strategy}' instead.")
    
    # Initialize chatbot
    chatbot = RAGChatbot(config_file, creds_file, cert_file, chatbot_config)
    
    # Process knowledge base
    process_kb = input("\nProcess knowledge base PDFs? (y/n): ").lower() == 'y'
    if process_kb:
        print("Processing knowledge base... This may take a while.")
        success = chatbot.process_knowledge_base()
        if success:
            print("Knowledge base processed successfully!")
        else:
            print("Failed to process knowledge base.")
    
    # Chat loop
    print("\nChatbot initialized! Type 'exit' to quit.")
    while True:
        question = input("\nQuestion: ")
        if question.lower() in ['exit', 'quit', 'q']:
            break
            
        result = chatbot.answer_question(question)
        
        print(f"\nAnswer: {result['answer']}")
        
        if result['sources']:
            print("\nSources:")
            for i, source in enumerate(result['sources'], 1):
                print(f"  {i}. {source['source']}")


## API Interface

class QuestionRequest(PydanticBaseModel):
    """Request model for question answering API."""
    question: str


class QuestionResponse(PydanticBaseModel):
    """Response model for question answering API."""
    question: str
    answer: str
    sources: List[Dict[str, str]] = []


class ProcessKBRequest(PydanticBaseModel):
    """Request model for processing knowledge base API."""
    pdf_directory: Optional[str] = None


class ProcessKBResponse(PydanticBaseModel):
    """Response model for processing knowledge base API."""
    success: bool
    message: str


def create_api(config_file: str, creds_file: str, cert_file: str):
    """Create FastAPI application for the chatbot."""
    app = FastAPI(
        title="Knowledge Base PDF Chatbot API",
        description="API for answering questions based on PDF knowledge base",
        version="1.0.0"
    )
    
    # Add CORS middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Create chatbot instance
    chatbot = RAGChatbot(config_file, creds_file, cert_file)
    
    @app.post("/answer", response_model=QuestionResponse)
    async def answer_question(request: QuestionRequest):
        """Answer a question using the knowledge base."""
        result = chatbot.answer_question(request.question)
        return result
    
    @app.post("/process-kb", response_model=ProcessKBResponse)
    async def process_knowledge_base(
        request: ProcessKBRequest, 
        background_tasks: BackgroundTasks
    ):
        """Process the knowledge base PDFs."""
        if request.pdf_directory:
            chatbot.pdf_processor.set_directory(request.pdf_directory)
        
        # Process in background
        background_tasks.add_task(chatbot.process_knowledge_base)
        
        return {
            "success": True,
            "message": "Knowledge base processing started in the background"
        }
    
    @app.get("/health")
    async def health_check():
        """Check if the API is healthy."""
        return {"status": "healthy"}
    
    return app


## Main function

def main():
    """Main function to run the chatbot."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Run the Knowledge Base PDF Chatbot")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to certificate file")
    parser.add_argument("--mode", choices=["cli", "api"], default="cli", help="Run mode (cli or api)")
    parser.add_argument("--host", default="127.0.0.1", help="Host for API server")
    parser.add_argument("--port", type=int, default=8000, help="Port for API server")
    
    args = parser.parse_args()
    
    if args.mode == "cli":
        run_cli(args.config, args.creds, args.cert)
    else:
        app = create_api(args.config, args.creds, args.cert)
        uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()
