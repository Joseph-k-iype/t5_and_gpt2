class SemanticColumnMatcher:
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH, 
                 matching_strategies=None, strategy_weights=None, nltk_path=None):
        self.env = OSEnv(config_file, creds_file, cert_file)
        
        # Initialize custom NLTK path if provided
        if nltk_path:
            logger.info(f"Using provided NLTK path: {nltk_path}")
            # Update environment variables for NLTK
            if os.path.exists(nltk_path):
                os.environ['NLTK_DATA'] = nltk_path
                logger.info(f"Set NLTK_DATA environment variable to: {nltk_path}")
                
            # Initialize NLTK resources
            try:
                nltk.download('wordnet', quiet=True, download_dir=nltk_path)
                nltk.download('punkt', quiet=True, download_dir=nltk_path)
                nltk.download('stopwords', quiet=True, download_dir=nltk_path)
            except Exception as e:
                logger.warning(f"Error downloading NLTK resources: {e}")
        
        # Initialize hybrid matcher with specified strategies or defaults
        if matching_strategies is None:
            matching_strategies = [
                MatchingStrategy.SEMANTIC,     # 0.4 weight by default
                MatchingStrategy.LINGUISTIC,   # 0.3 weight by default
                MatchingStrategy.FUZZY,        # 0.2 weight by default
                MatchingStrategy.JARO_WINKLER  # 0.1 weight by default
            ]
            
        # Default weights that favor semantic and linguistic approaches
        if strategy_weights is None:
            strategy_weights = {
                MatchingStrategy.SEMANTIC: 0.4,
                MatchingStrategy.LINGUISTIC: 0.3,
                MatchingStrategy.FUZZY: 0.2,
                MatchingStrategy.JARO_WINKLER: 0.1
            }
        
        self.hybrid_matcher = HybridMatcher(
            strategies=matching_strategies,
            weights=strategy_weights
        )
        
        # Setup other components
        self.setup_components()
        
    def _get_credential(self):
        """Get Azure credential based on environment settings"""
        if str_to_bool(self.env.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.env.get("AZURE_TENANT_ID"), 
                client_id=self.env.get("AZURE_CLIENT_ID"), 
                client_secret=self.env.get("AZURE_CLIENT_SECRET")
            )
    
    def setup_components(self):
        """Initialize all components needed for semantic matching"""
        # Set up Azure Chat LLM
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))
        api_version = self.env.get("API_VERSION", "2023-05-15")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        # Get token provider
        token_provider = get_bearer_token_provider(
            self._get_credential(),
            "https://cognitiveservices.azure.com/.default"
        )
        
        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token_provider=token_provider
        )
        
        # Set up Pydantic output parser
        self.parser = PydanticOutputParser(pydantic_object=MatchingResult)
        
        # Initialize job tracking for async processing
        self.jobs = {}
            
    def detect_file_encoding(self, file_path: str):
        """Detect the encoding of a file using chardet with multiple fallbacks"""
        # Read a sample of the file to detect encoding
        with open(file_path, 'rb') as f:
            # Read up to 1MB for encoding detection
            raw_data = f.read(1024 * 1024)
        
        # Detect encoding using chardet
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"chardet detected {encoding} with {confidence:.2f} confidence for {file_path}")
        
        # If detection failed or has low confidence, try common encodings
        if encoding is None or confidence < 0.7:
            fallback_encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'utf-16']
            
            for enc in fallback_encodings:
                try:
                    # Try reading a few lines with this encoding
                    with open(file_path, 'r', encoding=enc) as f:
                        f.readline()  # Just read the header
                        f.readline()  # And one data line
                    
                    logger.info(f"Fallback to {enc} encoding successful for {file_path}")
                    return enc
                except UnicodeDecodeError:
                    continue
            
            # If all fallbacks fail, use a replacement strategy with utf-8
            logger.warning(f"All encoding detection failed for {file_path}, using utf-8 with replacement")
            return 'utf-8'
        
        return encoding
        
    def load_csv_data(self, source_csv_path: str, target_csv_path: str):
        """Load the two CSV files and prepare them for processing with automatic encoding detection"""
        try:
            # Detect encoding for source CSV
            source_encoding = self.detect_file_encoding(source_csv_path)
            logger.info(f"Detected encoding for source CSV: {source_encoding}")
            
            # Load source CSV (with id, appname, colname) using detected encoding
            self.source_df = pd.read_csv(source_csv_path, encoding=source_encoding)
            logger.info(f"Loaded source CSV with {len(self.source_df)} rows")
            
            # Detect encoding for target CSV
            target_encoding = self.detect_file_encoding(target_csv_path)
            logger.info(f"Detected encoding for target CSV: {target_encoding}")
            
            # Load target CSV (with name and definition) using detected encoding
            self.target_df = pd.read_csv(target_csv_path, encoding=target_encoding)
            logger.info(f"Loaded target CSV with {len(self.target_df)} rows")
            
            # Verify required columns exist
            if not all(col in self.source_df.columns for col in ['id', 'appname', 'colname']):
                raise ValueError("Source CSV missing required columns: id, appname, colname")
                
            if not all(col in self.target_df.columns for col in ['name', 'definition']):
                raise ValueError("Target CSV missing required columns: name, definition")
                
            # Clean data
            self.source_df['colname'] = self.source_df['colname'].str.strip()
            self.target_df['name'] = self.target_df['name'].str.strip()
            self.target_df['definition'] = self.target_df['definition'].fillna('').str.strip()
            
            # Create enriched target data for better matching
            self.target_df['text_for_embedding'] = self.target_df.apply(
                lambda row: f"Column name: {row['name']}. Definition: {row['definition']}", 
                axis=1
            )
            
            # Add NLP features
            logger.info("Generating NLP features for target columns...")
            self.target_df['nlp_features'] = self.target_df['name'].apply(NLPHelper.extract_column_features)
            
            return True
        except Exception as e:
            logger.error(f"Error loading CSV data: {e}")
            raise
    
    def setup_vector_store(self):
        """Set up ChromaDB vector store with your existing EmbeddingClient and MyDocument classes"""
        try:
            # Initialize the embedding client
            embedding_client = EmbeddingClient(
                azure_api_version=self.env.get("API_VERSION", "2023-05-15"),
                embeddings_model=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
            )
            
            # Create in-memory Chroma settings
            chroma_settings = Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
            
            # Create ChromaDB client
            chroma_client = chromadb.Client(settings=chroma_settings)
            
            # Create collection name with a unique identifier
            collection_name = f"column_matcher_{uuid.uuid4().hex[:8]}"
            
            # Create collection
            collection = chroma_client.create_collection(name=collection_name)
            
            # Create documents for each target column using MyDocument
            documents = []
            
            logger.info(f"Creating documents and generating embeddings for {len(self.target_df)} target columns")
            
            for i, row in self.target_df.iterrows():
                doc_id = f"doc_{i}"
                doc_text = f"Column name: {row['name']}. Definition: {row['definition']}"
                doc_metadata = {
                    'name': row['name'],
                    'definition': row['definition']
                }
                
                # Create MyDocument instance
                doc = MyDocument(
                    id=doc_id,
                    text=doc_text,
                    metadata=doc_metadata
                )
                
                # Generate embeddings using EmbeddingClient
                doc_with_embedding = embedding_client.generate_embeddings(doc)
                documents.append(doc_with_embedding)
            
            # Process documents in batches to add to ChromaDB
            batch_size = 100
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i+batch_size]
                
                # Prepare data for batch insertion
                batch_ids = [doc.id for doc in batch_docs]
                batch_embeddings = [doc.embedding for doc in batch_docs]
                batch_texts = [doc.text for doc in batch_docs]
                batch_metadatas = [doc.metadata for doc in batch_docs]
                
                # Add documents to ChromaDB collection
                collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_texts,
                    metadatas=batch_metadatas
                )
                
                logger.info(f"Added batch of {len(batch_docs)} documents to ChromaDB")
            
            # Store ChromaDB collection and client for later use
            self.chroma_collection = collection
            self.chroma_client = chroma_client
            
            # Create a custom ChromaDB retriever
            class CustomChromaRetriever(BaseRetriever):
                """Custom retriever that works with ChromaDB using EmbeddingClient"""
                
                def __init__(self, collection, embedding_client, k=8):
                    super().__init__()
                    self.collection = collection
                    self.embedding_client = embedding_client
                    self.k = k
                
                def _get_relevant_documents(self, query, **kwargs):
                    # Create a document for the query
                    query_doc = MyDocument(
                        id="query",
                        text=query
                    )
                    
                    # Generate embedding for the query
                    query_doc_with_embedding = self.embedding_client.generate_embeddings(query_doc)
                    
                    # Query the collection
                    results = self.collection.query(
                        query_embeddings=[query_doc_with_embedding.embedding],
                        n_results=self.k,
                        include=["documents", "metadatas", "distances"]
                    )
                    
                    # Convert to Document format
                    documents = []
                    if results["ids"] and len(results["ids"][0]) > 0:
                        for i in range(len(results["ids"][0])):
                            doc = Document(
                                page_content=results["documents"][0][i],
                                metadata=results["metadatas"][0][i]
                            )
                            documents.append(doc)
                    
                    return documents
                
                def get_retrieved_docs_with_scores(self, query, k=None):
                    if k is not None:
                        self.k = k
                    
                    # Create a document for the query
                    query_doc = MyDocument(
                        id="query",
                        text=query
                    )
                    
                    # Generate embedding for the query
                    query_doc_with_embedding = self.embedding_client.generate_embeddings(query_doc)
                    
                    # Query the collection
                    results = self.collection.query(
                        query_embeddings=[query_doc_with_embedding.embedding],
                        n_results=self.k,
                        include=["documents", "metadatas", "distances"]
                    )
                    
                    # Convert to Document format with scores
                    documents_with_scores = []
                    if results["ids"] and len(results["ids"][0]) > 0:
                        for i in range(len(results["ids"][0])):
                            doc = Document(
                                page_content=results["documents"][0][i],
                                metadata=results["metadatas"][0][i]
                            )
                            # Convert distance to similarity score
                            distance = results["distances"][0][i]
                            similarity = 1.0 - min(1.0, distance)
                            documents_with_scores.append((doc, similarity, {}))
                    
                    return documents_with_scores
            
            # Create the retriever
            self.retriever = CustomChromaRetriever(
                collection=collection,
                embedding_client=embedding_client,
                k=8
            )
            
            # Save the embedding client for later use
            self.embedding_client = embedding_client
            
            logger.info(f"ChromaDB vector store created with {len(documents)} documents")
            return True
        except Exception as e:
            logger.error(f"Error setting up vector store: {e}")
            raise
    
    def enhance_retrieval(self, source_column, source_app, k=8):
        """Enhance vector retrieval with hybrid matching strategies"""
        # Get initial candidates from ChromaDB retriever
        query = f"Column name: {source_column} from application: {source_app}"
        
        # Use the retriever to get documents with scores
        initial_docs_with_scores = self.retriever.get_retrieved_docs_with_scores(query, k*2)
        
        # If TF-IDF vectorizer not fit yet, fit on all column names
        if not self.hybrid_matcher.tfidf_fit and len(initial_docs_with_scores) > 0:
            all_column_names = [doc.metadata['name'] for doc, _, _ in initial_docs_with_scores]
            self.hybrid_matcher.fit_tfidf(all_column_names)
        
        # Calculate hybrid similarity scores for each candidate
        scores = []
        for doc, chroma_score, _ in initial_docs_with_scores:
            target_name = doc.metadata['name']
            definition = doc.metadata.get('definition', '')
            
            # Calculate hybrid similarity using multiple strategies
            hybrid_results = self.hybrid_matcher.compute_similarity(
                source_column, 
                target_name
            )
            
            # Extract ensemble score and blend with ChromaDB score
            combined_score = 0.7 * hybrid_results['ensemble'] + 0.3 * chroma_score
            
            # Also compute similarity between source column and definition
            # if definition is available
            if definition:
                definition_sim = self.hybrid_matcher.compute_similarity(
                    source_column, 
                    definition
                )
                
                # Blend name similarity with definition similarity
                final_score = 0.7 * combined_score + 0.3 * definition_sim['ensemble']
            else:
                final_score = combined_score
                
            # Store document, score, and detailed results
            scores.append((doc, final_score, hybrid_results))
        
        # Sort by combined score and take top k
        scores.sort(key=lambda x: x[1], reverse=True)
        top_docs = [item[0] for item in scores[:k]]
        
        # Return the documents with their scores
        return top_docs, scores[:k]
    
    def create_rag_chain(self):
        """Create the RAG prompt chain for semantic matching"""
        # Define the semantic matching prompt template with format instructions
        template = """You are a semantic column matching expert. You need to find the most semantically similar matches 
between a source column name and potential target columns.

SOURCE COLUMN: {source_column}
SOURCE APPLICATION: {source_app}

POTENTIAL MATCHES:
{retrieved_documents}

INSTRUCTIONS:
1. Carefully analyze the semantic meaning of the source column name.
2. Consider the application context: {source_app}
3. Evaluate each potential match based on:
   - Semantic similarity to the source column name
   - How well the definition aligns with the likely purpose of the source column
   - Common database naming conventions and abbreviations
4. Think step-by-step to determine each match's relevance.
5. For each match, provide:
   - A confidence score between 0.0 and 1.0
   - A detailed justification
   - If the match isn't ideal, suggest a better name and explain why
6. Rank the top 4 matches in order of confidence score.

{format_instructions}
"""
        
        # Format instructions for Pydantic output
        format_instructions = self.parser.get_format_instructions()
        
        # Format the retrieved documents for the prompt
        def format_docs(docs_with_scores):
            result = []
            for i, (doc, combined_score, similarity_details) in enumerate(docs_with_scores):
                name = doc.metadata['name']
                definition = doc.metadata.get('definition', '')
                
                # Format similarity metrics
                sim_text = f"Similarity Score: {combined_score:.3f}"
                
                entry = f"MATCH {i+1}:\nName: {name}\nDefinition: {definition}\n{sim_text}\n"
                result.append(entry)
            
            return "\n".join(result)
        
        # Create the RAG chain
        self.rag_chain = (
            {
                "source_column": itemgetter("source_column"),
                "source_app": itemgetter("source_app"),
                "retrieved_documents": itemgetter("docs_with_scores") | format_docs,
                "format_instructions": lambda _: format_instructions
            }
            | ChatPromptTemplate.from_template(template)
            | self.llm
            | self.parser
        )
        
        logger.info("RAG chain created for semantic matching")
        return True

    def setup_agent_graph(self):
        """Create a LangGraph workflow for agent-based selection"""
        
        # Define state
        class AgentState(BaseModel):
            source_column: str
            source_app: str
            source_id: str
            retrieved_docs: List[Document] = []
            docs_with_scores: List[Tuple[Document, float, Dict]] = []
            matching_result: Optional[MatchingResult] = None
            
        # Define the retrieval node
        def retrieve_and_analyze(state):
            # Enhanced retrieval using ChromaDB and hybrid matching
            docs, docs_with_scores = self.enhance_retrieval(
                state.source_column, 
                state.source_app
            )
            
            return {
                "retrieved_docs": docs,
                "docs_with_scores": docs_with_scores
            }
        
        # Define the reasoning and selection node
        def reason_and_select(state):
            # Use the RAG chain to analyze and select matches
            matching_result = self.rag_chain.invoke({
                "source_column": state.source_column,
                "source_app": state.source_app,
                "docs_with_scores": state.docs_with_scores
            })
            
            return {"matching_result": matching_result}
        
        # Build the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("retrieve_and_analyze", retrieve_and_analyze)
        workflow.add_node("reason_and_select", reason_and_select)
        
        # Add edges
        workflow.add_edge("retrieve_and_analyze", "reason_and_select")
        workflow.add_edge("reason_and_select", END)
        
        # Set entry point
        workflow.set_entry_point("retrieve_and_analyze")
        
        # Compile the graph
        self.agent_graph = workflow.compile()
        
        logger.info("Agent graph created for column matching")
        return True
    
    def match_columns(self):
        """Process all source columns and find their semantic matches"""
        try:
            results = []
            
            for _, row in self.source_df.iterrows():
                source_id = row['id']
                source_app = row['appname']
                source_column = row['colname']
                
                logger.info(f"Processing column: {source_column} from app: {source_app}")
                
                # Execute the agent graph
                final_state = self.agent_graph.invoke({
                    "source_column": source_column,
                    "source_app": source_app,
                    "source_id": source_id
                })
                
                # Store results
                result = {
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "matching_result": final_state.matching_result
                }
                
                results.append(result)
                
            self.matching_results = results
            logger.info(f"Completed matching for {len(results)} source columns")
            return results
        except Exception as e:
            logger.error(f"Error during column matching: {e}")
            raise
    
    def format_results(self):
        """Format the matching results into a pandas DataFrame"""
        formatted_results = []
        
        for result in self.matching_results:
            source_id = result["source_id"]
            source_app = result["source_app"]
            source_column = result["source_column"]
            matching_result = result["matching_result"]
            
            for i, match in enumerate(matching_result.matches):
                formatted_results.append({
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "target_column": match.name,
                    "rank": i + 1,
                    "score": match.score,
                    "justification": match.justification,
                    "suggested_name": match.suggested_name if match.suggested_name else "",
                    "suggested_name_reason": match.suggested_name_reason if match.suggested_name_reason else ""
                })
        
        return pd.DataFrame(formatted_results)
    
    def save_results(self, output_path: str):
        """Save the matching results to a CSV file"""
        results_df = self.format_results()
        results_df.to_csv(output_path, index=False)
        logger.info(f"Results saved to {output_path}")
        return True
    
    def run_full_pipeline(self, source_csv_path: str, target_csv_path: str, output_path: str):
        """Run the complete semantic column matching pipeline"""
        try:
            logger.info("Starting semantic column matching pipeline")
            
            # Step 1: Load CSV data
            self.load_csv_data(source_csv_path, target_csv_path)
            
            # Step 2: Set up vector store with EmbeddingClient and MyDocument
            self.setup_vector_store()
            
            # Step 3: Create RAG chain
            self.create_rag_chain()
            
            # Step 4: Set up agent graph
            self.setup_agent_graph()
            
            # Step 5: Run matching process
            self.match_columns()
            
            # Step 6: Save results
            self.save_results(output_path)
            
            logger.info("Pipeline completed successfully")
            return self.format_results()
        except Exception as e:
            logger.error(f"Error in pipeline: {e}")
            raise






def setup_vector_store(self):
    """Set up ChromaDB vector store with manual embedding to avoid version compatibility issues"""
    try:
        # Initialize the embedding function
        embedding_func = AzureOpenAIEmbeddings(
            azure_deployment=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large"),
            openai_api_version=self.env.get("API_VERSION", "2023-05-15"),
            azure_endpoint=self.env.get("AZURE_ENDPOINT", ""),
            openai_api_type="azure",
            azure_ad_token_provider=get_bearer_token_provider(
                self._get_credential(),
                "https://cognitiveservices.azure.com/.default"
            )
        )
        
        # Create in-memory Chroma settings
        chroma_settings = Settings(
            anonymized_telemetry=False,
            allow_reset=True
        )
        
        # Create ChromaDB client
        chroma_client = chromadb.Client(settings=chroma_settings)
        
        # Create collection name with a unique identifier
        collection_name = f"column_matcher_{uuid.uuid4().hex[:8]}"
        
        # Create collection
        collection = chroma_client.create_collection(name=collection_name)
        
        # Prepare data for batch insertion
        ids = []
        documents = []
        metadatas = []
        
        # Create documents for each target column
        for i, row in self.target_df.iterrows():
            doc_id = f"doc_{i}"
            doc_text = f"Column name: {row['name']}. Definition: {row['definition']}"
            doc_metadata = {
                'name': row['name'],
                'definition': row['definition']
            }
            
            ids.append(doc_id)
            documents.append(doc_text)
            metadatas.append(doc_metadata)
        
        # Process documents in batches to generate embeddings
        batch_size = 100
        for i in range(0, len(documents), batch_size):
            batch_docs = documents[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            batch_metadatas = metadatas[i:i+batch_size]
            
            # Generate embeddings for this batch using AzureOpenAIEmbeddings
            batch_embeddings = []
            for doc in batch_docs:
                embedding = embedding_func.embed_query(doc)
                batch_embeddings.append(embedding)
            
            # Add documents to collection
            collection.add(
                ids=batch_ids,
                embeddings=batch_embeddings,
                documents=batch_docs,
                metadatas=batch_metadatas
            )
            
            logger.info(f"Added batch of {len(batch_docs)} documents to ChromaDB")
        
        # Store ChromaDB collection and client for later use
        self.chroma_collection = collection
        self.chroma_client = chroma_client
        
        # Create a custom retriever class for compatibility with existing code
        class SimpleChromaRetriever(BaseRetriever):
            """A simple retriever that works with ChromaDB directly"""
            
            collection: Any
            embedding_function: Any
            k: int = 8
            
            class Config:
                arbitrary_types_allowed = True
            
            def __init__(self, collection, embedding_function, k=8):
                super().__init__()
                self.collection = collection
                self.embedding_function = embedding_function
                self.k = k
            
            def _get_relevant_documents(self, query, **kwargs):
                # Generate embedding for query
                query_embedding = self.embedding_function.embed_query(query)
                
                # Query collection
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=self.k,
                    include=["documents", "metadatas", "distances"]
                )
                
                # Convert to LangChain Document format
                documents = []
                if results["ids"] and len(results["ids"][0]) > 0:
                    for i in range(len(results["ids"][0])):
                        doc = Document(
                            page_content=results["documents"][0][i],
                            metadata=results["metadatas"][0][i]
                        )
                        documents.append(doc)
                
                return documents
            
            def get_retrieved_docs_with_scores(self, query, k=None):
                if k is not None:
                    self.k = k
                
                # Generate embedding for query
                query_embedding = self.embedding_function.embed_query(query)
                
                # Query collection
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=self.k,
                    include=["documents", "metadatas", "distances"]
                )
                
                # Convert to LangChain Document format with scores
                documents_with_scores = []
                if results["ids"] and len(results["ids"][0]) > 0:
                    for i in range(len(results["ids"][0])):
                        doc = Document(
                            page_content=results["documents"][0][i],
                            metadata=results["metadatas"][0][i]
                        )
                        # Convert distance to similarity score
                        distance = results["distances"][0][i]
                        similarity = 1.0 - min(1.0, distance)
                        documents_with_scores.append((doc, similarity, {}))
                
                return documents_with_scores
        
        # Create the retriever
        self.retriever = SimpleChromaRetriever(collection, embedding_func)
        
        logger.info(f"ChromaDB vector store created with {len(documents)} documents")
        return True
    except Exception as e:
        logger.error(f"Error setting up vector store: {e}")
        raise

def run_full_pipeline(self, source_csv_path: str, target_csv_path: str, output_path: str):
    """Run the complete semantic column matching pipeline with fallbacks"""
    try:
        logger.info("Starting semantic column matching pipeline")
        
        # Step 1: Load CSV data
        self.load_csv_data(source_csv_path, target_csv_path)
        
        # Step 2: Set up vector store with fallbacks
        vector_store_success = False
        
        # First try: Updated LangChain Chroma approach
        try:
            logger.info("Attempting to set up vector store using updated LangChain Chroma approach...")
            from setup_vector_store import setup_vector_store
            setup_vector_store(self)
            vector_store_success = True
            logger.info("Successfully set up vector store using updated LangChain Chroma approach")
        except Exception as e:
            logger.warning(f"Updated LangChain Chroma approach failed: {e}")
        
        # Second try: Direct ChromaDB approach if first attempt failed
        if not vector_store_success:
            try:
                logger.info("Attempting to set up vector store using direct ChromaDB approach...")
                from setup_vector_store_direct import setup_vector_store_direct
                setup_vector_store_direct(self)
                vector_store_success = True
                logger.info("Successfully set up vector store using direct ChromaDB approach")
            except Exception as e:
                logger.warning(f"Direct ChromaDB approach failed: {e}")
        
        # If all attempts failed, raise exception
        if not vector_store_success:
            raise ValueError("All vector store setup attempts failed")
        
        # Step 3: Create RAG chain
        self.create_rag_chain()
        
        # Step 4: Set up agent graph
        self.setup_agent_graph()
        
        # Step 5: Run matching process
        self.match_columns()
        
        # Step 6: Save results
        self.save_results(output_path)
        
        logger.info("Pipeline completed successfully")
        return self.format_results()
    except Exception as e:
        logger.error(f"Error in pipeline: {e}")
        raise




import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Any, List, Dict, Tuple, Optional, Union
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel, Field, ValidationError, field_validator

# LangChain imports
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.docstore.document import Document
from langchain.embeddings.base import Embeddings
from langchain_community.vectorstores import Chroma as LangChainChroma
from langchain_community.embeddings import AzureOpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.retrievers import BaseRetriever
from operator import itemgetter
from langgraph.graph import END, StateGraph

# ChromaDB imports
import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

# NLP libraries
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import spacy
import difflib
import jellyfish
from fuzzywuzzy import fuzz
import statistics
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter, namedtuple
import re
import time
import asyncio
from datetime import datetime
from enum import Enum
import itertools

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Configuration paths
ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(tenant_id=self.get("AZURE_TENANT_ID"), client_id=self.get("AZURE_CLIENT_ID"), client_secret=self.get("AZURE_CLIENT_SECRET"))
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
                del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:  # Fixed: Changed var_name to key
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")

# Initialize NLP components with proper fallbacks
def initialize_nlp():
    """Initialize NLP components with fallbacks"""
    global nlp, lemmatizer
    
    # Initialize lemmatizer
    try:
        nltk.download('wordnet', quiet=True)
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        lemmatizer = WordNetLemmatizer()
    except Exception as e:
        logger.warning(f"Could not initialize WordNetLemmatizer: {e}")
        lemmatizer = None
        
    # Try to load spaCy model, with fallback options
    try:
        nlp = spacy.load("en_core_web_lg")  # Larger model with word vectors
    except:
        try:
            nlp = spacy.load("en_core_web_md")  # Medium model
        except:
            try:
                nlp = spacy.load("en_core_web_sm")  # Small model as fallback
            except:
                logger.warning("Could not load any spaCy model. Using minimal blank model.")
                nlp = spacy.blank("en")

# Call initialization
initialize_nlp()

# Pydantic models for structured outputs
class ColumnMatch(BaseModel):
    name: str = Field(description="The matched target column name")
    score: float = Field(description="Confidence score for this match (0.0 to 1.0)")
    justification: str = Field(description="Detailed justification for why this is a good match")
    suggested_name: Optional[str] = Field(None, description="Suggested improved name if current match isn't ideal")
    suggested_name_reason: Optional[str] = Field(None, description="Reason for suggesting an improved name")

class MatchingResult(BaseModel):
    source_column: str = Field(description="The source column name being matched")
    matches: List[ColumnMatch] = Field(description="The top matches for this source column")
    reasoning: str = Field(description="Detailed reasoning process for all matches")

class MatchingStrategy(Enum):
    """Enumeration of available matching strategies"""
    SEMANTIC = "semantic"           # Vector embedding similarity
    LINGUISTIC = "linguistic"       # NLP-based comparison
    LEVENSHTEIN = "levenshtein"     # Edit distance
    JARO_WINKLER = "jaro_winkler"   # String similarity favoring shared prefixes
    TFIDF = "tfidf"                 # Term frequency-inverse document frequency
    FUZZY = "fuzzy"                 # Fuzzy string matching
    HYBRID = "hybrid"               # Combination of multiple strategies

class NLPHelper:
    """Helper class for NLP operations using spaCy and NLTK"""
    
    @staticmethod
    def tokenize_and_clean(text):
        """Tokenize, lemmatize and clean text"""
        if not text:
            return []
            
        doc = nlp(text.lower())
        # Remove stop words and punctuation, then lemmatize
        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
        return tokens
    
    @staticmethod
    def compute_similarity(text1, text2):
        """Compute semantic similarity between two texts using spaCy"""
        if not text1 or not text2:
            return 0.0
        
        doc1 = nlp(text1.lower())
        doc2 = nlp(text2.lower())
        
        # If the model has vectors, use them
        if hasattr(doc1, 'has_vector') and hasattr(doc2, 'has_vector') and doc1.has_vector and doc2.has_vector:
            return doc1.similarity(doc2)
        
        # Fallback to token overlap if vectors not available
        tokens1 = set(NLPHelper.tokenize_and_clean(text1))
        tokens2 = set(NLPHelper.tokenize_and_clean(text2))
        
        if not tokens1 or not tokens2:
            return 0.0
            
        # Jaccard similarity as fallback
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def find_synonyms(word):
        """Find synonyms for a word using WordNet"""
        synonyms = set()
        try:
            for syn in wordnet.synsets(word):
                for lemma in syn.lemmas():
                    synonyms.add(lemma.name().replace('_', ' '))
        except Exception as e:
            logger.warning(f"Error finding synonyms for {word}: {e}")
        return list(synonyms)
    
    @staticmethod
    def extract_column_features(column_name):
        """Extract features from a column name"""
        if not column_name:
            return {
                'original': '',
                'tokens': [],
                'lemmatized': []
            }
            
        # Handle camelCase, snake_case, etc.
        # First, split by underscore
        parts = column_name.split('_')
        
        # Then handle camelCase within each part
        all_parts = []
        for part in parts:
            # Add spaces before capital letters and split
            spaced = ''.join([' ' + c if c.isupper() else c for c in part]).strip()
            all_parts.extend(spaced.split())
        
        # Clean and normalize
        normalized_parts = [part.lower() for part in all_parts if part]
        
        # Get lemmatized form of each part
        lemmatized = []
        if lemmatizer:
            lemmatized = [lemmatizer.lemmatize(part) for part in normalized_parts]
        else:
            lemmatized = normalized_parts
        
        return {
            'original': column_name,
            'tokens': normalized_parts,
            'lemmatized': lemmatized
        }

class HybridMatcher:
    """Implements multiple matching strategies and ensembles them for better results"""
    
    def __init__(self, strategies=None, weights=None):
        """
        Initialize with selected strategies and their weights
        
        Args:
            strategies: List of MatchingStrategy enums to use
            weights: Dictionary mapping strategies to their weights
        """
        self.available_strategies = {
            MatchingStrategy.SEMANTIC: self.semantic_similarity,
            MatchingStrategy.LINGUISTIC: self.linguistic_similarity,
            MatchingStrategy.LEVENSHTEIN: self.levenshtein_similarity,
            MatchingStrategy.JARO_WINKLER: self.jaro_winkler_similarity,
            MatchingStrategy.TFIDF: self.tfidf_similarity,
            MatchingStrategy.FUZZY: self.fuzzy_similarity
        }
        
        # Default to using all strategies with equal weights if none specified
        if strategies is None:
            self.strategies = list(MatchingStrategy)
            # Remove HYBRID to avoid recursion
            if MatchingStrategy.HYBRID in self.strategies:
                self.strategies.remove(MatchingStrategy.HYBRID)
        else:
            self.strategies = strategies
            
        # Set default equal weights if none provided
        if weights is None:
            total_strategies = len(self.strategies)
            self.weights = {strategy: 1.0/total_strategies for strategy in self.strategies}
        else:
            self.weights = weights
            
        # Normalize weights to sum to 1.0
        weight_sum = sum(self.weights.values())
        if weight_sum != 1.0:
            self.weights = {k: v/weight_sum for k, v in self.weights.items()}
            
        # Initialize TF-IDF vectorizer
        self.tfidf = None
        self.tfidf_fit = False
        
        # List to store rules for rule-based matching
        self.rules = []
    
    def fit_tfidf(self, corpus):
        """Fit the TF-IDF vectorizer on a corpus of column names"""
        if not corpus:
            logger.warning("Empty corpus provided for TF-IDF fitting")
            return self
            
        self.tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))
        self.tfidf.fit(corpus)
        self.tfidf_fit = True
        return self
    
    def semantic_similarity(self, text1, text2, nlp_model=None):
        """Compute semantic similarity using word embeddings"""
        if not text1 or not text2:
            return 0.0
            
        if nlp_model is None:
            nlp_model = nlp  # Use global spaCy model
            
        # Check if model has word vectors
        if not hasattr(nlp_model, 'has_vector'):
            # Fallback to character n-gram comparison
            return self.tfidf_similarity(text1, text2)
            
        doc1 = nlp_model(text1.lower())
        doc2 = nlp_model(text2.lower())
        
        # If vectors are available, use them
        if hasattr(doc1, 'has_vector') and hasattr(doc2, 'has_vector') and doc1.has_vector and doc2.has_vector:
            return max(0.0, min(1.0, doc1.similarity(doc2)))
        
        # Fallback to token-level similarity
        tokens1 = [token for token in doc1 if not token.is_stop and not token.is_punct]
        tokens2 = [token for token in doc2 if not token.is_stop and not token.is_punct]
        
        if not tokens1 or not tokens2:
            return 0.0
            
        # Cross-compare tokens and average
        similarities = []
        for token1 in tokens1:
            for token2 in tokens2:
                if hasattr(token1, 'has_vector') and hasattr(token2, 'has_vector') and token1.has_vector and token2.has_vector:
                    similarities.append(token1.similarity(token2))
        
        if similarities:
            return max(0.0, min(1.0, sum(similarities) / len(similarities)))
        else:
            return 0.0
    
    def linguistic_similarity(self, text1, text2):
        """Compute linguistic similarity using NLP features"""
        return NLPHelper.compute_similarity(text1, text2)
    
    def levenshtein_similarity(self, text1, text2):
        """Compute similarity based on Levenshtein edit distance"""
        if not text1 or not text2:
            return 0.0
            
        # Convert Levenshtein distance to similarity score
        max_len = max(len(text1), len(text2))
        if max_len == 0:
            return 1.0
            
        distance = jellyfish.levenshtein_distance(text1.lower(), text2.lower())
        similarity = 1.0 - (distance / max_len)
        return max(0.0, similarity)
    
    def jaro_winkler_similarity(self, text1, text2):
        """Compute Jaro-Winkler similarity, which favors strings with common prefixes"""
        if not text1 or not text2:
            return 0.0
            
        return jellyfish.jaro_winkler_similarity(text1.lower(), text2.lower())
    
    def tfidf_similarity(self, text1, text2):
        """Compute similarity using TF-IDF vectors"""
        if not text1 or not text2:
            return 0.0
            
        if not self.tfidf_fit:
            # Create on-the-fly for this comparison only
            temp_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))
            matrix = temp_tfidf.fit_transform([text1.lower(), text2.lower()])
            return (matrix * matrix.T)[0, 1]
        else:
            # Use pre-fitted vectorizer
            vec1 = self.tfidf.transform([text1.lower()])
            vec2 = self.tfidf.transform([text2.lower()])
            return (vec1 * vec2.T)[0, 0]
    
    def fuzzy_similarity(self, text1, text2):
        """Compute fuzzy string similarity ratio"""
        if not text1 or not text2:
            return 0.0
            
        # Use token_set_ratio to handle word reordering and partial matches
        return fuzz.token_set_ratio(text1.lower(), text2.lower()) / 100.0
    
    def compute_similarity(self, text1, text2, metadata1=None, metadata2=None):
        """
        Compute similarity using ensemble of selected strategies
        
        Args:
            text1: First text to compare
            text2: Second text to compare
            metadata1: Optional metadata for first text
            metadata2: Optional metadata for second text
            
        Returns:
            Dictionary with overall score and individual strategy scores
        """
        results = {}
        
        # Compute similarity for each selected strategy
        for strategy in self.strategies:
            try:
                score = self.available_strategies[strategy](text1, text2)
                results[strategy.value] = score
            except Exception as e:
                logger.warning(f"Error computing {strategy.value} similarity: {e}")
                results[strategy.value] = 0.0
            
        # Compute weighted ensemble score
        weighted_score = 0.0
        for strategy in self.strategies:
            weighted_score += results[strategy.value] * self.weights[strategy]
            
        results['ensemble'] = weighted_score
        
        # Include individual strategy contributions to final score
        results['contributions'] = {
            strategy.value: results[strategy.value] * self.weights[strategy] 
            for strategy in self.strategies
        }
        
        # Add metadata about strategies and weights used
        results['strategies_used'] = [s.value for s in self.strategies]
        results['weights_used'] = {s.value: self.weights[s] for s in self.strategies}
        
        return results

class SemanticColumnMatcher:
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH, 
                 matching_strategies=None, strategy_weights=None, nltk_path=None):
        self.env = OSEnv(config_file, creds_file, cert_file)
        
        # Initialize custom NLTK path if provided
        if nltk_path:
            logger.info(f"Using provided NLTK path: {nltk_path}")
            # Update environment variables for NLTK
            if os.path.exists(nltk_path):
                os.environ['NLTK_DATA'] = nltk_path
                logger.info(f"Set NLTK_DATA environment variable to: {nltk_path}")
        
        # Initialize hybrid matcher with specified strategies or defaults
        if matching_strategies is None:
            matching_strategies = [
                MatchingStrategy.SEMANTIC,     # 0.4 weight by default
                MatchingStrategy.LINGUISTIC,   # 0.3 weight by default
                MatchingStrategy.FUZZY,        # 0.2 weight by default
                MatchingStrategy.JARO_WINKLER  # 0.1 weight by default
            ]
            
        # Default weights that favor semantic and linguistic approaches
        if strategy_weights is None:
            strategy_weights = {
                MatchingStrategy.SEMANTIC: 0.4,
                MatchingStrategy.LINGUISTIC: 0.3,
                MatchingStrategy.FUZZY: 0.2,
                MatchingStrategy.JARO_WINKLER: 0.1
            }
        
        self.hybrid_matcher = HybridMatcher(
            strategies=matching_strategies,
            weights=strategy_weights
        )
        
        # Setup other components
        self.setup_components()
        
    def setup_components(self):
        """Initialize all components needed for semantic matching"""
        # Set up Azure Chat LLM
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))
        api_version = self.env.get("API_VERSION", "2023-05-15")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        # Get token provider
        token_provider = get_bearer_token_provider(
            self._get_credential(),
            "https://cognitiveservices.azure.com/.default"
        )
        
        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token_provider=token_provider
        )
        
        # Set up Pydantic output parser
        self.parser = PydanticOutputParser(pydantic_object=MatchingResult)
        
        # Initialize job tracking for async processing
        self.jobs = {}
    
    def _get_credential(self):
        if str_to_bool(self.env.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.env.get("AZURE_TENANT_ID"), 
                client_id=self.env.get("AZURE_CLIENT_ID"), 
                client_secret=self.env.get("AZURE_CLIENT_SECRET")
            )
        
    def load_csv_data(self, source_csv_path: str, target_csv_path: str):
        """Load the two CSV files and prepare them for processing with automatic encoding detection"""
        try:
            # Detect encoding for source CSV
            source_encoding = self.detect_file_encoding(source_csv_path)
            logger.info(f"Detected encoding for source CSV: {source_encoding}")
            
            # Load source CSV (with id, appname, colname) using detected encoding
            self.source_df = pd.read_csv(source_csv_path, encoding=source_encoding)
            logger.info(f"Loaded source CSV with {len(self.source_df)} rows")
            
            # Detect encoding for target CSV
            target_encoding = self.detect_file_encoding(target_csv_path)
            logger.info(f"Detected encoding for target CSV: {target_encoding}")
            
            # Load target CSV (with name and definition) using detected encoding
            self.target_df = pd.read_csv(target_csv_path, encoding=target_encoding)
            logger.info(f"Loaded target CSV with {len(self.target_df)} rows")
            
            # Verify required columns exist
            if not all(col in self.source_df.columns for col in ['id', 'appname', 'colname']):
                raise ValueError("Source CSV missing required columns: id, appname, colname")
                
            if not all(col in self.target_df.columns for col in ['name', 'definition']):
                raise ValueError("Target CSV missing required columns: name, definition")
                
            # Clean data
            self.source_df['colname'] = self.source_df['colname'].str.strip()
            self.target_df['name'] = self.target_df['name'].str.strip()
            self.target_df['definition'] = self.target_df['definition'].fillna('').str.strip()
            
            # Create enriched target data for better matching
            self.target_df['text_for_embedding'] = self.target_df.apply(
                lambda row: f"Column name: {row['name']}. Definition: {row['definition']}", 
                axis=1
            )
            
            # Add NLP features
            logger.info("Generating NLP features for target columns...")
            self.target_df['nlp_features'] = self.target_df['name'].apply(NLPHelper.extract_column_features)
            
            return True
        except Exception as e:
            logger.error(f"Error loading CSV data: {e}")
            raise
            
    def detect_file_encoding(self, file_path: str):
        """Detect the encoding of a file using chardet with multiple fallbacks"""
        # Read a sample of the file to detect encoding
        with open(file_path, 'rb') as f:
            # Read up to 1MB for encoding detection
            raw_data = f.read(1024 * 1024)
        
        # Detect encoding using chardet
        result = chardet.detect(raw_data)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"chardet detected {encoding} with {confidence:.2f} confidence for {file_path}")
        
        # If detection failed or has low confidence, try common encodings
        if encoding is None or confidence < 0.7:
            fallback_encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'utf-16']
            
            for enc in fallback_encodings:
                try:
                    # Try reading a few lines with this encoding
                    with open(file_path, 'r', encoding=enc) as f:
                        f.readline()  # Just read the header
                        f.readline()  # And one data line
                    
                    logger.info(f"Fallback to {enc} encoding successful for {file_path}")
                    return enc
                except UnicodeDecodeError:
                    continue
            
            # If all fallbacks fail, use a replacement strategy with utf-8
            logger.warning(f"All encoding detection failed for {file_path}, using utf-8 with replacement")
            return 'utf-8'
        
        return encoding
    
    def setup_vector_store(self):
        """Set up ChromaDB vector store with target column data using LangChain's built-in Chroma integration"""
        try:
            # Initialize the embedding function
            embedding_func = AzureOpenAIEmbeddings(
                azure_deployment=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large"),
                openai_api_version=self.env.get("API_VERSION", "2023-05-15"),
                azure_endpoint=self.env.get("AZURE_ENDPOINT", ""),
                openai_api_type="azure",
                azure_ad_token_provider=get_bearer_token_provider(
                    self._get_credential(),
                    "https://cognitiveservices.azure.com/.default"
                )
            )
            
            # Create documents for each target column in LangChain's Document format
            documents = []
            
            for _, row in self.target_df.iterrows():
                doc_text = f"Column name: {row['name']}. Definition: {row['definition']}"
                doc_metadata = {
                    'name': row['name'],
                    'definition': row['definition']
                }
                
                # Create Document instance
                doc = Document(
                    page_content=doc_text,
                    metadata=doc_metadata
                )
                documents.append(doc)
            
            # Create in-memory Chroma collection with LangChain's wrapper
            chroma_client = chromadb.Client(settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            ))
            
            # Create collection name
            collection_name = f"column_matcher_{uuid.uuid4().hex[:8]}"
            
            # Create Chroma vector store with LangChain's integration
            vectorstore = LangChainChroma.from_documents(
                documents=documents,
                embedding=embedding_func,
                client=chroma_client,
                collection_name=collection_name
            )
            
            # Get the retriever from the vector store
            self.retriever = vectorstore.as_retriever(search_kwargs={"k": 8})
            
            # Create a method to get documents with scores
            def get_retrieved_docs_with_scores(query, k=8):
                # Use the built-in similarity_search_with_score method
                docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)
                # Transform the results to match our expected format
                return [(doc, score, {}) for doc, score in docs_with_scores]
            
            # Attach the method to the retriever object
            setattr(self.retriever, 'get_retrieved_docs_with_scores', get_retrieved_docs_with_scores)
            
            # Store ChromaDB collection and client for later use
            self.chroma_collection = vectorstore._collection
            self.chroma_client = chroma_client
            self.vectorstore = vectorstore
            
            logger.info(f"ChromaDB vector store created with {len(documents)} documents using LangChain integration")
            return True
        except Exception as e:
            logger.error(f"Error setting up vector store: {e}")
            raise
    
    def enhance_retrieval(self, source_column, source_app, k=8):
        """Enhance vector retrieval with hybrid matching strategies"""
        # Get initial candidates from ChromaDB retriever
        query = f"Column name: {source_column} from application: {source_app}"
        
        # Use the attached method to get documents with scores
        initial_docs_with_scores = self.retriever.get_retrieved_docs_with_scores(query, k*2)
        
        # If TF-IDF vectorizer not fit yet, fit on all column names
        if not self.hybrid_matcher.tfidf_fit and len(initial_docs_with_scores) > 0:
            all_column_names = [doc.metadata['name'] for doc, _, _ in initial_docs_with_scores]
            self.hybrid_matcher.fit_tfidf(all_column_names)
        
        # Calculate hybrid similarity scores for each candidate
        scores = []
        for doc, chroma_score, _ in initial_docs_with_scores:
            target_name = doc.metadata['name']
            definition = doc.metadata.get('definition', '')
            
            # Calculate hybrid similarity using multiple strategies
            hybrid_results = self.hybrid_matcher.compute_similarity(
                source_column, 
                target_name
            )
            
            # Extract ensemble score and blend with ChromaDB score
            combined_score = 0.7 * hybrid_results['ensemble'] + 0.3 * chroma_score
            
            # Also compute similarity between source column and definition
            # if definition is available
            if definition:
                definition_sim = self.hybrid_matcher.compute_similarity(
                    source_column, 
                    definition
                )
                
                # Blend name similarity with definition similarity
                final_score = 0.7 * combined_score + 0.3 * definition_sim['ensemble']
            else:
                final_score = combined_score
                
            # Store document, score, and detailed results
            scores.append((doc, final_score, hybrid_results))
        
        # Sort by combined score and take top k
        scores.sort(key=lambda x: x[1], reverse=True)
        top_docs = [item[0] for item in scores[:k]]
        
        # Return the documents with their scores
        return top_docs, scores[:k]
    
    def create_rag_chain(self):
        """Create the RAG prompt chain for semantic matching"""
        # Define the semantic matching prompt template with format instructions
        template = """You are a semantic column matching expert. You need to find the most semantically similar matches 
between a source column name and potential target columns.

SOURCE COLUMN: {source_column}
SOURCE APPLICATION: {source_app}

POTENTIAL MATCHES:
{retrieved_documents}

INSTRUCTIONS:
1. Carefully analyze the semantic meaning of the source column name.
2. Consider the application context: {source_app}
3. Evaluate each potential match based on:
   - Semantic similarity to the source column name
   - How well the definition aligns with the likely purpose of the source column
   - Common database naming conventions and abbreviations
4. Think step-by-step to determine each match's relevance.
5. For each match, provide:
   - A confidence score between 0.0 and 1.0
   - A detailed justification
   - If the match isn't ideal, suggest a better name and explain why
6. Rank the top 4 matches in order of confidence score.

{format_instructions}
"""
        
        # Format instructions for Pydantic output
        format_instructions = self.parser.get_format_instructions()
        
        # Format the retrieved documents for the prompt
        def format_docs(docs_with_scores):
            docs = [doc for doc, _, _ in docs_with_scores]
            result = []
            for i, (doc, combined_score, similarity_details) in enumerate(docs_with_scores):
                name = doc.metadata['name']
                definition = doc.metadata.get('definition', '')
                
                # Format similarity metrics
                sim_text = f"Similarity Score: {combined_score:.3f}"
                
                entry = f"MATCH {i+1}:\nName: {name}\nDefinition: {definition}\n{sim_text}\n"
                result.append(entry)
            
            return "\n".join(result)
        
        # Create the RAG chain
        self.rag_chain = (
            {
                "source_column": itemgetter("source_column"),
                "source_app": itemgetter("source_app"),
                "retrieved_documents": itemgetter("docs_with_scores") | format_docs,
                "format_instructions": lambda _: format_instructions
            }
            | ChatPromptTemplate.from_template(template)
            | self.llm
            | self.parser
        )
        
        logger.info("RAG chain created for semantic matching")
        return True

    def setup_agent_graph(self):
        """Create a LangGraph workflow for agent-based selection"""
        
        # Define state
        class AgentState(BaseModel):
            source_column: str
            source_app: str
            source_id: str
            retrieved_docs: List[Document] = []
            docs_with_scores: List[Tuple[Document, float, Dict]] = []
            matching_result: Optional[MatchingResult] = None
            
        # Define the retrieval node
        def retrieve_and_analyze(state):
            # Enhanced retrieval using ChromaDB and hybrid matching
            docs, docs_with_scores = self.enhance_retrieval(
                state.source_column, 
                state.source_app
            )
            
            return {
                "retrieved_docs": docs,
                "docs_with_scores": docs_with_scores
            }
        
        # Define the reasoning and selection node
        def reason_and_select(state):
            # Use the RAG chain to analyze and select matches
            matching_result = self.rag_chain.invoke({
                "source_column": state.source_column,
                "source_app": state.source_app,
                "docs_with_scores": state.docs_with_scores
            })
            
            return {"matching_result": matching_result}
        
        # Build the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("retrieve_and_analyze", retrieve_and_analyze)
        workflow.add_node("reason_and_select", reason_and_select)
        
        # Add edges
        workflow.add_edge("retrieve_and_analyze", "reason_and_select")
        workflow.add_edge("reason_and_select", END)
        
        # Set entry point
        workflow.set_entry_point("retrieve_and_analyze")
        
        # Compile the graph
        self.agent_graph = workflow.compile()
        
        logger.info("Agent graph created for column matching")
        return True
    
    def match_columns(self):
        """Process all source columns and find their semantic matches"""
        try:
            results = []
            
            for _, row in self.source_df.iterrows():
                source_id = row['id']
                source_app = row['appname']
                source_column = row['colname']
                
                logger.info(f"Processing column: {source_column} from app: {source_app}")
                
                # Execute the agent graph
                final_state = self.agent_graph.invoke({
                    "source_column": source_column,
                    "source_app": source_app,
                    "source_id": source_id
                })
                
                # Store results
                result = {
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "matching_result": final_state.matching_result
                }
                
                results.append(result)
                
            self.matching_results = results
            logger.info(f"Completed matching for {len(results)} source columns")
            return results
        except Exception as e:
            logger.error(f"Error during column matching: {e}")
            raise
    
    def format_results(self):
        """Format the matching results into a pandas DataFrame"""
        formatted_results = []
        
        for result in self.matching_results:
            source_id = result["source_id"]
            source_app = result["source_app"]
            source_column = result["source_column"]
            matching_result = result["matching_result"]
            
            for i, match in enumerate(matching_result.matches):
                formatted_results.append({
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "target_column": match.name,
                    "rank": i + 1,
                    "score": match.score,
                    "justification": match.justification,
                    "suggested_name": match.suggested_name if match.suggested_name else "",
                    "suggested_name_reason": match.suggested_name_reason if match.suggested_name_reason else ""
                })
        
        return pd.DataFrame(formatted_results)
    
    def save_results(self, output_path: str):
        """Save the matching results to a CSV file"""
        results_df = self.format_results()
        results_df.to_csv(output_path, index=False)
        logger.info(f"Results saved to {output_path}")
        return True
    
    def run_full_pipeline(self, source_csv_path: str, target_csv_path: str, output_path: str):
        """Run the complete semantic column matching pipeline"""
        try:
            logger.info("Starting semantic column matching pipeline")
            
            # Step 1: Load CSV data
            self.load_csv_data(source_csv_path, target_csv_path)
            
            # Step 2: Set up vector store
            self.setup_vector_store()
            
            # Step 3: Create RAG chain
            self.create_rag_chain()
            
            # Step 4: Set up agent graph
            self.setup_agent_graph()
            
            # Step 5: Run matching process
            self.match_columns()
            
            # Step 6: Save results
            self.save_results(output_path)
            
            logger.info("Pipeline completed successfully")
            return self.format_results()
        except Exception as e:
            logger.error(f"Error in pipeline: {e}")
            raise


# REST API Models and Implementation
from fastapi import FastAPI, File, UploadFile, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from starlette.responses import StreamingResponse
import uvicorn
import io
import tempfile
import shutil

class APIConfig(BaseModel):
    """API configuration settings"""
    api_title: str = "ChromaDB Column Matcher API"
    api_description: str = "API for matching column names semantically using ChromaDB and Azure OpenAI Embeddings"
    api_version: str = "1.0.0"
    allowed_origins: List[str] = ["*"]
    host: str = "0.0.0.0"
    port: int = 8000
    nltk_path: Optional[str] = None  # Custom path to NLTK data


class ColumnTestRequest(BaseModel):
    """Model for testing a column name"""
    column_name: str = Field(..., description="Column name to test")
    application_name: Optional[str] = Field(None, description="Source application name for context")
    top_k: Optional[int] = Field(4, description="Number of matches to return")


class ColumnMatchResponse(BaseModel):
    """Response model for column match"""
    target_column: str
    score: float
    justification: str
    suggested_name: Optional[str] = None
    suggested_name_reason: Optional[str] = None


class ColumnTestResponse(BaseModel):
    """Response model for column test results"""
    source_column: str
    matches: List[ColumnMatchResponse]


class RestAPI:
    """REST API implementation using FastAPI"""
    
    def __init__(self, config: APIConfig = None):
        """Initialize the REST API"""
        if config is None:
            config = APIConfig()
            
        self.config = config
        self.app = FastAPI(
            title=config.api_title,
            description=config.api_description,
            version=config.api_version
        )
        
        # Configure CORS
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=config.allowed_origins,
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # Set up routes
        self.setup_routes()
        
        # Initialize matcher with custom NLTK path if provided
        if config.nltk_path:
            self.matcher = SemanticColumnMatcher(
                nltk_path=config.nltk_path
            )
        else:
            self.matcher = None
            
        self.vectorstore = None
        self.initialized = False
    
    def initialize_matcher(self, source_csv_path: str, target_csv_path: str):
        """Initialize the matcher with data sources"""
        try:
            if self.matcher is None:
                self.matcher = SemanticColumnMatcher()
            
            # Load CSV data
            self.matcher.load_csv_data(source_csv_path, target_csv_path)
            
            # Set up vector store using LangChain's built-in integration
            self.matcher.setup_vector_store()
            
            # Store retriever for later use
            self.retriever = self.matcher.retriever
            
            # Create RAG chain
            self.matcher.create_rag_chain()
            
            # Set up agent graph
            self.matcher.setup_agent_graph()
            
            self.initialized = True
            logger.info("Matcher initialized successfully with provided data sources")
            
            return True
        except Exception as e:
            logger.error(f"Error initializing matcher: {e}")
            raise
    
    def setup_routes(self):
        """Set up API routes"""
        
        @self.app.get("/", tags=["Health"])
        async def root():
            """API health check endpoint"""
            status_msg = "initialized" if self.initialized else "not initialized"
            docs_count = 0
            if hasattr(self, 'matcher') and self.matcher and hasattr(self.matcher, 'chroma_collection'):
                try:
                    docs_count = self.matcher.chroma_collection.count()
                except:
                    pass
                
            return {
                "status": "healthy", 
                "version": self.config.api_version,
                "matcher_status": status_msg,
                "embeddings_provider": "Azure OpenAI",
                "vector_database": "ChromaDB",
                "documents_count": docs_count
            }
        
        @self.app.post("/initialize", tags=["Setup"])
        async def initialize_api(
            source_file: UploadFile = File(...),
            target_file: UploadFile = File(...)
        ):
            """
            Initialize the matcher with data sources using ChromaDB and Azure OpenAI embeddings
            
            - **source_file**: CSV file with columns id, appname, colname
            - **target_file**: CSV file with columns name, definition
            """
            try:
                # Save uploaded files to temp directory
                temp_dir = tempfile.mkdtemp()
                source_path = os.path.join(temp_dir, "source.csv")
                target_path = os.path.join(temp_dir, "target.csv")
                
                # Save source file
                with open(source_path, "wb") as buffer:
                    shutil.copyfileobj(source_file.file, buffer)
                
                # Save target file
                with open(target_path, "wb") as buffer:
                    shutil.copyfileobj(target_file.file, buffer)
                
                # Initialize matcher with encoding detection
                if self.matcher is None:
                    self.matcher = SemanticColumnMatcher(
                        nltk_path=self.config.nltk_path
                    )
                
                # Initialize matcher with files - encoding will be auto-detected
                self.initialize_matcher(source_path, target_path)
                
                # Clean up temp files
                shutil.rmtree(temp_dir)
                
                return {"status": "success", "message": "Matcher initialized successfully with auto-detected encodings"}
                
            except Exception as e:
                logger.error(f"Error initializing API: {e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Error initializing API: {str(e)}"
                )
        
        @self.app.post("/test-column", response_model=ColumnTestResponse, tags=["Matching"])
        async def test_column(
            request: ColumnTestRequest
        ):
            """
            Test a new column name against the existing embeddings in ChromaDB
            
            Matches the provided column name against the previously loaded target columns
            using Azure OpenAI embeddings and LangChain's Chroma integration.
            
            - **column_name**: The column name to test
            - **application_name**: Optional application context
            - **top_k**: Number of matches to return (default: 4)
            """
            if not self.initialized:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Matcher not initialized. Call /initialize endpoint first."
                )
                
            try:
                source_column = request.column_name
                source_app = request.application_name or "Unknown"
                top_k = request.top_k or 4
                
                # Use agent to process the column
                final_state = self.matcher.agent_graph.invoke({
                    "source_column": source_column,
                    "source_app": source_app,
                    "source_id": str(uuid.uuid4())  # Generate a dummy ID
                })
                
                # Extract matches from result
                matching_result = final_state.matching_result
                
                # Format response
                matches = []
                for match in matching_result.matches[:top_k]:
                    matches.append(ColumnMatchResponse(
                        target_column=match.name,
                        score=match.score,
                        justification=match.justification,
                        suggested_name=match.suggested_name,
                        suggested_name_reason=match.suggested_name_reason
                    ))
                
                return ColumnTestResponse(
                    source_column=source_column,
                    matches=matches
                )
                
            except Exception as e:
                logger.error(f"Error testing column: {e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Error testing column: {str(e)}"
                )
    
    def run(self):
        """Run the FastAPI server using uvicorn"""
        uvicorn.run(
            self.app,
            host=self.config.host,
            port=self.config.port
        )


# Command-line interface for running either as API or direct script
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Financial column name matching')
    parser.add_argument('--mode', type=str, choices=['api', 'cli'], default='cli',
                       help='Run as API server or CLI tool')
    
    # CLI mode arguments
    parser.add_argument('--source', type=str, help='Path to source CSV with id, appname, colname')
    parser.add_argument('--target', type=str, help='Path to target CSV with name, definition')
    parser.add_argument('--output', type=str, default='column_matches.csv', 
                       help='Path to save results')
    
    # API mode arguments
    parser.add_argument('--host', type=str, default='0.0.0.0', help='API server host')
    parser.add_argument('--port', type=int, default=8000, help='API server port')
    
    # Custom path for NLTK
    parser.add_argument('--nltk-path', type=str, default='library/nltk', 
                      help='Custom path to NLTK resources')
    
    args = parser.parse_args()
    
    # Update the paths with command-line arguments
    nltk_path = args.nltk_path
    
    if args.mode == 'api':
        # Run as API server
        config = APIConfig(
            host=args.host,
            port=args.port,
            nltk_path=nltk_path
        )
            
        api = RestAPI(config)
        
        # If source and target files are provided, initialize the matcher
        if args.source and args.target:
            try:
                api.matcher = SemanticColumnMatcher(
                    nltk_path=nltk_path
                )
                api.initialize_matcher(args.source, args.target)
                logger.info("API initialized with provided data sources")
            except Exception as e:
                logger.error(f"Failed to initialize matcher: {e}")
                logger.info("API will start, but matcher is not initialized")
        
        # Run the API server using uvicorn
        logger.info(f"Starting API server on {args.host}:{args.port}")
        logger.info(f"Using NLTK path: {nltk_path}")
        api.run()
        
    else:
        # Run as CLI tool
        if not args.source or not args.target:
            parser.error("CLI mode requires --source and --target arguments")
        
        logger.info(f"Using NLTK path: {nltk_path}")
        
        matcher = SemanticColumnMatcher(
            nltk_path=nltk_path
        )
        
        logger.info(f"Loading CSV files with automatic encoding detection...")
        results = matcher.run_full_pipeline(args.source, args.target, args.output)
        
        # Print summary of results
        print(f"\nMatching Results Summary:")
        print(f"========================")
        
        # Group by source column
        by_source = results.groupby(['source_column'])
        
        for source_col, group in by_source:
            # Get top match
            top_match = group.iloc[0]
            print(f"\nSource Column: {source_col}")
            print(f"  Top Match: {top_match['target_column']} (Score: {top_match['score']:.2f})")
            
            # Show suggested name if available
            if top_match['suggested_name']:
                print(f"  Suggested Better Name: {top_match['suggested_name']}")
                print(f"  Reason: {top_match['suggested_name_reason']}")
            
            print(f"  Justification: {top_match['justification']}")
