#!/usr/bin/env python
"""
Business Terms Import Script

This script imports business terms from a CSV file into the vector database.
Enhanced with better encoding detection, optimized batch processing,
and HNSW index configuration for ChromaDB.
"""

import os
import sys
import logging
import argparse
import tempfile
from typing import Optional, Tuple, List, Dict, Any
import time
import uuid
from pathlib import Path
import shutil

# Add parent directory to path to import app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.core.business_terms import BusinessTermManager
from app.core.embedding import EmbeddingClient
from app.config.environment import get_os_env
from app.config.settings import get_vector_store

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def install_chardet():
    """Install chardet if not already available."""
    try:
        import chardet
        return True
    except ImportError:
        logger.info("chardet library not installed. Installing now...")
        try:
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "chardet"])
            import chardet
            logger.info("chardet successfully installed")
            return True
        except Exception as e:
            logger.error(f"Failed to install chardet: {e}")
            logger.error("Please install it manually: pip install chardet")
            return False

def detect_encoding(file_path: str) -> Tuple[str, float]:
    """
    Detect the encoding of a file using chardet with appropriate sample size.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Tuple containing the detected encoding and confidence
    """
    try:
        # Ensure chardet is installed
        import chardet
    except ImportError:
        if not install_chardet():
            logger.warning("Unable to install chardet. Using default encoding 'latin1' which rarely fails.")
            return "latin1", 0.0
        import chardet
        
    try:
        with open(file_path, 'rb') as f:
            # Read an appropriate sample size
            file_size = os.path.getsize(file_path)
            
            # For small files (<1MB), read the whole file
            # For larger files, read up to 1MB from the beginning, middle, and end
            if file_size <= 1024 * 1024:  # 1MB
                sample = f.read()
            else:
                # Read beginning (384KB), middle (384KB), and end (384KB)
                beginning = f.read(384 * 1024)
                
                f.seek(file_size // 2 - 192 * 1024)
                middle = f.read(384 * 1024)
                
                f.seek(max(0, file_size - 384 * 1024))
                end = f.read(384 * 1024)
                
                # Combine samples
                sample = beginning + middle + end
        
        result = chardet.detect(sample)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"Detected encoding: {encoding} (confidence: {confidence:.2f})")
        return encoding, confidence
    except Exception as e:
        logger.error(f"Error detecting encoding: {e}")
        logger.warning("Using default encoding 'latin1' which rarely fails")
        return "latin1", 0.0

def preprocess_csv(file_path: str, output_path: Optional[str] = None, 
                   remove_null_bytes: bool = True, remove_control_chars: bool = False,
                   use_temp: bool = True) -> str:
    """
    Preprocess a CSV file to remove problematic characters.
    
    Args:
        file_path: Path to the CSV file
        output_path: Path to save the cleaned file (optional)
        remove_null_bytes: Whether to remove null bytes
        remove_control_chars: Whether to remove control characters
        use_temp: Whether to use a temporary file if output_path is not provided
        
    Returns:
        Path to the cleaned CSV file
    """
    logger.info(f"Preprocessing CSV file: {file_path}")
    
    if not os.path.exists(file_path):
        logger.error(f"File not found: {file_path}")
        return file_path
    
    # Determine output path
    if not output_path:
        if use_temp:
            # Use a temporary file
            fd, output_path = tempfile.mkstemp(suffix='.csv')
            os.close(fd)  # Close the file descriptor
        else:
            # Use the original filename with .cleaned extension
            file_name = os.path.basename(file_path)
            dir_name = os.path.dirname(file_path)
            base_name, ext = os.path.splitext(file_name)
            output_path = os.path.join(dir_name, f"{base_name}.cleaned{ext}")
    
    try:
        # Read file in binary mode
        with open(file_path, 'rb') as f:
            content = f.read()
        
        cleaned_content = content
        
        # Remove null bytes if requested
        if remove_null_bytes:
            original_size = len(cleaned_content)
            cleaned_content = cleaned_content.replace(b'\x00', b'')
            new_size = len(cleaned_content)
            null_bytes_removed = original_size - new_size
            if null_bytes_removed > 0:
                logger.info(f"Removed {null_bytes_removed} null bytes from the file")
        
        # Remove control characters if requested
        if remove_control_chars:
            original_size = len(cleaned_content)
            # Replace control characters (except normal whitespace: \t, \n, \r)
            replacements = []
            for i in range(32):
                if i not in (9, 10, 13):  # Tab, LF, CR
                    replacements.append((bytes([i]), b''))
            
            for old, new in replacements:
                cleaned_content = cleaned_content.replace(old, new)
            
            new_size = len(cleaned_content)
            control_chars_removed = original_size - new_size
            if control_chars_removed > 0:
                logger.info(f"Removed {control_chars_removed} control characters from the file")
        
        # Only write a new file if changes were made
        if cleaned_content != content:
            with open(output_path, 'wb') as f:
                f.write(cleaned_content)
            logger.info(f"Cleaned CSV file saved to {output_path}")
            return output_path
        else:
            logger.info("No problematic characters found in the file, using original")
            if output_path != file_path and os.path.exists(output_path):
                # Remove the empty output file if it was created
                os.remove(output_path)
            return file_path
            
    except Exception as e:
        logger.error(f"Error preprocessing CSV: {e}")
        if output_path != file_path and os.path.exists(output_path):
            # Remove the output file if it was created
            os.remove(output_path)
        return file_path

def validate_csv(file_path: str) -> Tuple[bool, Optional[str]]:
    """
    Validate the CSV file has the required columns and determine its encoding.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        Tuple containing (is_valid, detected_encoding)
    """
    if not os.path.exists(file_path):
        logger.error(f"File doesn't exist: {file_path}")
        return False, None
        
    # Detect file encoding
    encoding, confidence = detect_encoding(file_path)
    
    if confidence < 0.7:
        logger.warning(f"Low confidence in encoding detection ({confidence:.2f})")
        
    # Define a function to check columns with a specific encoding
    def check_columns_with_encoding(enc):
        try:
            import csv
            with open(file_path, 'r', encoding=enc, errors='replace') as csvfile:
                # Just read the header row
                reader = csv.DictReader(csvfile)
                headers = reader.fieldnames
                
                if not headers:
                    logger.warning(f"No headers found with encoding {enc}")
                    return False
                
                # Check for required columns using any of their possible names
                name_columns = ['PBT_NAME', 'NAME', 'TERM_NAME', 'BUSINESS_TERM_NAME', 'TERM']
                definition_columns = ['PBT_DEFINITION', 'DEFINITION', 'TERM_DEFINITION', 'DESCRIPTION']
                
                name_found = any(col in headers for col in name_columns)
                definition_found = any(col in headers for col in definition_columns)
                
                if not name_found:
                    logger.warning(f"Missing name column with encoding {enc}. Expected one of: {', '.join(name_columns)}")
                    return False
                
                if not definition_found:
                    logger.warning(f"Missing definition column with encoding {enc}. Expected one of: {', '.join(definition_columns)}")
                    return False
                
                logger.info(f"CSV headers with encoding {enc}: {', '.join(headers)}")
                return True
        except UnicodeDecodeError:
            logger.warning(f"Cannot decode file with encoding {enc}")
            return False
        except Exception as e:
            logger.warning(f"Error checking CSV with encoding {enc}: {e}")
            return False
    
    # Try with detected encoding first
    if check_columns_with_encoding(encoding):
        return True, encoding
    
    # Try with fallback encodings
    fallback_encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1', 'utf-16']
    for enc in fallback_encodings:
        if enc != encoding:  # Skip if already tried
            logger.info(f"Trying validation with fallback encoding: {enc}")
            if check_columns_with_encoding(enc):
                return True, enc
    
    logger.error("CSV validation failed with all attempted encodings")
    return False, None

def print_term_counts(term_manager):
    """Print the distribution of terms by CDM, domain, and other categories."""
    terms = term_manager.get_all_terms()
    if not terms:
        logger.info("No terms in the database")
        return
        
    # Count by CDM
    cdm_counts = {}
    domain_counts = {}
    total_terms = len(terms)
    
    for term in terms:
        metadata = term.metadata if hasattr(term, 'metadata') else {}
        
        # Count by CDM
        cdm = metadata.get('cdm', 'Unknown')
        cdm_counts[cdm] = cdm_counts.get(cdm, 0) + 1
        
        # Count by domain/category if available
        domain = metadata.get('category', metadata.get('domain', 'Unknown'))
        domain_counts[domain] = domain_counts.get(domain, 0) + 1
    
    # Print summary
    logger.info(f"Term Distribution Summary (Total: {total_terms})")
    
    if cdm_counts:
        logger.info("By CDM:")
        for cdm, count in sorted(cdm_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_terms) * 100
            logger.info(f"  {cdm}: {count} ({percentage:.1f}%)")
    
    if domain_counts:
        logger.info("By Domain/Category:")
        for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True)[:10]:  # Top 10
            percentage = (count / total_terms) * 100
            logger.info(f"  {domain}: {count} ({percentage:.1f}%)")
        
        # If there are more than 10 domains, show count of others
        if len(domain_counts) > 10:
            other_count = sum([count for domain, count in domain_counts.items() 
                               if domain not in [d for d, _ in sorted(domain_counts.items(), 
                                                                    key=lambda x: x[1], 
                                                                    reverse=True)[:10]]])
            percentage = (other_count / total_terms) * 100
            logger.info(f"  Others: {other_count} ({percentage:.1f}%)")

def main():
    # Ensure chardet is available
    install_chardet()
        
    parser = argparse.ArgumentParser(description="Import business terms from a CSV file")
    parser.add_argument("csv_file", help="Path to the CSV file containing business terms")
    parser.add_argument("--batch-size", type=int, default=100, help="Number of terms to process in each batch")
    parser.add_argument("--validate-only", action="store_true", help="Only validate the CSV file, don't import")
    parser.add_argument("--encoding", help="Force a specific encoding instead of auto-detection")
    parser.add_argument("--no-delete", action="store_true", help="Don't delete terms not found in the CSV")
    parser.add_argument("--chroma-dir", help="Override the ChromaDB directory")
    parser.add_argument("--chroma-collection", help="Override the ChromaDB collection name")
    parser.add_argument("--clean", action="store_true", help="Preprocess the CSV to remove problematic characters")
    parser.add_argument("--clean-output", help="Path to save the cleaned CSV file (only with --clean)")
    parser.add_argument("--remove-control-chars", action="store_true", help="Remove all control characters during preprocessing (only with --clean)")
    parser.add_argument("--embedding-model", default=os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small"),
                        help="Embedding model to use (default: text-embedding-3-small)")
    parser.add_argument("--summary", action="store_true", help="Print a summary of term distribution after import")
    
    args = parser.parse_args()
    
    # Set environment variables if overrides provided
    if args.chroma_dir:
        os.environ["CHROMA_PERSIST_DIR"] = args.chroma_dir
        logger.info(f"Using ChromaDB directory: {args.chroma_dir}")
    
    if args.chroma_collection:
        os.environ["CHROMA_COLLECTION"] = args.chroma_collection
        logger.info(f"Using ChromaDB collection: {args.chroma_collection}")
    
    if args.embedding_model:
        os.environ["EMBEDDING_MODEL"] = args.embedding_model
        logger.info(f"Using embedding model: {args.embedding_model}")
    
    # Process the file first if clean option is selected
    csv_path_to_use = args.csv_file
    if args.clean:
        logger.info("Preprocessing CSV file to remove problematic characters")
        csv_path_to_use = preprocess_csv(
            args.csv_file, 
            output_path=args.clean_output,
            remove_null_bytes=True,
            remove_control_chars=args.remove_control_chars,
            use_temp=args.clean_output is None
        )
    
    # Validate the CSV file and determine encoding
    is_valid, detected_encoding = validate_csv(csv_path_to_use)
    if not is_valid:
        logger.error("CSV file validation failed")
        return 1
    
    if args.validate_only:
        logger.info("CSV file validation successful")
        logger.info(f"Detected encoding: {detected_encoding}")
        return 0
    
    # Determine which encoding to use
    # Always prioritize explicit encoding if provided, otherwise use 'auto'
    encoding_to_use = args.encoding if args.encoding else "auto"
    logger.info(f"Using encoding: {encoding_to_use}")
    
    # Initialize business term manager and perform import
    try:
        business_term_manager = BusinessTermManager()
        
        # Get initial term count for comparison
        initial_term_count = business_term_manager.get_term_count()
        logger.info(f"Initial term count: {initial_term_count}")
        
        # Import the terms
        start_time = time.time()
        added_count = business_term_manager.import_terms_from_csv(
            csv_path=csv_path_to_use,
            encoding=encoding_to_use,
            batch_size=args.batch_size
        )
        
        total_time = time.time() - start_time
        term_count = business_term_manager.get_term_count()
        
        logger.info(f"Import completed successfully in {total_time:.2f} seconds")
        logger.info(f"Added {added_count} terms")
        logger.info(f"Total terms in database: {term_count}")
        
        # Print term distribution summary if requested
        if args.summary:
            print_term_counts(business_term_manager)
        
        # Clean up the temporary file if we created one
        if args.clean and args.clean_output is None and csv_path_to_use != args.csv_file:
            try:
                os.remove(csv_path_to_use)
                logger.info(f"Removed temporary file: {csv_path_to_use}")
            except Exception as e:
                logger.warning(f"Could not remove temporary file {csv_path_to_use}: {e}")
        
        return 0
    except Exception as e:
        logger.error(f"Import failed: {e}")
        
        # Clean up temporary file if there was an error
        if args.clean and args.clean_output is None and csv_path_to_use != args.csv_file:
            try:
                os.remove(csv_path_to_use)
                logger.info(f"Removed temporary file: {csv_path_to_use}")
            except Exception as remove_error:
                logger.warning(f"Could not remove temporary file {csv_path_to_use}: {remove_error}")
        
        return 1

if __name__ == "__main__":
    sys.exit(main())
