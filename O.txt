#!/usr/bin/env python
"""
Business Terms Import Script

This script imports business terms from a CSV file into the vector database.
Enhanced with better encoding detection, optimized batch processing,
and HNSW index configuration for ChromaDB.
"""

import os
import sys
import logging
import argparse
from typing import Optional, Tuple, List, Dict, Any
import pandas as pd
import uuid
import csv
import time
import json
import numpy as np
from pathlib import Path
import chromadb
from chromadb.config import Settings

# Add parent directory to path to import app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.core.business_terms import BusinessTermManager, ConceptRelationship
from app.core.embedding import MyDocument, EmbeddingClient
from app.config.environment import get_os_env
from app.config.settings import get_vector_store
from app.core.vector_store_chroma import ChromaDBVectorStore

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def check_dependencies():
    """Check if required dependencies are installed."""
    try:
        import chardet
        return True
    except ImportError:
        logger.error("chardet library not installed. Installing now...")
        try:
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "chardet"])
            import chardet
            logger.info("chardet successfully installed")
            return True
        except Exception as e:
            logger.error(f"Failed to install chardet: {e}")
            logger.error("Please install it manually: pip install chardet")
            return False

def detect_encoding(file_path: str) -> Tuple[str, float]:
    """
    Detect the encoding of a file using chardet.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Tuple containing the detected encoding and confidence
    """
    try:
        import chardet
    except ImportError:
        logger.warning("chardet not available. Defaulting to UTF-8.")
        return "utf-8", 0.0
        
    try:
        with open(file_path, 'rb') as f:
            # Read a sample of the file for detection
            # For large files, we don't need to read the whole file
            file_size = os.path.getsize(file_path)
            # Read up to 1MB or the entire file for smaller files
            sample = f.read(min(1024 * 1024, file_size))
        
        result = chardet.detect(sample)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"Detected encoding: {encoding} (confidence: {confidence:.2f})")
        return encoding, confidence
    except Exception as e:
        logger.error(f"Error detecting encoding: {e}")
        return "utf-8", 0.0

def validate_csv(file_path: str) -> bool:
    """
    Validate the CSV file has the required columns.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        bool: True if valid, False otherwise
    """
    try:
        # Detect file encoding
        encoding, confidence = detect_encoding(file_path)
        
        if confidence < 0.7:
            logger.warning(f"Low confidence in encoding detection ({confidence:.2f})")
        
        # Try with detected encoding first
        try:
            df = pd.read_csv(file_path, nrows=1, encoding=encoding)
        except Exception as e:
            logger.warning(f"Failed to read with detected encoding {encoding}: {e}")
            
            # Fallback encodings to try
            fallback_encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
            for enc in fallback_encodings:
                if enc != encoding:  # Skip if already tried
                    try:
                        logger.info(f"Trying with encoding: {enc}")
                        df = pd.read_csv(file_path, nrows=1, encoding=enc)
                        logger.info(f"Successfully read with encoding: {enc}")
                        encoding = enc  # Update to the working encoding
                        break
                    except Exception:
                        continue
            else:
                logger.error("Failed to read CSV with all attempted encodings")
                return False
        
        # Check for required columns - using PBT_NAME, PBT_DEFINITION, CDM format
        # Define all possible column name variations we support
        name_columns = ['PBT_NAME', 'NAME', 'TERM_NAME', 'BUSINESS_TERM_NAME', 'TERM']
        definition_columns = ['PBT_DEFINITION', 'DEFINITION', 'TERM_DEFINITION', 'DESCRIPTION']
        
        # Check if any of the name column variants exist
        name_col_found = any(col in df.columns for col in name_columns)
        definition_col_found = any(col in df.columns for col in definition_columns)
        
        if not name_col_found:
            logger.error(f"Missing name column. Expected one of: {', '.join(name_columns)}")
            return False
            
        if not definition_col_found:
            logger.error(f"Missing definition column. Expected one of: {', '.join(definition_columns)}")
            return False
        
        # Store the detected encoding in the environment for later use
        os.environ["CSV_DETECTED_ENCODING"] = encoding
        
        # Print column summary
        logger.info(f"CSV columns found: {', '.join(df.columns)}")
        return True
    except Exception as e:
        logger.error(f"Error validating CSV file: {e}")
        return False

def clean_csv(input_file: str, output_file: Optional[str] = None) -> Optional[str]:
    """
    Clean and prepare CSV file for import.
    
    Args:
        input_file: Path to the input CSV file
        output_file: Path to save the cleaned CSV file, if not provided, a temp file is created
        
    Returns:
        str: Path to the cleaned CSV file, or None if cleaning failed
    """
    try:
        if not os.path.exists(input_file):
            logger.error(f"Input file doesn't exist: {input_file}")
            return None
            
        # Create output file path if not provided
        if not output_file:
            basename = os.path.basename(input_file)
            dirname = os.path.dirname(input_file)
            filename, ext = os.path.splitext(basename)
            output_file = os.path.join(dirname, f"{filename}_cleaned{ext}")
        
        # Get the detected encoding from environment or detect it
        encoding = os.environ.get("CSV_DETECTED_ENCODING")
        if not encoding:
            encoding, _ = detect_encoding(input_file)
        
        logger.info(f"Reading CSV with encoding: {encoding}")
        
        # Read the CSV file with detected encoding
        try:
            df = pd.read_csv(input_file, encoding=encoding)
        except Exception as e:
            logger.warning(f"Error reading with encoding {encoding}: {e}")
            # Fallback to latin1 which rarely fails (but might not interpret characters correctly)
            logger.info("Falling back to latin1 encoding")
            df = pd.read_csv(input_file, encoding='latin1')
        
        # Basic cleaning: remove extra whitespace, drop completely empty rows
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].str.strip()
        
        df = df.dropna(how='all')
        
        # Map standard column names
        name_columns = ['PBT_NAME', 'NAME', 'TERM_NAME', 'BUSINESS_TERM_NAME', 'TERM']
        definition_columns = ['PBT_DEFINITION', 'DEFINITION', 'TERM_DEFINITION', 'DESCRIPTION']
        cdm_columns = ['CDM', 'DOMAIN', 'SUBJECT_AREA']
        category_columns = ['CATEGORY', 'TERM_CATEGORY', 'TYPE']
        
        # Rename columns to match our expected format
        column_mapping = {}
        headers = df.columns.tolist()
        
        # Find and map name column
        for col in name_columns:
            if col in headers:
                column_mapping[col] = 'name'
                break
        
        # Find and map definition column
        for col in definition_columns:
            if col in headers:
                column_mapping[col] = 'description'
                break
        
        # Find and map CDM column if it exists
        for col in cdm_columns:
            if col in headers:
                column_mapping[col] = 'cdm'
                break
        
        # Find and map category column if it exists
        for col in category_columns:
            if col in headers:
                column_mapping[col] = 'category'
                break
        
        # If columns need to be renamed, do it
        if column_mapping:
            df = df.rename(columns=column_mapping)
        
        # Ensure ID column exists
        if 'id' not in df.columns:
            logger.info("Adding 'id' column with generated UUIDs")
            df['id'] = [f"term-{uuid.uuid4()}" for _ in range(len(df))]
        
        # Ensure required columns exist
        for col in ['name', 'description']:
            if col not in df.columns:
                logger.error(f"Required column '{col}' missing from CSV after mapping")
                return None
        
        # Handle missing values 
        df['name'] = df['name'].fillna('')
        df['description'] = df['description'].fillna('')
        
        # Filter rows with empty name or description
        valid_rows = (df['name'] != '') & (df['description'] != '')
        invalid_count = (~valid_rows).sum()
        
        if invalid_count > 0:
            logger.warning(f"Removing {invalid_count} rows with empty name or description")
            df = df[valid_rows]
        
        # Preprocessing for better import
        # Convert all metadata columns to strings
        metadata_cols = [col for col in df.columns if col not in ['id', 'name', 'description']]
        for col in metadata_cols:
            df[col] = df[col].astype(str)
        
        # Save cleaned CSV with UTF-8 encoding for consistency
        df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"Cleaned CSV saved to {output_file}")
        
        return output_file
    except Exception as e:
        logger.error(f"Error cleaning CSV: {e}")
        return None

def setup_chroma_with_hnsw(persist_dir: str, collection_name: str) -> Optional[ChromaDBVectorStore]:
    """
    Set up ChromaDB with optimized HNSW configuration.
    
    Args:
        persist_dir: Directory for persistent storage
        collection_name: Name of the ChromaDB collection
        
    Returns:
        ChromaDBVectorStore instance or None if setup fails
    """
    try:
        # Ensure the persist directory exists
        os.makedirs(persist_dir, exist_ok=True)
        
        # Use our enhanced ChromaDBVectorStore that configures HNSW properly
        vector_store = ChromaDBVectorStore(
            collection_name=collection_name,
            persist_dir=persist_dir
        )
        
        # Test if the vector store is working properly
        health = vector_store.health_check()
        if health["status"] != "healthy":
            logger.error(f"Vector store health check failed: {health.get('error', 'Unknown error')}")
            return None
            
        logger.info(f"ChromaDB set up successfully with collection: {collection_name}")
        logger.info(f"Current term count: {health.get('term_count', 0)}")
        
        return vector_store
    except Exception as e:
        logger.error(f"Error setting up ChromaDB: {e}")
        return None

def import_terms_in_batches(
    file_path: str, 
    embedding_client: EmbeddingClient, 
    vector_store: ChromaDBVectorStore,
    batch_size: int = 100, 
    delete_missing: bool = True
) -> Tuple[int, int, int]:
    """
    Import terms from CSV in optimized batches, with progress tracking.
    
    Args:
        file_path: Path to the CSV file
        embedding_client: EmbeddingClient instance
        vector_store: ChromaDBVectorStore instance
        batch_size: Number of terms per batch
        delete_missing: Whether to delete terms not in CSV
        
    Returns:
        Tuple of (added_count, updated_count, deleted_count)
    """
    try:
        # Read the CSV file
        df = pd.read_csv(file_path, encoding='utf-8')
        
        # Get existing terms for tracking what needs to be deleted later
        existing_terms = {}
        for term in vector_store.get_all_terms():
            term_key = f"{term['name']}::{term['description']}"
            existing_terms[term_key] = term['id']
        
        # Track terms found in CSV for deletion tracking
        csv_term_keys = set()
        
        # Progress tracking
        total_rows = len(df)
        added_count = 0
        updated_count = 0
        skipped_count = 0
        error_count = 0
        start_time = time.time()
        logger.info(f"Starting import of {total_rows} terms in batches of {batch_size}")
        
        # Process in batches
        for batch_start in range(0, total_rows, batch_size):
            batch_end = min(batch_start + batch_size, total_rows)
            batch_df = df.iloc[batch_start:batch_end]
            
            # Track time for ETA calculation
            batch_start_time = time.time()
            
            # Prepare batch data
            docs_to_embed = []
            batch_terms = []
            
            for _, row in batch_df.iterrows():
                # Get required fields
                term_id = str(row.get('id', '')).strip()
                name = str(row['name']).strip()
                description = str(row['description']).strip()
                
                # Skip rows with empty name or description
                if not name or not description:
                    skipped_count += 1
                    continue
                
                # Generate a unique ID if not provided
                if not term_id:
                    term_id = f"term-{uuid.uuid4()}"
                
                # Create term key for tracking
                term_key = f"{name}::{description}"
                csv_term_keys.add(term_key)
                
                # Extract metadata from other columns
                metadata = {}
                for col in row.index:
                    if col not in ['id', 'name', 'description'] and not pd.isna(row[col]):
                        metadata[col] = str(row[col])
                
                # Store term info for later
                batch_terms.append({
                    "id": term_id,
                    "name": name,
                    "description": description,
                    "term_key": term_key,
                    "metadata": metadata
                })
                
                # Create document for embedding
                doc = MyDocument(
                    id=term_id,
                    text=f"{name}. {description}"
                )
                docs_to_embed.append(doc)
            
            # Generate embeddings in batch for better efficiency
            docs_with_embeddings = embedding_client.batch_generate_embeddings(docs_to_embed)
            
            # Create vectors batch for ChromaDB
            vectors_batch = []
            for i, doc_with_embedding in enumerate(docs_with_embeddings):
                term = batch_terms[i]
                
                if not doc_with_embedding.embedding:
                    logger.warning(f"Skipping term without embedding: {term['name']}")
                    error_count += 1
                    continue
                
                vectors_batch.append({
                    "id": term["id"],
                    "name": term["name"],
                    "description": term["description"],
                    "embedding": doc_with_embedding.embedding,
                    "metadata": term.get("metadata", {})
                })
            
            # Store vectors in ChromaDB
            if vectors_batch:
                for term in vectors_batch:
                    term_key = f"{term['name']}::{term['description']}"
                    # Check if term exists and needs update or is new
                    if term_key in existing_terms:
                        if existing_terms[term_key] == term['id']:
                            # Same ID, this is an update
                            updated_count += 1
                        else:
                            # Different ID, treat as a new term
                            added_count += 1
                    else:
                        # New term
                        added_count += 1
                
                # Batch store the vectors
                inserted = vector_store.batch_store_vectors(vectors_batch)
                
                if inserted != len(vectors_batch):
                    logger.warning(f"Expected to insert {len(vectors_batch)} terms, but inserted {inserted}")
            
            # Calculate and log progress
            batch_duration = time.time() - batch_start_time
            elapsed_total = time.time() - start_time
            progress = batch_end / total_rows * 100
            
            # Calculate ETA
            if batch_end > batch_start:
                items_per_second = (batch_end - batch_start) / batch_duration
                remaining_items = total_rows - batch_end
                eta_seconds = remaining_items / items_per_second if items_per_second > 0 else 0
                eta_str = time.strftime("%H:%M:%S", time.gmtime(eta_seconds))
            else:
                eta_str = "Unknown"
                items_per_second = 0
            
            logger.info(
                f"Progress: {progress:.1f}% - Batch {batch_start//batch_size + 1}/{(total_rows + batch_size - 1)//batch_size} - "
                f"Added: {added_count}, Updated: {updated_count}, Skipped: {skipped_count}, "
                f"Errors: {error_count} - Rate: {items_per_second:.1f} terms/sec - ETA: {eta_str}"
            )
        
        # Handle deletion of terms not in the CSV
        deleted_count = 0
        if delete_missing:
            terms_to_delete = []
            for term_key, term_id in existing_terms.items():
                if term_key not in csv_term_keys:
                    terms_to_delete.append(term_id)
            
            if terms_to_delete:
                logger.info(f"Deleting {len(terms_to_delete)} terms not found in CSV")
                
                # Delete in batches of 20 to avoid overwhelming the system
                for i in range(0, len(terms_to_delete), 20):
                    batch = terms_to_delete[i:i+20]
                    for term_id in batch:
                        if vector_store.delete_term(term_id):
                            deleted_count += 1
                    
                    if i % 100 == 0 and i > 0:
                        logger.info(f"Deleted {deleted_count}/{len(terms_to_delete)} terms")
        
        # Log final summary
        total_time = time.time() - start_time
        logger.info(
            f"Import complete: Added {added_count}, Updated {updated_count}, "
            f"Skipped {skipped_count}, Deleted {deleted_count}, "
            f"Errors {error_count} in {total_time:.1f} seconds "
            f"({total_rows/total_time:.1f} terms/sec)"
        )
        
        return added_count, updated_count, deleted_count
    
    except Exception as e:
        logger.error(f"Error importing terms: {e}")
        raise

def initialize_concept_relationships(vector_store: ChromaDBVectorStore):
    """
    Initialize concept relationships for better term matching.
    
    Args:
        vector_store: ChromaDBVectorStore instance
    """
    try:
        # Get all terms
        all_terms = vector_store.get_all_terms()
        
        # Initialize concept relationships
        concept_relationships = ConceptRelationship()
        concept_relationships.initialize_from_terms(all_terms)
        
        logger.info(
            f"Initialized concept relationships with "
            f"{len(concept_relationships.concept_map)} concepts and "
            f"{sum(len(v) for v in concept_relationships.related_terms.values())} relationships"
        )
        
        # Log category and CDM information
        logger.info(f"Found {len(concept_relationships.category_concepts)} categories")
        logger.info(f"Found {len(concept_relationships.cdm_concepts)} CDMs")
        
        # Log statistics for important categories and CDMs
        category_stats = sorted(
            [(cat, len(terms)) for cat, terms in concept_relationships.category_concepts.items()],
            key=lambda x: x[1],
            reverse=True
        )
        
        cdm_stats = sorted(
            [(cdm, len(terms)) for cdm, terms in concept_relationships.cdm_concepts.items()],
            key=lambda x: x[1],
            reverse=True
        )
        
        # Log top categories
        if category_stats:
            logger.info("Top categories:")
            for cat, count in category_stats[:5]:
                logger.info(f"  - {cat}: {count} terms")
        
        # Log top CDMs
        if cdm_stats:
            logger.info("Top CDMs:")
            for cdm, count in cdm_stats[:5]:
                logger.info(f"  - {cdm}: {count} terms")
        
        return concept_relationships
    except Exception as e:
        logger.error(f"Error initializing concept relationships: {e}")
        return None

def main():
    # Ensure all dependencies are installed
    if not check_dependencies():
        return 1
        
    parser = argparse.ArgumentParser(description="Import business terms from a CSV file")
    parser.add_argument("csv_file", help="Path to the CSV file containing business terms")
    parser.add_argument("--batch-size", type=int, default=100, help="Number of terms to process in each batch")
    parser.add_argument("--clean", action="store_true", help="Clean and prepare the CSV file before import")
    parser.add_argument("--validate-only", action="store_true", help="Only validate the CSV file, don't import")
    parser.add_argument("--output", help="Path to save the cleaned CSV file")
    parser.add_argument("--chroma-dir", help="Override the ChromaDB directory")
    parser.add_argument("--chroma-collection", help="Override the ChromaDB collection name")
    parser.add_argument("--encoding", help="Force a specific encoding instead of auto-detection")
    parser.add_argument("--no-delete", action="store_true", help="Don't delete terms not found in the CSV")
    parser.add_argument("--hnsw-params", type=str, help="JSON string with HNSW parameters for ChromaDB")
    
    args = parser.parse_args()
    
    # Set environment variables if overrides provided
    chroma_dir = args.chroma_dir or os.environ.get("CHROMA_PERSIST_DIR", "./data/chroma_db")
    if args.chroma_dir:
        os.environ["CHROMA_PERSIST_DIR"] = args.chroma_dir
        logger.info(f"Using ChromaDB directory: {args.chroma_dir}")
    
    chroma_collection = args.chroma_collection or os.environ.get("CHROMA_COLLECTION", "business_terms")
    if args.chroma_collection:
        os.environ["CHROMA_COLLECTION"] = args.chroma_collection
        logger.info(f"Using ChromaDB collection: {args.chroma_collection}")
        
    # Set encoding if provided
    if args.encoding:
        os.environ["CSV_DETECTED_ENCODING"] = args.encoding
        logger.info(f"Using forced encoding: {args.encoding}")
    
    # Validate the CSV file
    if not validate_csv(args.csv_file):
        logger.error("CSV file validation failed")
        return 1
    
    if args.validate_only:
        logger.info("CSV file validation successful")
        return 0
    
    # Clean the CSV file if requested
    csv_file_to_import = args.csv_file
    if args.clean:
        cleaned_file = clean_csv(args.csv_file, args.output)
        if not cleaned_file:
            logger.error("CSV cleaning failed")
            return 1
        csv_file_to_import = cleaned_file
    
    # Set up ChromaDB with optimized HNSW configuration
    vector_store = setup_chroma_with_hnsw(chroma_dir, chroma_collection)
    if not vector_store:
        logger.error("Failed to set up ChromaDB")
        return 1
    
    # Create embedding client
    embedding_client = EmbeddingClient()
    
    # Import the terms
    try:
        added, updated, deleted = import_terms_in_batches(
            file_path=csv_file_to_import,
            embedding_client=embedding_client,
            vector_store=vector_store,
            batch_size=args.batch_size,
            delete_missing=not args.no_delete
        )
        
        # Initialize concept relationships for better term matching
        initialize_concept_relationships(vector_store)
        
        logger.info(f"Import completed successfully: {added} added, {updated} updated, {deleted} deleted")
        return 0
    except Exception as e:
        logger.error(f"Import failed: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
