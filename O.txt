import logging
from typing import Optional
from langchain_openai import AzureChatOpenAI
from azure.identity import get_bearer_token_provider
from app.config.environment import get_os_env

logger = logging.getLogger(__name__)

def get_llm(proxy_enabled: Optional[bool] = None) -> AzureChatOpenAI:
    """
    Get the language model for the application.
    
    Args:
        proxy_enabled: Override the PROXY_ENABLED setting in the config file
    
    Returns:
        AzureChatOpenAI: The language model
    """
    # Get environment with proxy setting override if provided
    env = get_os_env(proxy_enabled=proxy_enabled)
    
    logger.info(f"Setting up Azure OpenAI client with proxy enabled: {env.get('PROXY_ENABLED', 'False')}")
    
    # Get token provider for Azure AD auth
    token_provider = get_bearer_token_provider(
        env.credential,
        "https://cognitiveservices.azure.com/.default"
    )
    
    # Get model configuration
    model_name = env.get("MODEL_NAME", "gpt-4o")
    temperature = float(env.get("TEMPERATURE", "0.3"))  # Lower temperature for more deterministic outputs
    max_tokens = int(env.get("MAX_TOKENS", "2000"))
    api_version = env.get("API_VERSION", "2023-05-15")
    azure_endpoint = env.get("AZURE_ENDPOINT", "")
    
    logger.info(f"Using model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")
    
    # Create and return the LLM
    return AzureChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        azure_ad_token_provider=token_provider
    )
