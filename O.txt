"""
LLM Context Tagging Agent - Loads all business terms into LLM context for comprehensive matching.

This agent provides the full business term repository to the LLM, enabling it to
make semantically rich matching decisions with complete knowledge of available terms.
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
import json
import re

from app.core.models import TaggingResult
from app.core.business_terms import BusinessTermManager
from app.core.embedding import EmbeddingClient, MyDocument
from app.config.settings import get_llm

logger = logging.getLogger(__name__)

class LLMContextTaggingAgent:
    """
    Tagging agent that loads all business terms into the LLM's context window,
    allowing for comprehensive matching with complete knowledge of the term repository.
    """
    
    def __init__(self):
        """Initialize the tagging agent."""
        self.business_term_manager = BusinessTermManager()
        self.embedding_client = EmbeddingClient()
        self.llm = get_llm()  # Get GPT-4o-mini model
    
    async def tag_element(self, element_id: str, name: str, description: str, top_k: int = 3) -> TaggingResult:
        """
        Tag a data element with the most similar business terms by loading all terms
        into the LLM's context window for comprehensive matching.
        
        Args:
            element_id: Unique identifier for the element
            name: Name of the data element
            description: Description of the data element
            top_k: Number of top matching terms to return
            
        Returns:
            TaggingResult containing matching terms and confidence scores
        """
        try:
            # Validate inputs
            if not name or not description:
                logger.warning(f"Empty name or description for element: {element_id}")
                return TaggingResult(
                    element_id=element_id,
                    element_name=name or "",
                    element_description=description or "",
                    matching_terms=[],
                    confidence_scores=[],
                    modeling_required=True,
                    message="Name or description is empty. Modeling should be performed."
                )
            
            # Get all business terms from the repository
            all_terms = self.business_term_manager.get_all_terms()
            
            if not all_terms:
                logger.warning("No business terms found in the repository")
                return TaggingResult(
                    element_id=element_id,
                    element_name=name,
                    element_description=description,
                    matching_terms=[],
                    confidence_scores=[],
                    modeling_required=True,
                    message="No business terms found in the repository. Modeling should be performed."
                )
            
            # Step 1: First use vector search to get candidate terms to narrow down the search
            # This helps to reduce the amount of data we send to the LLM while still
            # providing a diverse set of potentially relevant terms
            vector_candidates = await self._get_vector_candidates(
                element_id=element_id,
                name=name,
                description=description,
                top_k=min(50, len(all_terms))  # Get more candidates for LLM to evaluate
            )
            
            # Step 2: Prepare full context for LLM
            context_terms = self._prepare_llm_context(
                all_terms=all_terms,
                vector_candidates=vector_candidates
            )
            
            # Step 3: Use LLM to find and rank matches
            matches = await self._match_with_llm(
                element_name=name,
                element_description=description,
                context_terms=context_terms,
                top_k=top_k
            )
            
            # Format the results
            matching_terms = []
            confidence_scores = []
            
            for term in matches[:top_k]:
                matching_terms.append({
                    "id": term["id"],
                    "name": term["name"],
                    "description": term["description"],
                    "similarity": term["score"]
                })
                confidence_scores.append(term["score"])
            
            # Determine if modeling is required
            modeling_required = False
            message = ""
            
            if not matching_terms:
                modeling_required = True
                message = "No matching terms found. Modeling should be performed."
            elif max(confidence_scores) < 0.5:
                modeling_required = True
                message = f"Low confidence matches (max: {max(confidence_scores):.2f}). Consider modeling a new term."
            else:
                message = "Found matches using comprehensive LLM context matching."
                if matches and "reasoning" in matches[0]:
                    message = matches[0]["reasoning"]
            
            return TaggingResult(
                element_id=element_id,
                element_name=name,
                element_description=description,
                matching_terms=matching_terms,
                confidence_scores=confidence_scores,
                modeling_required=modeling_required,
                message=message
            )
                
        except Exception as e:
            logger.error(f"Error tagging element: {e}")
            return TaggingResult(
                element_id=element_id,
                element_name=name,
                element_description=description,
                matching_terms=[],
                confidence_scores=[],
                modeling_required=True,
                message=f"Error during tagging: {str(e)}. Modeling should be performed."
            )
    
    async def _get_vector_candidates(self, element_id: str, name: str, description: str, top_k: int = 50) -> List[Dict[str, Any]]:
        """
        Get candidate terms using vector similarity search to help narrow down the context.
        
        Args:
            element_id: Element ID
            name: Element name
            description: Element description
            top_k: Number of candidates to return
            
        Returns:
            List of candidate terms with vector similarity scores
        """
        try:
            # Create embedding for the data element
            doc = MyDocument(
                id=element_id,
                text=f"{name} {name} {description}"  # Repeat name to give it more weight
            )
            
            doc_with_embedding = self.embedding_client.generate_embeddings(doc)
            
            if not doc_with_embedding.embedding:
                logger.warning(f"Could not generate embedding for element: {name}")
                return []
            
            # Perform vector search with a low threshold to get diverse candidates
            similar_terms = self.business_term_manager.vector_store.find_similar_vectors(
                query_vector=doc_with_embedding.embedding,
                top_k=top_k,
                threshold=0.05  # Very low threshold to get diverse candidates
            )
            
            return similar_terms
        
        except Exception as e:
            logger.error(f"Error getting vector candidates: {e}")
            return []
    
    def _prepare_llm_context(self, all_terms: List[Any], vector_candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Prepare business terms for LLM context in an efficient format, prioritizing
        vector candidates but including a representative subset of all terms.
        
        Args:
            all_terms: All business terms from the repository
            vector_candidates: Terms found by vector search
            
        Returns:
            List of terms formatted for LLM context
        """
        context_terms = []
        seen_ids = set()
        max_context_terms = 100  # Limit to protect LLM context window
        
        # First add all vector candidates
        for term in vector_candidates:
            if term["id"] not in seen_ids:
                seen_ids.add(term["id"])
                context_terms.append({
                    "id": term["id"],
                    "name": term["name"],
                    "description": term["description"]
                })
            
            if len(context_terms) >= max_context_terms:
                return context_terms
        
        # Calculate remaining slots
        remaining_slots = max_context_terms - len(context_terms)
        
        # If we still have room, add a sampling of other terms
        # that weren't in the vector candidates
        other_terms = []
        for term in all_terms:
            if term.id not in seen_ids:
                other_terms.append({
                    "id": term.id,
                    "name": term.name,
                    "description": term.description
                })
        
        # If we have more other terms than remaining slots, use a sampling strategy
        if len(other_terms) > remaining_slots and remaining_slots > 0:
            # Take terms at regular intervals to get a representative sample
            step = len(other_terms) // remaining_slots
            sampled_terms = [other_terms[i] for i in range(0, len(other_terms), step)][:remaining_slots]
            context_terms.extend(sampled_terms)
        else:
            # Add all remaining terms if they fit
            context_terms.extend(other_terms[:remaining_slots])
        
        return context_terms
    
    async def _match_with_llm(self, element_name: str, element_description: str, 
                           context_terms: List[Dict[str, Any]], top_k: int) -> List[Dict[str, Any]]:
        """
        Use the LLM to match the data element with business terms from the full context.
        
        Args:
            element_name: Name of the data element
            element_description: Description of the data element
            context_terms: Business terms formatted for LLM context
            top_k: Number of top matches to return
            
        Returns:
            List of matched terms with scores and reasoning
        """
        try:
            # Format the business terms for the prompt
            business_terms_text = ""
            for i, term in enumerate(context_terms):
                business_terms_text += f"Term {i+1}:\n"
                business_terms_text += f"ID: {term['id']}\n"
                business_terms_text += f"Name: {term['name']}\n"
                business_terms_text += f"Description: {term['description']}\n\n"
            
            # Create a prompt for the LLM evaluation
            prompt = f"""
            You are an expert in data governance and business terminology. Your task is to find the best business 
            term matches for a data element from a comprehensive list of business terms.

            DATA ELEMENT TO TAG:
            Name: {element_name}
            Description: {element_description}

            AVAILABLE BUSINESS TERMS:
            {business_terms_text}

            Based on semantic meaning and conceptual alignment, identify the best matching business terms for this data element.
            Consider:
            1. Conceptual alignment - does the business term represent the same concept as the data element?
            2. Semantic meaning - do they refer to the same thing, even if using different words?
            3. Scope and specificity - is the business term at the right level of detail?

            DO NOT simply match based on keywords. Focus on true semantic meaning and business purpose.

            Respond with a JSON array of objects containing your top {top_k} matches:
            [
                {{
                    "id": "term_id",
                    "name": "term_name",
                    "description": "term_description",
                    "score": 0.0-1.0,  // Your confidence score from 0 to 1
                    "reasoning": "Detailed explanation of why this term matches"
                }},
                ...
            ]

            If no good matches exist (all would score below 0.5), return an empty array [] and explain why modeling a new term is needed.
            Sort the results by score (highest first).
            """
            
            # Use the LLM to find matches
            from langchain_core.prompts import PromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            prompt_template = PromptTemplate.from_template(prompt)
            chain = prompt_template | self.llm | StrOutputParser()
            
            result = await chain.ainvoke({})
            
            # Extract JSON from the response
            try:
                # Try to find JSON in the response
                json_match = re.search(r'\[\s*\{.*\}\s*\]|\[\s*\]', result, re.DOTALL)
                if json_match:
                    llm_results = json.loads(json_match.group(0))
                else:
                    # If no JSON found, attempt to parse the entire result
                    llm_results = json.loads(result)
                
                # If we got an empty array, look for the explanation in the text
                if not llm_results:
                    explanation_match = re.search(r'because|since|reason|explain', result, re.IGNORECASE)
                    explanation = result[explanation_match.start():] if explanation_match else "No suitable matches found. Consider modeling a new term."
                    
                    return []
                
                # Process the results
                matches = []
                for match in llm_results:
                    # Find the full term details from context_terms if needed
                    term_id = match.get("id")
                    term_name = match.get("name")
                    term_description = match.get("description")
                    
                    # If the LLM didn't return complete term details, look them up
                    if not term_name or not term_description:
                        term_details = next((t for t in context_terms if t["id"] == term_id), None)
                        if term_details:
                            term_name = term_details["name"]
                            term_description = term_details["description"]
                    
                    matches.append({
                        "id": term_id,
                        "name": term_name,
                        "description": term_description,
                        "score": match.get("score", 0.5),
                        "reasoning": match.get("reasoning", "")
                    })
                
                return matches
                
            except Exception as parse_error:
                logger.error(f"Error parsing LLM results: {parse_error}")
                logger.debug(f"Raw LLM result: {result}")
                
                # Try to extract term IDs as a fallback
                term_ids = re.findall(r'"id"\s*:\s*"([^"]+)"', result)
                
                if term_ids:
                    # Construct basic matches from extracted IDs
                    fallback_matches = []
                    for term_id in term_ids[:top_k]:
                        # Find term details in context
                        term_details = next((t for t in context_terms if t["id"] == term_id), None)
                        if term_details:
                            fallback_matches.append({
                                "id": term_id,
                                "name": term_details["name"],
                                "description": term_details["description"],
                                "score": 0.5,  # Default score
                                "reasoning": "Extracted from LLM response during JSON parsing error"
                            })
                    
                    if fallback_matches:
                        return fallback_matches
                
                # If we couldn't extract anything useful, use vector candidates as fallback
                return []
                
        except Exception as e:
            logger.error(f"Error in LLM matching: {e}")
            return []
    
    async def _fallback_to_vector_search(self, element_id: str, name: str, description: str, top_k: int) -> List[Dict[str, Any]]:
        """
        Fallback to pure vector search when LLM context matching fails.
        
        Args:
            element_id: Element ID
            name: Element name
            description: Element description
            top_k: Number of results to return
            
        Returns:
            List of matched terms with scores
        """
        try:
            # Create embedding for the data element
            doc = MyDocument(
                id=element_id,
                text=f"{name} {name} {description}"  # Repeat name to give it more weight
            )
            
            doc_with_embedding = self.embedding_client.generate_embeddings(doc)
            
            if not doc_with_embedding.embedding:
                return []
            
            # Use vector search with standard threshold
            similar_terms = self.business_term_manager.vector_store.find_similar_vectors(
                query_vector=doc_with_embedding.embedding,
                top_k=top_k,
                threshold=0.3  # Standard threshold
            )
            
            # Format results
            results = []
            for term in similar_terms:
                results.append({
                    "id": term["id"],
                    "name": term["name"],
                    "description": term["description"],
                    "score": term["similarity"],
                    "reasoning": "Match found using vector similarity search"
                })
            
            return results
            
        except Exception as e:
            logger.error(f"Error in fallback vector search: {e}")
            return []
