import os
import csv
import logging
import uuid
from typing import Dict, List, Any, Optional, Tuple
from pydantic import BaseModel, Field
import numpy as np
import chromadb
from chromadb.config import Settings
from app.core.embedding import EmbeddingClient, MyDocument
from app.config.environment import get_os_env

logger = logging.getLogger(__name__)

class BusinessTerm(BaseModel):
    """Model representing a preferred business term."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    name: str
    description: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
class TaggingResult(BaseModel):
    """Model representing the result of tagging an element with business terms."""
    element_name: str
    element_description: str
    matching_terms: List[Dict[str, Any]]
    confidence_scores: List[float]
    
class TaggingValidationResult(BaseModel):
    """Model representing the validation result of tagging."""
    is_valid: bool
    feedback: str
    suggested_alternatives: List[Dict[str, Any]] = Field(default_factory=list)
    
class BusinessTermManager:
    """Manager for business terms and tagging operations."""
    
    def __init__(self, persistent_dir: str = "./chroma_db"):
        """Initialize the business term manager."""
        self.env = get_os_env()
        self.embedding_client = EmbeddingClient()
        self.persistent_dir = persistent_dir
        self.chroma_client = self._setup_chroma_db()
        
        # Get or create collection with HNSW index using cosine similarity
        try:
            # Try to get existing collection first
            self.collection = self.chroma_client.get_collection("business_terms")
            logger.info("Retrieved existing business_terms collection")
        except Exception as e:
            # Collection doesn't exist, create new one with HNSW and cosine similarity
            logger.info("Creating new business_terms collection with HNSW cosine space")
            self.collection = self.chroma_client.create_collection(
                name="business_terms",
                metadata={"hnsw:space": "cosine"},  # Use cosine similarity for HNSW index
                embedding_function=None  # We'll provide embeddings manually
            )
    
    def _setup_chroma_db(self):
        """Set up the ChromaDB client with persistent storage."""
        try:
            # Create directory if it doesn't exist
            os.makedirs(self.persistent_dir, exist_ok=True)
            
            # Initialize ChromaDB client with persistent storage using current API
            client = chromadb.PersistentClient(
                path=self.persistent_dir,
                settings=chromadb.Settings(
                    anonymized_telemetry=False
                )
            )
            
            logger.info(f"ChromaDB initialized with persistent storage at {self.persistent_dir}")
            return client
        except Exception as e:
            logger.error(f"Error setting up ChromaDB: {e}")
            raise
    
    def compute_cosine_similarity(self, embedding1, embedding2):
        """
        Manually compute cosine similarity between two embeddings.
        
        Args:
            embedding1: First embedding vector
            embedding2: Second embedding vector
            
        Returns:
            float: Cosine similarity value between -1 and 1 (higher is more similar)
        """
        # Convert to numpy arrays
        a = np.array(embedding1)
        b = np.array(embedding2)
        
        # Compute dot product and magnitudes
        dot_product = np.dot(a, b)
        magnitude_a = np.linalg.norm(a)
        magnitude_b = np.linalg.norm(b)
        
        # Avoid division by zero
        if magnitude_a == 0 or magnitude_b == 0:
            return 0
            
        # Compute cosine similarity
        return dot_product / (magnitude_a * magnitude_b)
            
    def import_terms_from_csv(self, csv_path: str, batch_size: int = 100) -> int:
        """
        Import business terms from a CSV file.
        
        Args:
            csv_path: Path to the CSV file
            batch_size: Size of batches for adding to ChromaDB
            
        Returns:
            Number of terms imported
        """
        try:
            # First, get all existing terms from the collection
            existing_terms = {}
            try:
                results = self.collection.get(limit=10000)
                if results["ids"]:
                    for i, term_id in enumerate(results["ids"]):
                        name = results["metadatas"][i]["name"]
                        description = results["metadatas"][i]["description"]
                        # Create a key to identify unique terms
                        term_key = f"{name}::{description}"
                        existing_terms[term_key] = term_id
                logger.info(f"Found {len(existing_terms)} existing terms in ChromaDB")
            except Exception as e:
                logger.warning(f"Error getting existing terms: {e}")
                existing_terms = {}
            
            # Read the CSV and track which terms exist in it
            csv_term_keys = set()
            terms_to_add = []
            
            with open(csv_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                
                for row in reader:
                    if 'name' not in row or 'description' not in row:
                        logger.warning(f"Skipping row missing required fields: {row}")
                        continue
                    
                    name = row['name'].strip()
                    description = row['description'].strip()
                    term_key = f"{name}::{description}"
                    csv_term_keys.add(term_key)
                    
                    # If this term already exists in ChromaDB, skip it
                    if term_key in existing_terms:
                        continue
                    
                    # Add new term to our list to process
                    term_id = str(uuid.uuid4())
                    terms_to_add.append({
                        "id": term_id,
                        "name": name,
                        "description": description,
                        "term_key": term_key
                    })
            
            # Process new terms in batches
            added_count = 0
            for i in range(0, len(terms_to_add), batch_size):
                batch = terms_to_add[i:i+batch_size]
                
                # Process this batch
                ids = []
                embeddings = []
                metadatas = []
                documents = []
                
                for term in batch:
                    # Create document for embedding
                    doc = MyDocument(
                        id=term["id"],
                        text=f"{term['name']}. {term['description']}"
                    )
                    
                    # Generate embedding
                    doc_with_embedding = self.embedding_client.generate_embeddings(doc)
                    
                    if not doc_with_embedding.embedding:
                        logger.warning(f"Could not generate embedding for term: {term['name']}. Skipping.")
                        continue
                    
                    # Add to lists for batch addition
                    ids.append(term["id"])
                    embeddings.append(doc_with_embedding.embedding)
                    metadatas.append({
                        "name": term["name"],
                        "description": term["description"]
                    })
                    documents.append(f"{term['name']}. {term['description']}")
                
                # Add batch to ChromaDB
                if ids:
                    self.collection.add(
                        ids=ids,
                        embeddings=embeddings,
                        metadatas=metadatas,
                        documents=documents
                    )
                    added_count += len(ids)
                    logger.info(f"Added batch of {len(ids)} terms to ChromaDB")
            
            # Find terms to delete (in ChromaDB but not in CSV)
            terms_to_delete = []
            for term_key, term_id in existing_terms.items():
                if term_key not in csv_term_keys:
                    terms_to_delete.append(term_id)
            
            # Delete terms in batches
            deleted_count = 0
            for i in range(0, len(terms_to_delete), batch_size):
                batch = terms_to_delete[i:i+batch_size]
                self.collection.delete(ids=batch)
                deleted_count += len(batch)
                logger.info(f"Deleted batch of {len(batch)} terms from ChromaDB")
            
            logger.info(f"Import complete: Added {added_count} new terms, deleted {deleted_count} terms")
            return added_count
            
        except Exception as e:
            logger.error(f"Error importing terms from CSV: {e}")
            raise
    
    def tag_element(self, name: str, description: str, top_k: int = 3) -> TaggingResult:
        """
        Tag a data element with the most similar business terms.
        Since we've configured HNSW with cosine space, this will use cosine similarity.
        
        Args:
            name: Enhanced data element name
            description: Enhanced data element description
            top_k: Number of top terms to return
            
        Returns:
            Tagging result with matching terms and confidence scores
        """
        try:
            # Create document for embedding
            doc = MyDocument(
                id=str(uuid.uuid4()),
                text=f"{name}. {description}"
            )
            
            # Generate embedding
            doc_with_embedding = self.embedding_client.generate_embeddings(doc)
            
            if not doc_with_embedding.embedding:
                raise ValueError(f"Could not generate embedding for element: {name}")
            
            # Query the collection - will use HNSW with cosine space as configured
            results = self.collection.query(
                query_embeddings=[doc_with_embedding.embedding],
                n_results=top_k,
                include=["metadatas", "documents", "distances"]
            )
            
            matching_terms = []
            confidence_scores = []
            
            if results["ids"] and len(results["ids"][0]) > 0:
                for i, term_id in enumerate(results["ids"][0]):
                    # With HNSW cosine space, the distance is 1-cosine_similarity
                    # Convert back to similarity
                    distance = results["distances"][0][i]
                    cosine_similarity = 1 - distance
                    
                    matching_terms.append({
                        "id": term_id,
                        "name": results["metadatas"][0][i]["name"],
                        "description": results["metadatas"][0][i]["description"]
                    })
                    confidence_scores.append(cosine_similarity)
            
            return TaggingResult(
                element_name=name,
                element_description=description,
                matching_terms=matching_terms,
                confidence_scores=confidence_scores
            )
            
        except Exception as e:
            logger.error(f"Error tagging element: {e}")
            # Fall back to manual cosine similarity if query fails
            try:
                logger.info("Falling back to manual cosine similarity calculation")
                
                # Get all terms from the collection with their embeddings
                results = self.collection.get(include=["embeddings", "metadatas"])
                
                if not results["ids"] or len(results["ids"]) == 0:
                    logger.warning("No terms found in the collection")
                    return TaggingResult(
                        element_name=name,
                        element_description=description,
                        matching_terms=[],
                        confidence_scores=[]
                    )
                
                # Calculate cosine similarity manually for each term
                similarities = []
                for i, term_id in enumerate(results["ids"]):
                    # Get term embedding
                    term_embedding = results["embeddings"][i]
                    
                    # Calculate cosine similarity
                    similarity = self.compute_cosine_similarity(doc_with_embedding.embedding, term_embedding)
                    
                    similarities.append({
                        "id": term_id,
                        "name": results["metadatas"][i]["name"],
                        "description": results["metadatas"][i]["description"],
                        "similarity": similarity
                    })
                
                # Sort by similarity (highest first)
                sorted_similarities = sorted(similarities, key=lambda x: x["similarity"], reverse=True)
                
                # Get top_k results
                top_results = sorted_similarities[:top_k]
                
                matching_terms = []
                confidence_scores = []
                
                for result in top_results:
                    matching_terms.append({
                        "id": result["id"],
                        "name": result["name"],
                        "description": result["description"]
                    })
                    confidence_scores.append(result["similarity"])
                
                return TaggingResult(
                    element_name=name,
                    element_description=description,
                    matching_terms=matching_terms,
                    confidence_scores=confidence_scores
                )
            except Exception as fallback_error:
                logger.error(f"Fallback method also failed: {fallback_error}")
                raise e
    
    async def validate_tagging(self, tagging_result: TaggingResult) -> TaggingValidationResult:
        """
        Validate the tagging result to check if it makes sense.
        
        Args:
            tagging_result: The tagging result to validate
            
        Returns:
            Validation result with feedback and suggestions
        """
        try:
            # If there are no matching terms, validation fails
            if not tagging_result.matching_terms:
                return TaggingValidationResult(
                    is_valid=False,
                    feedback="No matching business terms found.",
                    suggested_alternatives=[]
                )
            
            # Check confidence scores - with cosine similarity, good matches have scores > 0.75
            highest_confidence = max(tagging_result.confidence_scores) if tagging_result.confidence_scores else 0
            
            # If highest confidence is less than 0.75, validation is questionable
            if highest_confidence < 0.75:
                # Find alternative terms with a different query approach
                alternative_doc = MyDocument(
                    id=str(uuid.uuid4()),
                    # Use only the name for a different perspective
                    text=tagging_result.element_name
                )
                
                alternative_doc_with_embedding = self.embedding_client.generate_embeddings(alternative_doc)
                
                if alternative_doc_with_embedding.embedding:
                    try:
                        # Try to use the configured collection first (HNSW with cosine similarity)
                        alt_results = self.collection.query(
                            query_embeddings=[alternative_doc_with_embedding.embedding],
                            n_results=5,  # Get more results to have alternatives after filtering
                            include=["metadatas", "documents", "distances"]
                        )
                        
                        alternatives = []
                        if alt_results["ids"] and len(alt_results["ids"][0]) > 0:
                            for i, term_id in enumerate(alt_results["ids"][0]):
                                if term_id not in [term["id"] for term in tagging_result.matching_terms]:
                                    # With HNSW cosine, distance is 1-cosine_similarity
                                    distance = alt_results["distances"][0][i]
                                    alt_confidence = 1 - distance
                                    
                                    alternatives.append({
                                        "id": term_id,
                                        "name": alt_results["metadatas"][0][i]["name"],
                                        "description": alt_results["metadatas"][0][i]["description"],
                                        "confidence": alt_confidence
                                    })
                            
                        # If we have alternatives, return them, otherwise fall through to manual method
                        if alternatives:
                            return TaggingValidationResult(
                                is_valid=False,
                                feedback=f"Low confidence match ({highest_confidence:.2f}). Consider reviewing alternative terms.",
                                suggested_alternatives=alternatives[:3]  # Limit to top 3
                            )
                    
                    except Exception as e:
                        logger.warning(f"Error finding alternatives with HNSW query: {e}")
                        # Continue to manual method
                    
                    # Get all terms from the collection with their embeddings (manual method)
                    try:
                        results = self.collection.get(include=["embeddings", "metadatas"])
                        
                        if not results["ids"] or len(results["ids"]) == 0:
                            return TaggingValidationResult(
                                is_valid=False,
                                feedback=f"Low confidence match ({highest_confidence:.2f}) but no alternative terms found.",
                                suggested_alternatives=[]
                            )
                        
                        # Calculate cosine similarity manually for each term
                        alternative_similarities = []
                        for i, term_id in enumerate(results["ids"]):
                            # Skip terms that were already in the top matches
                            if term_id in [term["id"] for term in tagging_result.matching_terms]:
                                continue
                                
                            # Calculate cosine similarity
                            similarity = self.compute_cosine_similarity(
                                alternative_doc_with_embedding.embedding, 
                                results["embeddings"][i]
                            )
                            
                            alternative_similarities.append({
                                "id": term_id,
                                "name": results["metadatas"][i]["name"],
                                "description": results["metadatas"][i]["description"],
                                "confidence": similarity
                            })
                        
                        # Sort by similarity (highest first) and get top 3
                        sorted_alternatives = sorted(
                            alternative_similarities, 
                            key=lambda x: x["confidence"], 
                            reverse=True
                        )[:3]
                        
                        return TaggingValidationResult(
                            is_valid=False,
                            feedback=f"Low confidence match ({highest_confidence:.2f}). Consider reviewing alternative terms.",
                            suggested_alternatives=sorted_alternatives
                        )
                        
                    except Exception as e:
                        logger.error(f"Error finding alternative terms: {e}")
                        # If manual method fails, return without alternatives
                        return TaggingValidationResult(
                            is_valid=False,
                            feedback=f"Low confidence match ({highest_confidence:.2f}). Consider reviewing different terms.",
                            suggested_alternatives=[]
                        )
            
            # High confidence match
            return TaggingValidationResult(
                is_valid=True,
                feedback=f"Strong match with highest confidence of {highest_confidence:.2f}.",
                suggested_alternatives=[]
            )
            
        except Exception as e:
            logger.error(f"Error validating tagging: {e}")
            raise
    
    def get_all_terms(self) -> List[BusinessTerm]:
        """
        Get all business terms in the collection.
        
        Returns:
            List of all business terms
        """
        try:
            # Query all items (limited to 10000 for safety)
            results = self.collection.get(limit=10000)
            
            terms = []
            if results["ids"]:
                for i, term_id in enumerate(results["ids"]):
                    terms.append(BusinessTerm(
                        id=term_id,
                        name=results["metadatas"][i]["name"],
                        description=results["metadatas"][i]["description"]
                    ))
            
            return terms
            
        except Exception as e:
            logger.error(f"Error getting all terms: {e}")
            raise
    
    def get_term_by_id(self, term_id: str) -> Optional[BusinessTerm]:
        """
        Get a business term by its ID.
        
        Args:
            term_id: ID of the term to retrieve
            
        Returns:
            Business term if found, None otherwise
        """
        try:
            results = self.collection.get(ids=[term_id])
            
            if results["ids"] and len(results["ids"]) > 0:
                return BusinessTerm(
                    id=results["ids"][0],
                    name=results["metadatas"][0]["name"],
                    description=results["metadatas"][0]["description"]
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Error getting term by ID: {e}")
            raise
