"""
AI Tagging Evaluation Agent - Uses AI/LLM to evaluate term matches for quality.

This agent evaluates the quality of business term matches and provides detailed
reasoning about the match quality, allowing for more accurate tagging decisions.
"""

import logging
import re
from typing import Dict, Any, List, Optional, Tuple, Union
from app.core.models import TaggingResult
from app.config.settings import get_llm

logger = logging.getLogger(__name__)

class AITaggingEvaluationAgent:
    """
    Agent that uses AI/LLM to evaluate the quality of business term matches.
    
    This agent provides detailed analysis of term matches and can determine
    if the matches are appropriate for the data element or if new term modeling
    is required.
    """
    
    def __init__(self):
        """Initialize the AI tagging evaluation agent."""
        # No need to initialize the LLM here, we'll get it when needed
        pass
    
    async def evaluate_tagging_result(self, tagging_result: TaggingResult) -> Tuple[bool, float, str, TaggingResult]:
        """
        Evaluate tagging results using LLM to determine if matches are appropriate.
        
        Args:
            tagging_result: Tagging result to evaluate
            
        Returns:
            Tuple containing:
            - is_valid: Whether any terms are valid matches
            - overall_confidence: Confidence score (0-1)
            - reasoning: Detailed reasoning for the evaluation
            - improved_result: Potentially improved tagging result
        """
        try:
            # Skip evaluation if modeling is required or no matches found
            if tagging_result.modeling_required or not tagging_result.matching_terms:
                return (
                    False, 
                    0.0, 
                    "No suitable matches found or modeling already required.", 
                    tagging_result
                )
            
            # Get LLM from settings
            llm = get_llm()
            if not llm:
                logger.warning("LLM not available for tagging evaluation")
                # Return a simplified result based on confidence scores
                avg_confidence = sum(tagging_result.confidence_scores) / len(tagging_result.confidence_scores) if tagging_result.confidence_scores else 0.0
                return (
                    avg_confidence >= 0.5,
                    avg_confidence,
                    f"Basic confidence evaluation: {avg_confidence:.2f}",
                    tagging_result
                )
            
            # Format terms for the LLM
            formatted_terms = self._format_terms_for_prompt(tagging_result.matching_terms, tagging_result.confidence_scores)
            
            # Create the evaluation prompt
            evaluation_prompt = f"""
            You are an expert in data governance and business terminology with deep domain knowledge.
            Your task is to evaluate whether the following business terms are appropriate for a data element.
            
            Data Element:
            - Name: {tagging_result.element_name}
            - Description: {tagging_result.element_description}
            
            Matched Business Terms:
            {formatted_terms}
            
            Provide your evaluation as follows:
            
            1. Analysis of Each Term:
               - For each term, assess how well it matches the data element semantically
               - Consider conceptual alignment, completeness, and appropriate specificity
               - Provide a score from 0 to 10 for each term
            
            2. Overall Assessment:
               - Determine if ANY of the terms are good matches (score >= 7)
               - Provide an overall confidence score between 0.0 and 1.0
               - Explain your reasoning in detail
               
            3. Best Match:
               - Identify which term (if any) is the best match and why
               
            4. Final Recommendation:
               - Should the data element be modeled as a new term? Yes or No
               - If No, which existing term should be used?
            
            Your assessment:
            """
            
            # Get response from LLM
            from langchain_core.output_parsers import StrOutputParser
            
            # Create a chain for evaluation
            chain = llm | StrOutputParser()
            
            # Get reasoning with timeout
            import asyncio
            try:
                reasoning_task = chain.ainvoke(evaluation_prompt)
                reasoning = await asyncio.wait_for(reasoning_task, timeout=20.0)
            except asyncio.TimeoutError:
                logger.warning("LLM evaluation timed out")
                # Fall back to confidence scores
                avg_confidence = sum(tagging_result.confidence_scores) / len(tagging_result.confidence_scores) if tagging_result.confidence_scores else 0.0
                return (
                    avg_confidence >= 0.5,
                    avg_confidence,
                    "LLM evaluation timed out. Using average confidence score.",
                    tagging_result
                )
            
            # Extract key information from the reasoning
            is_valid = self._extract_is_valid(reasoning)
            overall_confidence = self._extract_confidence(reasoning)
            best_match_index = self._extract_best_match_index(reasoning, tagging_result.matching_terms)
            
            # Create an improved result if a clear best match was identified
            improved_result = tagging_result
            if best_match_index is not None and 0 <= best_match_index < len(tagging_result.matching_terms):
                # Put the best match first and adjust confidence scores
                best_term = tagging_result.matching_terms[best_match_index]
                best_confidence = max(tagging_result.confidence_scores[best_match_index], overall_confidence)
                
                # Create a new list with the best match first, followed by others
                reordered_terms = [best_term] + [term for i, term in enumerate(tagging_result.matching_terms) if i != best_match_index]
                reordered_scores = [best_confidence] + [score for i, score in enumerate(tagging_result.confidence_scores) if i != best_match_index]
                
                improved_result = TaggingResult(
                    element_id=tagging_result.element_id,
                    element_name=tagging_result.element_name,
                    element_description=tagging_result.element_description,
                    matching_terms=reordered_terms,
                    confidence_scores=reordered_scores,
                    modeling_required=not is_valid,
                    message=f"AI evaluated matches with confidence {overall_confidence:.2f}"
                )
            
            return (is_valid, overall_confidence, reasoning, improved_result)
        
        except Exception as e:
            logger.error(f"Error evaluating tagging result: {e}")
            # Return original result with low confidence on error
            return (
                False,
                0.3,
                f"Error during evaluation: {str(e)}",
                tagging_result
            )
    
    async def find_better_matches(self, 
                                element_name: str, 
                                element_description: str, 
                                all_terms: List[Dict[str, Any]], 
                                current_matches: List[Dict[str, Any]],
                                max_suggestions: int = 3) -> List[Dict[str, Any]]:
        """
        Find better matches using LLM reasoning across all available terms.
        
        Args:
            element_name: Name of the data element
            element_description: Description of the data element
            all_terms: All available business terms
            current_matches: Current matched terms (to exclude)
            max_suggestions: Maximum number of better matches to suggest
            
        Returns:
            List of better matching terms if found
        """
        try:
            # Get LLM from settings
            llm = get_llm()
            if not llm:
                logger.warning("LLM not available for finding better matches")
                return []
            
            # Format current matches for reference
            current_matches_text = "\n".join([
                f"- {term['name']}: {term['description'][:100]}..."
                for term in current_matches
            ])
            
            # Extract current term IDs to exclude
            current_ids = set(term.get("id", "") for term in current_matches)
            
            # Select a sample of other terms (not in current matches)
            other_terms = [term for term in all_terms if term["id"] not in current_ids]
            
            # If we have too many terms, first filter by basic text similarity
            if len(other_terms) > 30:
                filtered_terms = self._pre_filter_terms(element_name, element_description, other_terms, max_terms=30)
            else:
                filtered_terms = other_terms
            
            # Format terms for prompt
            terms_text = ""
            for i, term in enumerate(filtered_terms):
                terms_text += f"TERM {i+1}:\n"
                terms_text += f"ID: {term['id']}\n"
                terms_text += f"Name: {term['name']}\n"
                terms_text += f"Description: {term['description']}\n"
                # Add metadata if present
                if "metadata" in term and term["metadata"]:
                    for key, value in term["metadata"].items():
                        terms_text += f"{key}: {value}\n"
                terms_text += "\n"
            
            # Create the prompt for finding better matches
            prompt = f"""
            You are an expert in data governance and business terminology with deep domain knowledge.
            Your task is to identify if any of the following business terms are better matches for a data element
            than the current matches.
            
            Data Element:
            - Name: {element_name}
            - Description: {element_description}
            
            Current Matches:
            {current_matches_text}
            
            Alternative Terms:
            {terms_text}
            
            Analyze each alternative term and identify up to {max_suggestions} terms that are better matches
            for the data element than the current matches.
            
            For each better match you identify, provide:
            1. The term number
            2. Why it's a better match than the current matches
            3. A similarity score from 0.0 to 1.0 representing how good the match is
            
            Format your response as follows:
            
            BETTER MATCH #1:
            Term: [Term number]
            Reason: [Detailed explanation]
            Similarity: [Score between 0.0 and 1.0]
            
            BETTER MATCH #2:
            ...
            
            If none of the alternative terms are better matches, explicitly state "No better matches found."
            """
            
            # Get response from LLM
            from langchain_core.output_parsers import StrOutputParser
            chain = llm | StrOutputParser()
            
            # Get reasoning with timeout
            import asyncio
            try:
                reasoning_task = chain.ainvoke(prompt)
                reasoning = await asyncio.wait_for(reasoning_task, timeout=30.0)
            except asyncio.TimeoutError:
                logger.warning("LLM analysis timed out when finding better matches")
                return []
            
            # Check if response indicates no better matches
            if "no better matches found" in reasoning.lower():
                logger.info("LLM found no better matches")
                return []
            
            # Extract better matches
            better_matches = []
            match_sections = reasoning.split("BETTER MATCH #")
            
            for section in match_sections[1:]:  # Skip the first (empty) section
                # Extract term number
                term_match = re.search(r"Term:\s*(\d+)", section)
                if not term_match:
                    continue
                    
                try:
                    term_idx = int(term_match.group(1)) - 1  # Convert to 0-based
                    
                    if term_idx < 0 or term_idx >= len(filtered_terms):
                        continue
                        
                    # Extract similarity score
                    similarity_match = re.search(r"Similarity:\s*(\d+\.\d+)", section)
                    similarity = 0.7  # Default if not found
                    
                    if similarity_match:
                        similarity = float(similarity_match.group(1))
                        similarity = max(0.0, min(similarity, 1.0))  # Ensure in range [0,1]
                    
                    # Get the term
                    term = filtered_terms[term_idx]
                    
                    # Create matching term entry
                    term_entry = {
                        "id": term["id"],
                        "name": term["name"],
                        "description": term["description"],
                        "similarity": similarity
                    }
                    
                    # Add any metadata
                    if "metadata" in term:
                        for key, value in term["metadata"].items():
                            term_entry[key] = value
                    
                    better_matches.append(term_entry)
                    
                    if len(better_matches) >= max_suggestions:
                        break
                        
                except (ValueError, IndexError):
                    continue
            
            # Sort by similarity (highest first)
            better_matches.sort(key=lambda x: x.get("similarity", 0), reverse=True)
            
            return better_matches
            
        except Exception as e:
            logger.error(f"Error finding better matches: {e}")
            return []
    
    def _format_terms_for_prompt(self, matching_terms: List[Dict[str, Any]], confidence_scores: List[float]) -> str:
        """Format matching terms for use in an LLM prompt."""
        result = ""
        for i, (term, score) in enumerate(zip(matching_terms, confidence_scores)):
            result += f"TERM {i+1} (Similarity: {score:.2f})\n"
            result += f"Name: {term['name']}\n"
            result += f"Description: {term['description']}\n"
            
            # Include all available metadata
            if "category" in term:
                result += f"Category: {term['category']}\n"
            if "cdm" in term:
                result += f"CDM: {term['cdm']}\n"
                
            result += "\n"
        return result
    
    def _extract_is_valid(self, reasoning: str) -> bool:
        """Extract whether any terms are valid matches from the reasoning."""
        # Look for positive indicators
        positive_indicators = [
            "good match",
            "appropriate match",
            "score >= 7",
            "score ≥ 7",
            "score of 7 or higher",
            "recommended term",
            "existing term should be used",
            "no new modeling required",
            "modeling is not required"
        ]
        
        # Look for negative indicators
        negative_indicators = [
            "no good match",
            "no appropriate match",
            "modeling is required",
            "new term should be created",
            "should be modeled as a new term: yes"
        ]
        
        reasoning_lower = reasoning.lower()
        
        # Check for explicit recommendations
        if "final recommendation:" in reasoning_lower:
            recommendation_section = reasoning_lower.split("final recommendation:")[1].strip().split("\n")[0]
            if any(neg in recommendation_section.lower() for neg in ["yes", "new term", "modeling required"]):
                return False
            if any(pos in recommendation_section.lower() for pos in ["no", "existing term", "no modeling"]):
                return True
        
        # Check for positive and negative indicators
        positive_matches = sum(1 for indicator in positive_indicators if indicator in reasoning_lower)
        negative_matches = sum(1 for indicator in negative_indicators if indicator in reasoning_lower)
        
        # If more positive than negative, consider valid
        return positive_matches > negative_matches
    
    def _extract_confidence(self, reasoning: str) -> float:
        """Extract the overall confidence score from the reasoning."""
        import re
        
        # Look for confidence patterns
        confidence_patterns = [
            r"confidence score[^\d]*?(\d+\.\d+)",
            r"confidence[^\d]*?(\d+\.\d+)",
            r"overall confidence[^\d]*?(\d+\.\d+)",
            r"confidence level[^\d]*?(\d+\.\d+)"
        ]
        
        # Try each pattern
        for pattern in confidence_patterns:
            matches = re.search(pattern, reasoning.lower())
            if matches:
                try:
                    confidence = float(matches.group(1))
                    return max(0.0, min(confidence, 1.0))  # Ensure in range [0,1]
                except ValueError:
                    pass
        
        # If no explicit confidence found, look for scores out of 10
        score_pattern = r"score[^\d]*?(\d+)(?:\s*\/\s*10| out of 10)"
        score_matches = re.findall(score_pattern, reasoning.lower())
        if score_matches:
            try:
                # Average the scores and convert to 0-1 scale
                avg_score = sum(int(score) for score in score_matches) / len(score_matches)
                return avg_score / 10.0
            except ValueError:
                pass
        
        # Default confidence based on text sentiment
        positive_indicators = ["good match", "appropriate", "recommended", "suitable"]
        negative_indicators = ["poor match", "inappropriate", "not recommended", "unsuitable"]
        
        positive_count = sum(reasoning.lower().count(indicator) for indicator in positive_indicators)
        negative_count = sum(reasoning.lower().count(indicator) for indicator in negative_indicators)
        
        # Calculate a sentiment-based confidence
        total = positive_count + negative_count
        if total > 0:
            return 0.5 + (0.4 * (positive_count - negative_count) / total)
        
        # Default moderate confidence
        return 0.5
    
    def _extract_best_match_index(self, reasoning: str, matching_terms: List[Dict[str, Any]]) -> Optional[int]:
        """Extract the index of the best matching term from the reasoning."""
        import re
        
        # Look for explicit best match statements
        best_match_patterns = [
            r"best match.*?(?:is|:|=)\s*(?:term|match)?\s*(\d+)",
            r"term\s*(\d+)\s*is the best match",
            r"recommend(?:ed)? term\s*(\d+)"
        ]
        
        for pattern in best_match_patterns:
            matches = re.search(pattern, reasoning.lower())
            if matches:
                try:
                    term_num = int(matches.group(1))
                    if 1 <= term_num <= len(matching_terms):
                        return term_num - 1  # Convert to 0-based index
                except ValueError:
                    pass
        
        # If no explicit mention, look for term names
        for i, term in enumerate(matching_terms):
            term_name = term["name"].lower()
            # Look for patterns like "X is the best match" where X is the term name
            if re.search(fr"{re.escape(term_name)}\s+is\s+the\s+best\s+match", reasoning.lower()):
                return i
        
        # If no clear best match found, return the one with highest score pattern
        score_pattern = r"term\s*(\d+).*?score.*?(\d+)"
        score_matches = re.findall(score_pattern, reasoning.lower())
        
        if score_matches:
            # Find the term with the highest score
            max_score = 0
            max_term_idx = None
            
            for term_str, score_str in score_matches:
                try:
                    term_idx = int(term_str) - 1  # Convert to 0-based
                    score = int(score_str)
                    
                    if score > max_score and 0 <= term_idx < len(matching_terms):
                        max_score = score
                        max_term_idx = term_idx
                except ValueError:
                    pass
            
            return max_term_idx
        
        # No best match identified
        return None
    
    def _pre_filter_terms(self, element_name: str, element_description: str, terms: List[Dict[str, Any]], max_terms: int = 30) -> List[Dict[str, Any]]:
        """
        Pre-filter terms by basic text similarity to reduce the number for LLM analysis.
        This is a simple filtering approach for when we have a large number of terms.
        
        Args:
            element_name: Name of the data element
            element_description: Description of the data element
            terms: List of terms to filter
            max_terms: Maximum number of terms to return
            
        Returns:
            Filtered list of terms
        """
        # Simple function to score based on text overlap
        def text_similarity_score(term):
            name = term["name"].lower()
            description = term["description"].lower()
            
            element_name_lower = element_name.lower()
            element_desc_lower = element_description.lower()
            
            # Check for exact or partial name matches
            name_score = 0
            if name == element_name_lower:
                name_score = 1.0
            elif name in element_name_lower or element_name_lower in name:
                name_score = 0.8
            else:
                # Calculate word overlap
                name_words = set(name.split())
                element_words = set(element_name_lower.split())
                if name_words and element_words:
                    overlap = len(name_words.intersection(element_words))
                    name_score = overlap / max(len(name_words), len(element_words))
            
            # Check for key terms in description
            desc_score = 0
            element_key_words = set([word for word in element_name_lower.split() if len(word) > 3])
            if element_key_words:
                matches = sum(1 for word in element_key_words if word in description)
                desc_score = matches / len(element_key_words) * 0.5
            
            # Combine scores (name match is more important)
            return name_score * 0.7 + desc_score * 0.3
        
        # Score and sort terms
        scored_terms = [(term, text_similarity_score(term)) for term in terms]
        scored_terms.sort(key=lambda x: x[1], reverse=True)
        
        # Return top terms
        return [term for term, score in scored_terms[:max_terms]]
