"""
Application Settings - Configuration settings and utilities for the API.

This module provides functions to retrieve and manage application settings, 
including language model configuration, vector database selection, and other
global settings.
"""

import logging
import os
from typing import Optional, Union, Dict, Any, TYPE_CHECKING
from langchain_openai import AzureChatOpenAI
from azure.identity import get_bearer_token_provider
from app.config.environment import get_os_env
from utils.auth_helper import get_azure_token_manual  # Import the manual token helper

# TYPE_CHECKING is used to avoid circular imports during type checking
if TYPE_CHECKING:
    from app.core.vector_store import VectorStore

logger = logging.getLogger(__name__)

def get_vector_db_type() -> str:
    """
    Get the vector database type from environment settings.
    
    Returns:
        str: Vector database type (always "chroma" to avoid pgvector issues)
    """
    # Always return "chroma" regardless of environment settings
    # This ensures we always use ChromaDB and avoid proxy issues with pgvector
    return "chroma"

def get_vector_store(vector_db_type: Optional[str] = None) -> 'VectorStore':
    """
    Get the vector store implementation based on settings.
    
    Args:
        vector_db_type: Override the vector database type from environment
    
    Returns:
        VectorStore: The vector store implementation (always ChromaDB)
    """
    from app.core.vector_store import VectorStore
    
    # Always use ChromaDB regardless of input parameter
    from app.core.vector_store_chroma import ChromaDBVectorStore
    persist_dir = os.environ.get("CHROMA_PERSIST_DIR", "./data/chroma_db")
    collection_name = os.environ.get("CHROMA_COLLECTION", "business_terms")
    logger.info(f"Using ChromaDB as vector store with collection '{collection_name}' in '{persist_dir}'")
    return ChromaDBVectorStore(collection_name=collection_name, persist_dir=persist_dir)

def get_llm(proxy_enabled: Optional[bool] = True) -> AzureChatOpenAI:
    """
    Get the language model for the application.
    
    Args:
        proxy_enabled: Override the PROXY_ENABLED setting in the config file (default: True)
    
    Returns:
        AzureChatOpenAI: The language model
    """
    # Get environment with proxy setting override if provided
    env = get_os_env(proxy_enabled=proxy_enabled)
    
    logger.info(f"Setting up Azure OpenAI client with proxy enabled: {env.get('PROXY_ENABLED', 'True')}")
    
    try:
        # First try the standard token provider approach
        token_provider = get_bearer_token_provider(
            env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
    except Exception as e:
        logger.warning(f"Standard token provider failed: {e}. Trying manual approach.")
        
        # Fall back to manual token approach if the standard approach fails
        try:
            token = get_azure_token_manual(
                tenant_id=env.get("AZURE_TENANT_ID", ""),
                client_id=env.get("AZURE_CLIENT_ID", ""),
                client_secret=env.get("AZURE_CLIENT_SECRET", ""),
                scope="https://cognitiveservices.azure.com/.default"
            )
            
            if token:
                logger.info("Successfully obtained Azure token using manual method")
                # Create a simple callable token provider
                token_provider = lambda: token
            else:
                logger.error("Failed to obtain Azure token using manual method")
                # Return a mock LLM object that will log errors instead of making API calls
                return MockLLM()
        except Exception as e2:
            logger.error(f"Manual token acquisition also failed: {e2}")
            return MockLLM()
    
    # Get model configuration
    model_name = env.get("MODEL_NAME", "gpt-4o")
    temperature = float(env.get("TEMPERATURE", "0.3"))  # Lower temperature for more deterministic outputs
    max_tokens = int(env.get("MAX_TOKENS", "2000"))
    api_version = env.get("API_VERSION", "2023-05-15")
    azure_endpoint = env.get("AZURE_ENDPOINT", "")
    
    logger.info(f"Using model: {model_name}, temperature: {temperature}, max_tokens: {max_tokens}")
    
    # Create and return the LLM
    return AzureChatOpenAI(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
        api_version=api_version,
        azure_endpoint=azure_endpoint,
        azure_ad_token_provider=token_provider
    )

# Mock LLM class for fallback when authentication fails
class MockLLM:
    """Mock LLM that logs errors instead of making API calls."""
    
    def __init__(self):
        self.error_message = "Azure authentication failed. Using mock LLM."
        logger.error(self.error_message)
    
    def invoke(self, *args, **kwargs):
        logger.error(f"Cannot invoke LLM: {self.error_message}")
        return "Azure authentication failed. Please check your proxy settings and credentials."
    
    async def ainvoke(self, *args, **kwargs):
        logger.error(f"Cannot invoke LLM asynchronously: {self.error_message}")
        return "Azure authentication failed. Please check your proxy settings and credentials."

def get_app_settings() -> Dict[str, Any]:
    """
    Get all application settings.
    
    Returns:
        Dict[str, Any]: Dictionary containing all application settings
    """
    env = get_os_env()
    
    # Vector database settings - always use ChromaDB
    vector_db_type = "chroma"  # Force ChromaDB
    vector_db_settings = {
        "type": vector_db_type,
        "persist_dir": env.get("CHROMA_PERSIST_DIR", "./data/chroma_db"),
        "collection": env.get("CHROMA_COLLECTION", "business_terms")
    }
    
    # PostgreSQL settings
    pg_settings = {
        "host": env.get("PG_HOST", "localhost"),
        "port": int(env.get("PG_PORT", "5432")),
        "database": env.get("PG_DB", "metadata_db"),
        "user": env.get("PG_USER", "postgres"),
        "db_schema": env.get("PG_SCHEMA", "ai_stitching_platform"),  # Changed from 'schema' to 'db_schema'
        "min_connections": int(env.get("PG_MIN_CONNECTIONS", "2")),
        "max_connections": int(env.get("PG_MAX_CONNECTIONS", "10"))
    }
    
    # OpenAI model settings
    model_settings = {
        "model": env.get("MODEL_NAME", "gpt-4o"),
        "temperature": float(env.get("TEMPERATURE", "0.3")),
        "max_tokens": int(env.get("MAX_TOKENS", "2000")),
        "api_version": env.get("API_VERSION", "2023-05-15"),
        "azure_endpoint": env.get("AZURE_ENDPOINT", "")
    }
    
    # Proxy settings - default to enabled
    proxy_settings = {
        "enabled": env.get("PROXY_ENABLED", "True").lower() in ("true", "t", "1", "yes", "y"),
        "domain": env.get("HTTPS_PROXY_DOMAIN", "")
    }
    
    # General settings
    general_settings = {
        "similarity_threshold": float(env.get("SIMILARITY_THRESHOLD", "0.5")),
        "monitoring_interval": int(env.get("MONITORING_INTERVAL", "300")),
        "secured_endpoints": env.get("SECURED_ENDPOINTS", "False").lower() in ("true", "t", "1", "yes", "y")
    }
    
    return {
        "version": "1.0.0",
        "vector_database": vector_db_settings,
        "postgresql": pg_settings,
        "model": model_settings,
        "proxy": proxy_settings,
        "general": general_settings
    }
