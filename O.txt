import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set the style for our plots
plt.style.use('ggplot')
sns.set_palette("Set2")

def analyze_data_quality_issues(df_path):
    """
    Comprehensive analysis of data quality issues in the incident dataframe
    
    Parameters:
    -----------
    df_path : str
        Path to the CSV file containing the incident data
    
    Returns:
    --------
    df : pandas.DataFrame
        The loaded dataframe
    """
    # Load the dataframe
    df = pd.read_csv(df_path)
    
    # Print basic information about the dataframe
    print(f"Dataset shape: {df.shape}")
    print("\nFirst few rows:")
    print(df.head())
    
    # Check for missing values
    print("\nMissing values by column:")
    print(df.isnull().sum())
    
    # Basic statistics
    print("\n--- BASIC STATISTICS ---")
    total_incidents = len(df)
    data_issues = df[df['is_data_issue'] == True].shape[0]
    data_issue_pct = (data_issues / total_incidents) * 100
    
    print(f"Total incidents: {total_incidents}")
    print(f"Data quality issues: {data_issues} ({data_issue_pct:.2f}%)")
    print(f"Non-data issues: {total_incidents - data_issues} ({100 - data_issue_pct:.2f}%)")
    
    # Analyze data quality dimensions
    print("\n--- DATA QUALITY DIMENSION ANALYSIS ---")
    # Check that data_quality_dimension is populated when is_data_issue is True
    quality_check = df[(df['is_data_issue'] == True) & (df['data_quality_dimension'].isnull())].shape[0]
    print(f"Number of data issues without a quality dimension: {quality_check}")
    
    # Get distribution of data quality dimensions
    if data_issues > 0:
        dimension_counts = df[df['is_data_issue'] == True]['data_quality_dimension'].value_counts()
        dimension_pct = dimension_counts / data_issues * 100
        
        print("\nData Quality Dimension Distribution:")
        for dimension, count in dimension_counts.items():
            print(f"{dimension}: {count} ({dimension_pct[dimension]:.2f}%)")
        
        # Create a bar chart of data quality dimensions
        plt.figure(figsize=(12, 6))
        sns.barplot(x=dimension_counts.index, y=dimension_counts.values)
        plt.title('Data Quality Dimension Distribution')
        plt.xlabel('Dimension')
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('data_quality_dimensions.png')
        plt.close()
    
    # Analyze confidence scores
    print("\n--- CONFIDENCE SCORE ANALYSIS ---")
    avg_overall = df['confidence_score'].mean()
    avg_data_issues = df[df['is_data_issue'] == True]['confidence_score'].mean()
    avg_non_data_issues = df[df['is_data_issue'] == False]['confidence_score'].mean()
    
    print(f"Overall average confidence score: {avg_overall:.4f}")
    print(f"Average confidence for data issues: {avg_data_issues:.4f}")
    print(f"Average confidence for non-data issues: {avg_non_data_issues:.4f}")
    
    # Statistical significance test for difference in confidence scores
    t_stat, p_value = stats.ttest_ind(
        df[df['is_data_issue'] == True]['confidence_score'],
        df[df['is_data_issue'] == False]['confidence_score'],
        equal_var=False  # Welch's t-test (doesn't assume equal variances)
    )
    
    print(f"\nT-test for difference in confidence scores between data and non-data issues:")
    print(f"t-statistic: {t_stat:.4f}, p-value: {p_value:.4f}")
    print(f"The difference is {'statistically significant' if p_value < 0.05 else 'not statistically significant'} at Î±=0.05")
    
    # Visualize confidence score comparison
    plt.figure(figsize=(10, 6))
    data_to_plot = [
        df['confidence_score'],
        df[df['is_data_issue'] == True]['confidence_score'],
        df[df['is_data_issue'] == False]['confidence_score']
    ]
    labels = ['Overall', 'Data Issues', 'Non-Data Issues']
    
    sns.boxplot(data=data_to_plot)
    plt.title('Confidence Score Comparison')
    plt.ylabel('Confidence Score')
    plt.xticks(range(len(labels)), labels)
    plt.tight_layout()
    plt.savefig('confidence_score_comparison.png')
    plt.close()
    
    # Analyze confidence scores by data quality dimension
    if data_issues > 0:
        print("\nAverage Confidence Score by Data Quality Dimension:")
        dimension_confidence = df[df['is_data_issue'] == True].groupby('data_quality_dimension')['confidence_score'].agg(['mean', 'count', 'std']).sort_values('mean', ascending=False)
        
        for dimension, stats in dimension_confidence.iterrows():
            print(f"{dimension}: {stats['mean']:.4f} (count: {stats['count']}, std: {stats['std']:.4f})")
        
        # Visualize confidence by dimension
        plt.figure(figsize=(12, 6))
        sns.barplot(
            x=dimension_confidence.index, 
            y=dimension_confidence['mean'],
            yerr=dimension_confidence['std'],
            capsize=0.2
        )
        plt.title('Average Confidence Score by Data Quality Dimension')
        plt.xlabel('Dimension')
        plt.ylabel('Avg. Confidence Score')
        plt.xticks(rotation=45)
        plt.ylim(0, 1)  # Assuming confidence is between 0 and 1
        plt.tight_layout()
        plt.savefig('confidence_by_dimension.png')
        plt.close()
        
        # Create a heatmap for the counts
        pivot_data = pd.pivot_table(
            df[df['is_data_issue'] == True], 
            values='id', 
            index='data_quality_dimension',
            aggfunc='count'
        )
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(pivot_data, annot=True, cmap='YlGnBu', fmt='g')
        plt.title('Data Quality Issue Counts')
        plt.tight_layout()
        plt.savefig('data_quality_heatmap.png')
        plt.close()
    
    # Text analysis on incident descriptions
    print("\n--- TEXT ANALYSIS OF INCIDENT DESCRIPTIONS ---")
    # Extract most common words in data issue descriptions
    if data_issues > 0:
        from collections import Counter
        import re
        
        # Define a function to tokenize and clean text
        def tokenize(text):
            # Convert to lowercase and split into words
            words = re.findall(r'\b\w+\b', text.lower())
            # Remove common stop words (you can expand this list)
            stop_words = {'the', 'and', 'is', 'in', 'to', 'of', 'a', 'with', 'for', 'on', 'at'}
            return [word for word in words if word not in stop_words and len(word) > 2]
        
        # Tokenize all data issue descriptions
        all_words = []
        for desc in df[df['is_data_issue'] == True]['incident_description']:
            if isinstance(desc, str):  # Ensure it's a string (not NaN)
                all_words.extend(tokenize(desc))
        
        # Count word frequencies
        word_counts = Counter(all_words)
        print("\nMost common words in data issue descriptions:")
        for word, count in word_counts.most_common(15):
            print(f"{word}: {count}")
        
        # Word cloud (if available)
        try:
            from wordcloud import WordCloud
            
            # Create and generate a word cloud image
            wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(all_words))
            
            # Display the word cloud
            plt.figure(figsize=(10, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('Common Words in Data Issue Descriptions')
            plt.tight_layout()
            plt.savefig('data_issue_wordcloud.png')
            plt.close()
            print("\nWord cloud created for data issue descriptions")
        except ImportError:
            print("\nNote: WordCloud library not available. Install with 'pip install wordcloud' to generate word clouds.")
    
    # Correlations and advanced analysis
    print("\n--- CORRELATION ANALYSIS ---")
    
    # Convert categorical variables to numeric for correlation analysis
    df_corr = df.copy()
    df_corr['is_data_issue_num'] = df_corr['is_data_issue'].astype(int)
    
    # One-hot encode data quality dimensions
    if data_issues > 0:
        dummies = pd.get_dummies(df_corr['data_quality_dimension'], prefix='dim')
        df_corr = pd.concat([df_corr, dummies], axis=1)
    
    # Calculate correlations with confidence score
    corr_columns = ['confidence_score', 'is_data_issue_num'] + [col for col in df_corr.columns if col.startswith('dim_')]
    corr_matrix = df_corr[corr_columns].corr()
    
    print("\nCorrelation with confidence_score:")
    print(corr_matrix['confidence_score'].sort_values(ascending=False))
    
    # Visualize correlations
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=True, fmt='.2f', square=True, linewidths=.5)
    plt.title('Correlation Matrix')
    plt.tight_layout()
    plt.savefig('correlation_matrix.png')
    plt.close()
    
    print("\nAnalysis complete. Visualizations saved as PNG files.")
    return df

# Example usage:
# df = analyze_data_quality_issues('your_incidents_data.csv')

# To run additional analysis or create custom visualizations:
def custom_analysis(df):
    """
    Run custom analysis on the dataframe based on specific needs
    
    Parameters:
    -----------
    df : pandas.DataFrame
        The dataframe containing incident data
    """
    # Example: Time-based analysis if you have a date/time column
    if 'incident_date' in df.columns:
        # Convert to datetime if needed
        df['incident_date'] = pd.to_datetime(df['incident_date'])
        
        # Group by month and data issue type
        monthly_data = df.groupby([
            pd.Grouper(key='incident_date', freq='M'),
            'is_data_issue'
        ]).size().unstack().fillna(0)
        
        # Plot time series
        plt.figure(figsize=(14, 6))
        monthly_data.plot(kind='line', marker='o')
        plt.title('Data Issues Over Time')
        plt.xlabel('Month')
        plt.ylabel('Number of Incidents')
        plt.legend(['Non-Data Issues', 'Data Issues'])
        plt.grid(True)
        plt.tight_layout()
        plt.savefig('issues_over_time.png')
        plt.close()
    
    # Example: Joint plot of data issue vs confidence
    plt.figure(figsize=(10, 6))
    sns.violinplot(x='is_data_issue', y='confidence_score', data=df)
    plt.title('Confidence Score Distribution by Issue Type')
    plt.xlabel('Is Data Issue')
    plt.ylabel('Confidence Score')
    plt.tight_layout()
    plt.savefig('confidence_distribution.png')
    plt.close()

# Example: Analyzing detailed patterns in data issues
def analyze_data_issue_patterns(df):
    """
    Analyze patterns in data issues to identify common characteristics
    
    Parameters:
    -----------
    df : pandas.DataFrame
        The dataframe containing incident data
    """
    # Only analyze data issues
    data_issues_df = df[df['is_data_issue'] == True].copy()
    
    if len(data_issues_df) == 0:
        print("No data issues found for pattern analysis.")
        return
    
    # 1. Frequency of each dimension
    dimension_freq = data_issues_df['data_quality_dimension'].value_counts(normalize=True) * 100
    
    # 2. Word frequency by dimension (for more nuanced analysis)
    from collections import Counter
    import re
    
    def tokenize(text):
        if not isinstance(text, str):
            return []
        words = re.findall(r'\b\w+\b', text.lower())
        stop_words = {'the', 'and', 'is', 'in', 'to', 'of', 'a', 'with', 'for', 'on', 'at'}
        return [word for word in words if word not in stop_words and len(word) > 2]
    
    # Create a dictionary to store words by dimension
    dimension_words = {}
    
    for dimension in data_issues_df['data_quality_dimension'].unique():
        if not pd.isna(dimension):
            dimension_texts = data_issues_df[data_issues_df['data_quality_dimension'] == dimension]['incident_description']
            all_words = []
            for text in dimension_texts:
                all_words.extend(tokenize(text))
            
            dimension_words[dimension] = Counter(all_words).most_common(10)
    
    # Print results
    print("\n--- DATA ISSUE PATTERN ANALYSIS ---")
    print("\nFrequency of Data Quality Dimensions:")
    for dimension, percentage in dimension_freq.items():
        print(f"{dimension}: {percentage:.2f}%")
    
    print("\nTop words in descriptions by dimension:")
    for dimension, word_counts in dimension_words.items():
        print(f"\n{dimension}:")
        for word, count in word_counts:
            print(f"  {word}: {count}")
    
    # 3. Optional: Clustered analysis of incident descriptions
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.cluster import KMeans
        from sklearn.decomposition import PCA
        
        # Only use rows with non-null descriptions
        text_df = data_issues_df[data_issues_df['incident_description'].notna()].copy()
        
        if len(text_df) >= 5:  # Need a minimum number for clustering
            # Create TF-IDF vectors
            tfidf = TfidfVectorizer(max_features=100, stop_words='english')
            tfidf_matrix = tfidf.fit_transform(text_df['incident_description'])
            
            # Determine optimal number of clusters (2-5)
            from sklearn.metrics import silhouette_score
            
            silhouette_scores = []
            K = range(2, min(6, len(text_df)))
            
            for k in K:
                kmeans = KMeans(n_clusters=k, random_state=42)
                kmeans.fit(tfidf_matrix)
                silhouette_scores.append(silhouette_score(tfidf_matrix, kmeans.labels_))
            
            # Choose best K
            best_k = K[np.argmax(silhouette_scores)]
            print(f"\nOptimal number of clusters: {best_k}")
            
            # Final clustering
            kmeans = KMeans(n_clusters=best_k, random_state=42)
            text_df['cluster'] = kmeans.fit_predict(tfidf_matrix)
            
            # Get top terms per cluster
            order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
            terms = tfidf.get_feature_names_out()
            
            print("\nTop terms per cluster:")
            for i in range(best_k):
                print(f"\nCluster {i}:")
                print(', '.join([terms[ind] for ind in order_centroids[i, :10]]))
                
                # Show dimension distribution in this cluster
                cluster_dims = text_df[text_df['cluster'] == i]['data_quality_dimension'].value_counts(normalize=True) * 100
                print("\nDimension distribution in this cluster:")
                for dim, pct in cluster_dims.items():
                    print(f"  {dim}: {pct:.2f}%")
                
                # Show sample descriptions
                print("\nSample descriptions:")
                sample_indices = text_df[text_df['cluster'] == i].index[:3]
                for idx in sample_indices:
                    desc = data_issues_df.loc[idx, 'incident_description']
                    print(f"  - {desc[:100]}...")
            
            # Visualize the clusters (if possible)
            pca = PCA(n_components=2)
            coords = pca.fit_transform(tfidf_matrix.toarray())
            
            # Create a plot
            plt.figure(figsize=(12, 8))
            for i in range(best_k):
                cluster_points = coords[text_df['cluster'] == i]
                plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')
            
            plt.title('Clusters of Data Issue Descriptions')
            plt.xlabel('PCA Component 1')
            plt.ylabel('PCA Component 2')
            plt.legend()
            plt.tight_layout()
            plt.savefig('data_issue_clusters.png')
            plt.close()
            
            print("\nCluster visualization saved as 'data_issue_clusters.png'")
    except ImportError:
        print("\nNote: Advanced text clustering requires scikit-learn. Install with 'pip install scikit-learn' to perform clustering analysis.")
    
    # 4. Resolution analysis
    print("\n--- RESOLUTION ANALYSIS ---")
    
    # Extract common terms in resolutions by dimension
    resolution_words = {}
    
    for dimension in data_issues_df['data_quality_dimension'].unique():
        if not pd.isna(dimension):
            resolution_texts = data_issues_df[data_issues_df['data_quality_dimension'] == dimension]['incident_resolution']
            all_words = []
            for text in resolution_texts:
                all_words.extend(tokenize(text))
            
            resolution_words[dimension] = Counter(all_words).most_common(10)
    
    print("\nCommon resolution terms by dimension:")
    for dimension, word_counts in resolution_words.items():
        print(f"\n{dimension} resolutions common terms:")
        for word, count in word_counts:
            print(f"  {word}: {count}")

# Example: Generate summary recommendations
def generate_recommendations(df):
    """
    Generate recommendations based on the data quality analysis
    
    Parameters:
    -----------
    df : pandas.DataFrame
        The dataframe containing incident data
    """
    # Find the most common data quality dimensions
    if 'data_quality_dimension' in df.columns:
        data_issues_df = df[df['is_data_issue'] == True]
        
        if len(data_issues_df) > 0:
            top_dimensions = data_issues_df['data_quality_dimension'].value_counts().head(3)
            
            print("\n--- RECOMMENDATIONS BASED ON DATA ANALYSIS ---")
            print("\nBased on the data quality analysis, consider focusing on these areas:")
            
            for dimension, count in top_dimensions.items():
                if dimension == 'Completeness':
                    print(f"\n1. Improve {dimension} ({count} incidents):")
                    print("   - Implement required field validation in data entry forms")
                    print("   - Add data completeness checks in ETL processes")
                    print("   - Monitor data completeness metrics over time")
                
                elif dimension == 'Accuracy':
                    print(f"\n2. Enhance {dimension} ({count} incidents):")
                    print("   - Implement validation rules for critical data elements")
                    print("   - Consider automated data quality scanning tools")
                    print("   - Establish regular data reconciliation processes")
                
                elif dimension == 'Consistency':
                    print(f"\n3. Address {dimension} ({count} incidents):")
                    print("   - Standardize data formats across systems")
                    print("   - Implement data governance policies")
                    print("   - Create centralized reference data management")
                
                elif dimension == 'Timeliness':
                    print(f"\n4. Improve {dimension} ({count} incidents):")
                    print("   - Review and optimize data update frequencies")
                    print("   - Implement real-time data processing where critical")
                    print("   - Monitor data freshness metrics")
                
                elif dimension == 'Validity':
                    print(f"\n5. Enhance {dimension} ({count} incidents):")
                    print("   - Implement stronger data validation rules")
                    print("   - Add pre-processing validation steps")
                    print("   - Consider constraint enforcement at database level")
                
                elif dimension == 'Uniqueness':
                    print(f"\n6. Address {dimension} ({count} incidents):")
                    print("   - Implement unique key constraints")
                    print("   - Add duplicate detection processes")
                    print("   - Review entity resolution procedures")
            
            # General recommendations
            print("\nGeneral recommendations:")
            print("1. Establish a data quality framework with clear ownership and metrics")
            print("2. Implement regular data quality monitoring and reporting")
            print("3. Create a data quality issue resolution process with SLAs")
            print("4. Consider data profiling tools to proactively identify issues")
        else:
            print("\nNo data quality issues found in the dataset to generate recommendations.")

if __name__ == "__main__":
    # Replace with your actual file path
    file_path = "your_incidents_data.csv"
    
    print("Running data quality analysis...")
    df = analyze_data_quality_issues(file_path)
    
    # Uncomment these lines to run additional analyses
    # custom_analysis(df)
    # analyze_data_issue_patterns(df)
    # generate_recommendations(df)
