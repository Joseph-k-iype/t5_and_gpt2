import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union, Tuple, Callable, Literal
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel, Field
from langchain_openai import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.schema.document import Document as LC_DOCUMENT
from langchain.embeddings.base import Embeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
from pydantic import BaseModel, ValidationError, field_validator
import langgraph.graph
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class

class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        self.credential = self._get_credential()
    
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(tenant_id=self.get("AZURE_TENANT_ID"), client_id=self.get("AZURE_CLIENT_ID"), client_secret=self.get("AZURE_CLIENT_SECRET"))
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                raise FileNotFoundError(f"The file '{dotenvfile}' does not exist or is not readable")
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## embedding class + Document class

class MyDocument(BaseModel):
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}

class EmbeddingClient:
    def __init__(self, env: OSEnv, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.env = env
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            self.env.credential,
            "https://cognitiveservices.azure.com/.default"
        )
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        
        # Using positional parameter for token_provider as in original code
        return AzureOpenAI(token_provider, self.azure_api_version)
    
    def generate_embeddings(self, doc: MyDocument)->MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc
    
    def generate_batch_embeddings(self, docs: List[MyDocument]) -> List[MyDocument]:
        try:
            texts = [doc.text for doc in docs]
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=texts
            )
            
            for i, doc in enumerate(docs):
                doc.embedding = response.data[i].embedding
            
            return docs
        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            return docs

class AzureChatbot:
    def __init__(self, env: OSEnv):
        self.env = env
        self.llm = self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
    
    def _setup_chat_model(self):
        try:
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            
            # Using model_deployment_name instead of model_name - this is the key parameter
            # for AzureChatOpenAI in newer versions
            self.llm = AzureChatOpenAI(
                deployment_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider
            )
            
            return self.llm
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise
    
    def generate_reasoning(self, project_overview: str, purpose: str) -> str:
        messages = [
            SystemMessage(content="You are an AI assistant that helps categorize project overviews based on their purposes. Please provide clear, concise reasoning for why a project matches a particular purpose."),
            HumanMessage(content=f"Project Overview: {project_overview}\n\nPurpose: {purpose}\n\nExplain why this project overview matches (or doesn't match) this purpose. Keep your reasoning brief but specific.")
        ]
        
        result = self.llm.invoke(messages)
        return result.content

# Data models for LangGraph
class Purpose(BaseModel):
    id: str
    name: str
    embedding: List[float] = Field(default_factory=list)

class Project(BaseModel):
    case_id: str
    overview: str
    embedding: List[float] = Field(default_factory=list)

class PurposeMatch(BaseModel):
    purpose_id: str
    purpose_name: str
    matching_score: float
    reasoning: str = ""

class ProjectMapping(BaseModel):
    case_id: str
    overview: str
    matches: List[PurposeMatch] = Field(default_factory=list)

class AgentState(BaseModel):
    purposes: List[Purpose] = Field(default_factory=list)
    projects: List[Project] = Field(default_factory=list)
    current_project_index: int = 0
    results: List[ProjectMapping] = Field(default_factory=list)
    status: str = "initializing"

# File processing functions
def load_purposes_file(file_path: str) -> List[Purpose]:
    """Load the purposes from an Excel file."""
    try:
        df = pd.read_excel(file_path)
        purposes = []
        for i, col in enumerate(df.columns):
            if col and not pd.isna(col):  # Make sure column name is valid
                purpose = Purpose(id=str(i), name=col)
                purposes.append(purpose)
        
        if not purposes:
            logger.warning("No valid purposes found in the Excel file.")
        return purposes
    except Exception as e:
        logger.error(f"Error loading purposes file: {e}")
        raise

def load_projects_file(file_path: str) -> List[Project]:
    """Load the projects from a CSV file."""
    try:
        df = pd.read_csv(file_path)
        required_columns = ['case id', 'project overview']
        
        # Verify that required columns exist
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"Required column '{col}' not found in CSV file.")
        
        projects = []
        for _, row in df.iterrows():
            case_id = str(row['case id'])
            overview = str(row['project overview'])
            
            # Skip rows with empty overviews
            if pd.isna(overview) or not overview.strip():
                logger.warning(f"Skipping project with case ID {case_id} due to empty overview.")
                continue
                
            project = Project(case_id=case_id, overview=overview)
            projects.append(project)
        
        if not projects:
            logger.warning("No valid projects found in the CSV file.")
        return projects
    except Exception as e:
        logger.error(f"Error loading projects file: {e}")
        raise

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """Calculate cosine similarity between two vectors."""
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = sum(a * a for a in vec1) ** 0.5
    norm2 = sum(b * b for b in vec2) ** 0.5
    
    # Handle zero division case
    if norm1 == 0 or norm2 == 0:
        return 0.0
        
    return dot_product / (norm1 * norm2)

# LangGraph Agent Functions
def initialize(state: AgentState) -> AgentState:
    """Initialize the agent state with purposes and projects."""
    logger.info("Initializing agent...")
    state.status = "loading_data"
    return state

def load_data(state: AgentState, purposes_file: str, projects_file: str, env: OSEnv) -> AgentState:
    """Load data from files and generate embeddings."""
    try:
        logger.info("Loading purposes and projects data...")
        state.purposes = load_purposes_file(purposes_file)
        state.projects = load_projects_file(projects_file)
        
        embedding_client = EmbeddingClient(env)
        
        # Generate embeddings for purposes
        purpose_docs = [MyDocument(id=p.id, text=p.name) for p in state.purposes]
        purpose_docs = embedding_client.generate_batch_embeddings(purpose_docs)
        
        for i, doc in enumerate(purpose_docs):
            state.purposes[i].embedding = doc.embedding
            # Verify embeddings are valid
            if not doc.embedding or all(e == 0 for e in doc.embedding):
                logger.warning(f"Warning: Purpose '{state.purposes[i].name}' has an empty or zero embedding")
        
        # Generate embeddings for projects
        project_docs = [MyDocument(id=p.case_id, text=p.overview) for p in state.projects]
        project_docs = embedding_client.generate_batch_embeddings(project_docs)
        
        for i, doc in enumerate(project_docs):
            state.projects[i].embedding = doc.embedding
            # Verify embeddings are valid
            if not doc.embedding or all(e == 0 for e in doc.embedding):
                logger.warning(f"Warning: Project '{state.projects[i].case_id}' has an empty or zero embedding")
        
        state.status = "processing"
        return state
    except Exception as e:
        logger.error(f"Error in load_data: {e}")
        state.status = "error"
        return state

def process_next_project(state: AgentState, chatbot: AzureChatbot, top_k: int = 5) -> AgentState:
    """Process the next project in the queue using LLM-enhanced matching."""
    if state.current_project_index >= len(state.projects):
        state.status = "completed"
        return state
    
    current_project = state.projects[state.current_project_index]
    logger.info(f"Processing project {current_project.case_id}: {current_project.overview[:50]}...")
    
    # Step 1: Use similarity search to find candidate purposes (top-k)
    similarities = []
    for purpose in state.purposes:
        similarity = cosine_similarity(current_project.embedding, purpose.embedding)
        similarities.append((purpose, similarity))
    
    # Sort by similarity score (descending)
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # Get top-k purposes as candidates (or all if less than k)
    candidate_purposes = similarities[:min(top_k, len(similarities))]
    
    # Step 2: Use LLM to evaluate which purposes match the project
    project_mapping = evaluate_purposes_with_llm(
        chatbot, 
        current_project, 
        candidate_purposes, 
        state.purposes
    )
    
    state.results.append(project_mapping)
    state.current_project_index += 1
    
    if state.current_project_index >= len(state.projects):
        state.status = "completed"
    
    return state

def evaluate_purposes_with_llm(chatbot: AzureChatbot, project: Project, 
                              candidate_purposes: List[Tuple[Purpose, float]], 
                              all_purposes: List[Purpose]) -> ProjectMapping:
    """Use LLM to evaluate which purposes match the project and generate reasoning."""
    # Create a mapping for the current project
    project_mapping = ProjectMapping(
        case_id=project.case_id,
        overview=project.overview,
        matches=[]
    )
    
    # Prepare the list of all purposes for the prompt
    all_purposes_text = "\n".join([f"{i+1}. {purpose.name}" for i, purpose in enumerate(all_purposes)])
    
    # Prepare the list of candidate purposes (from similarity search)
    candidate_purposes_text = "\n".join([
        f"{all_purposes.index(purpose)+1}. {purpose.name} (Similarity: {similarity:.4f})"
        for purpose, similarity in candidate_purposes
    ])
    
    # Create a prompt for the LLM to evaluate which purposes match
    system_content = """You are an expert analyst tasked with mapping project overviews to their most relevant purposes. 
    Analyze the provided project overview and determine which purposes it aligns with from the candidate list.
    You should select ONLY the purposes that genuinely match the project's intent and scope.
    For each selected purpose, provide clear, specific reasoning explaining why it matches.
    Your analysis should be factual and objective, based solely on the content of the project overview."""
    
    user_content = f"""# Project Overview
{project.overview}

# All Available Purposes
{all_purposes_text}

# Candidate Purposes (from initial similarity search)
{candidate_purposes_text}

# Instructions
1. Review the project overview carefully
2. Evaluate each candidate purpose and determine if it genuinely matches the project
3. You may also select purposes not in the candidate list if they're clearly relevant
4. For each matching purpose, explain your reasoning
5. Format your response as follows:

MATCHING PURPOSES:
- Purpose ID: [purpose number from the list]
  Name: [purpose name]
  Reasoning: [clear explanation of why this purpose matches]
  
- Purpose ID: [next matching purpose number]
  ...

If no purposes match, state "NO MATCHING PURPOSES" and explain why."""
    
    # Get the LLM's evaluation using direct messages
    messages = [
        SystemMessage(content=system_content),
        HumanMessage(content=user_content)
    ]
    
    result = chatbot.llm.invoke(messages)
    response_text = result.content
    
    # Parse the LLM's response to extract matching purposes and reasoning
    if "NO MATCHING PURPOSES" in response_text:
        # No matching purposes found
        logger.info(f"No matching purposes found for project {project.case_id}")
        return project_mapping
    
    # Extract matching purposes and reasoning using regex
    pattern = r"Purpose ID: (\d+)[\s\n]+Name: (.+?)[\s\n]+Reasoning: (.+?)(?=\n\n-|\Z)"
    matches = re.finditer(pattern, response_text, re.DOTALL)
    
    for match in matches:
        purpose_id = int(match.group(1)) - 1  # Convert to 0-based index
        purpose_name = match.group(2).strip()
        reasoning = match.group(3).strip()
        
        # Find the similarity score if this was a candidate purpose
        similarity = 0.0
        for purpose, score in candidate_purposes:
            if purpose.id == str(purpose_id):
                similarity = score
                break
        
        # Add to the project mapping
        if 0 <= purpose_id < len(all_purposes):
            purpose_match = PurposeMatch(
                purpose_id=all_purposes[purpose_id].id,
                purpose_name=purpose_name,
                matching_score=similarity,
                reasoning=reasoning
            )
            project_mapping.matches.append(purpose_match)
    
    return project_mapping

# This function is no longer needed as reasoning is now generated during the evaluation process

def save_results(state: AgentState, output_file: str) -> AgentState:
    """Save the results to a file."""
    logger.info(f"Saving results to {output_file}...")
    
    results = []
    for project_mapping in state.results:
        for match in project_mapping.matches:
            results.append({
                "case_id": project_mapping.case_id,
                "project_overview": project_mapping.overview,
                "purpose": match.purpose_name,
                "matching_score": match.matching_score,
                "reasoning": match.reasoning
            })
    
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False)
    logger.info(f"Results saved to {output_file}")
    
    state.status = "saved"
    return state

def should_continue_processing(state: AgentState) -> str:
    """Determine the next step in the workflow."""
    if state.current_project_index < len(state.projects) and state.status == "processing":
        return "process_next"
    else:
        return "save_results"

# Main function to run the pipeline
def main(purposes_file: str, projects_file: str, output_file: str, config_path: str = CONFIG_PATH, creds_path: str = CREDS_PATH, cert_path: str = CERT_PATH):
    # Initialize environment
    env = OSEnv(config_path, creds_path, cert_path)
    chatbot = AzureChatbot(env)
    
    # Define the workflow graph
    workflow = StateGraph(AgentState)
    
    # Add nodes to the graph
    workflow.add_node("initialize", lambda state: initialize(state))
    workflow.add_node("load_data", lambda state: load_data(state, purposes_file, projects_file, env))
    workflow.add_node("process_next", lambda state: process_next_project(state, chatbot))
    workflow.add_node("save_results", lambda state: save_results(state, output_file))
    
    # Add edges to the graph
    workflow.add_edge(START, "initialize")
    workflow.add_edge("initialize", "load_data")
    workflow.add_edge("load_data", "process_next")
    workflow.add_conditional_edges(
        "process_next",
        should_continue_processing,
        {
            "process_next": "process_next",
            "save_results": "save_results"
        }
    )
    # Make save_results the end node
    # In some versions, we need to explicitly connect to END
    try:
        workflow.add_edge("save_results", END)
    except Exception as e:
        logger.warning(f"Could not add edge to END: {e}")
        # In this case, save_results will implicitly be an end node since
        # no outgoing edges are defined
    
    # Compile the graph
    app = workflow.compile()
    
    # Execute the graph
    logger.info("Starting project-purpose mapping workflow...")
    result = app.invoke({"purposes": [], "projects": [], "current_project_index": 0, "results": [], "status": "initializing"})
    
    # Check the result structure to access the final state
    try:
        if hasattr(result, 'status'):
            logger.info(f"Workflow completed with status: {result.status}")
        elif isinstance(result, dict) and 'status' in result:
            logger.info(f"Workflow completed with status: {result['status']}")
        else:
            logger.info("Workflow completed. Check the output file for results.")
    except Exception as e:
        logger.warning(f"Could not access status in result: {e}")
        logger.info("Workflow execution completed. Check the output file for results.")
    
    return result

if __name__ == "__main__":
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='Map projects to purposes using AI')
    parser.add_argument('--purposes', required=True, help='Path to the Excel file containing purposes')
    parser.add_argument('--projects', required=True, help='Path to the CSV file containing projects')
    parser.add_argument('--output', required=True, help='Path to save the output CSV file')
    parser.add_argument('--config', default=CONFIG_PATH, help='Path to the config file')
    parser.add_argument('--creds', default=CREDS_PATH, help='Path to the credentials file')
    parser.add_argument('--cert', default=CERT_PATH, help='Path to the certificate file')
    args = parser.parse_args()
    
    # Run the main function
    main(
        purposes_file=args.purposes,
        projects_file=args.projects,
        output_file=args.output,
        config_path=args.config,
        creds_path=args.creds,
        cert_path=args.cert
    )
