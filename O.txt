import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
from typing import Optional, Any, Dict, List, Union, Tuple
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# Import from your existing code
from paste import (
    OSEnv, MyDocument, EmbeddingClient, AzureChatbot,
    CONFIG_PATH, CREDS_PATH, CERT_PATH, logger
)

# Import ChromaDB
import chromadb
from chromadb.config import Settings

# Import LangGraph and LangChain
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain.chains import ConversationChain

# Simplified CSV data processor
class CSVProcessor:
    def __init__(self, first_csv_path: str, second_csv_path: str):
        self.first_csv_path = first_csv_path
        self.second_csv_path = second_csv_path
        self.first_df = None
        self.second_df = None
    
    def _detect_encoding(self, file_path: str) -> str:
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        return result['encoding']
    
    def load_data(self):
        # Load CSVs with proper encoding
        first_encoding = self._detect_encoding(self.first_csv_path)
        second_encoding = self._detect_encoding(self.second_csv_path)
        
        self.first_df = pd.read_csv(self.first_csv_path, encoding=first_encoding)
        self.second_df = pd.read_csv(self.second_csv_path, encoding=second_encoding)
        
        # Clean and normalize columns
        self.first_df.columns = [col.strip().lower() for col in self.first_df.columns]
        self.second_df.columns = [col.strip().lower() for col in self.second_df.columns]
        
        # Fill missing values
        self.first_df = self.first_df.fillna('')
        self.second_df = self.second_df.fillna('')
        
        logger.info(f"First CSV loaded with {len(self.first_df)} rows")
        logger.info(f"Second CSV loaded with {len(self.second_df)} rows")
        
        # Return a tuple of both dataframes
        return self.first_df, self.second_df

# Semantic Mapper with LangGraph integration - only persisting first CSV vectors
class SemanticMapper:
    def __init__(self, config_file: str, creds_file: str, cert_file: str, persist_dir: str):
        """Initialize the semantic mapper with required components."""
        # Setup environment
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.persist_dir = persist_dir
        
        # Create directory if it doesn't exist
        os.makedirs(self.persist_dir, exist_ok=True)
        
        # Initialize embedding client
        self.embedding_client = EmbeddingClient(
            azure_api_version=self.env.get("AZURE_API_VERSION", "2023-05-15"),
            embeddings_model=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
        )
        
        # Initialize ChromaDB client - only for persisting first CSV
        self.chroma_client = chromadb.PersistentClient(
            path=self.persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Initialize LLM for confidence scoring
        self.chatbot = AzureChatbot(config_file, creds_file, cert_file)
        
        # Collection name - only need one for the first CSV
        self.collection_name = "first_csv_vectors"
        
        # Create LangGraph for the matching workflow
        self.graph = self._create_agent_graph()
        
        # Confidence threshold
        self.confidence_threshold = 0.75
    
    def _create_or_get_collection(self):
        """Create or get the collection for first CSV vectors."""
        try:
            # Try to get the collection
            collection = self.chroma_client.get_collection(name=self.collection_name)
            logger.info(f"Retrieved existing collection: {self.collection_name}")
        except Exception:
            # Create if it doesn't exist
            collection = self.chroma_client.create_collection(name=self.collection_name)
            logger.info(f"Created new collection: {self.collection_name}")
        
        return collection
    
    def _create_document_with_embedding(self, doc_id: str, text: str, metadata: Dict[str, Any]) -> Tuple[str, List[float], Dict[str, Any]]:
        """Create a document and generate its embedding using the EmbeddingClient."""
        # Create MyDocument instance
        doc = MyDocument(id=doc_id, text=text, metadata=metadata)
        
        # Generate embedding
        embedded_doc = self.embedding_client.generate_embeddings(doc)
        
        return doc_id, embedded_doc.embedding, metadata
    
    def index_first_csv(self, first_df):
        """Index only the first CSV data into ChromaDB for persistence."""
        # Get or create collection
        collection = self._create_or_get_collection()
        
        # Check if collection already has data
        if collection.count() > 0:
            logger.info(f"Collection already contains {collection.count()} documents")
            # Optionally check if update is needed based on the first CSV size
            if collection.count() >= len(first_df):
                logger.info("Collection already contains all the first CSV data, skipping indexing")
                return
        
        # Process first CSV
        docs = []
        embeddings = []
        ids = []
        metadatas = []
        
        logger.info("Generating embeddings for first CSV data...")
        for idx, row in tqdm(first_df.iterrows(), total=len(first_df), desc="Processing first CSV"):
            # Create document text
            doc_id = f"first_{idx}"
            text = f"Name: {row['name']}\nDefinition: {row['definition']}\nOwned by: {row['owned by']}"
            
            # Create metadata
            metadata = {
                "name": row['name'],
                "definition": row['definition'],
                "owned_by": row['owned by'],
                "original_index": int(idx)
            }
            
            # Generate embedding
            doc_id, embedding, metadata = self._create_document_with_embedding(doc_id, text, metadata)
            
            # Store the results
            docs.append(text)
            embeddings.append(embedding)
            ids.append(doc_id)
            metadatas.append(metadata)
        
        # Add documents to collection in batches
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            end = min(i + batch_size, len(ids))
            collection.add(
                ids=ids[i:end],
                documents=docs[i:end],
                embeddings=embeddings[i:end],
                metadatas=metadatas[i:end]
            )
        
        logger.info(f"Indexed {len(ids)} documents from first CSV")
    
    def find_match_for_second_csv_row(self, row, n_results: int = 3) -> Dict:
        """Find matching names in the first CSV collection for a single row from the second CSV."""
        # Extract data from second CSV row
        name = row['name']
        
        # Build context string from available metadata
        context_parts = []
        for field in ['code', 'taxonomy path 1', 'taxonomy path 2', 'parent', 'description', 'record examples']:
            if field in row and row[field]:
                context_parts.append(f"{field.capitalize()}: {row[field]}")
        
        context = "; ".join(context_parts)
        
        # Create query text
        query_text = f"Name: {name}"
        if context:
            query_text += f"\nContext: {context}"
        
        # Generate embedding for query
        query_doc = MyDocument(id="query", text=query_text)
        embedded_query = self.embedding_client.generate_embeddings(query_doc)
        query_embedding = embedded_query.embedding
        
        # Query the collection
        collection = self._create_or_get_collection()
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        return results, name, context
    
    def _create_agent_graph(self):
        """Create the LangGraph for the name matching workflow."""
        try:
            # Define state type for the graph
            class AgentState(dict):
                """State for the name matching agent graph."""
                pass
            
            # Define the nodes in our graph
            def evaluate_match_quality(state):
                """Evaluate the quality of the best match and assign a confidence score."""
                source_name = state["source_name"]
                match_results = state["match_results"]
                source_context = state["source_context"]
                
                if not match_results["ids"] or len(match_results["ids"]) == 0:
                    return {
                        "best_match": None,
                        "confidence_score": 0.0,
                        "suggestion": None
                    }
                
                # Get best match
                best_match = {
                    "id": match_results["ids"][0],
                    "name": match_results["metadatas"][0]["name"],
                    "definition": match_results["metadatas"][0]["definition"],
                    "owned_by": match_results["metadatas"][0]["owned_by"],
                    "distance": match_results["distances"][0] if "distances" in match_results else None,
                }
                
                # Create prompt for LLM
                prompt = f"""
                Evaluate the quality of this semantic name match and provide a confidence score between 0 and 1:
                
                Source name: {source_name}
                Target match: {best_match['name']}
                
                Source context: {source_context}
                Target definition: {best_match['definition']}
                
                Based on semantic meaning, how confident are you that these names refer to the same concept?
                Provide your response as a JSON with the following structure:
                {{
                    "confidence_score": [score between 0 and 1],
                    "reasoning": "[your detailed reasoning]"
                }}
                """
                
                # Get evaluation from LLM
                response = self.chatbot.conversation.run(prompt)
                
                # Parse the JSON response
                try:
                    # Extract JSON from the response
                    json_str = response.strip()
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    evaluation = json.loads(json_str)
                    confidence_score = float(evaluation["confidence_score"])
                    
                    return {
                        "best_match": best_match,
                        "confidence_score": confidence_score,
                        "reasoning": evaluation.get("reasoning", "")
                    }
                except Exception as e:
                    logger.error(f"Error parsing LLM response: {e}")
                    # Default to using similarity score if parsing fails
                    confidence_score = 1 - min(1, best_match.get("distance", 0.5) / 2)
                    return {
                        "best_match": best_match,
                        "confidence_score": confidence_score,
                        "reasoning": f"Failed to parse LLM response, using similarity score instead: {confidence_score}"
                    }
            
            def suggest_better_name(state):
                """Suggest a better name if the match quality is poor."""
                confidence_score = state["confidence_score"]
                source_name = state["source_name"]
                best_match = state["best_match"]
                source_context = state["source_context"]
                
                if confidence_score >= self.confidence_threshold or not best_match:
                    # No need for suggestion if confidence is high enough
                    return {"suggestion": None}
                
                # Create a prompt for the LLM to suggest a better name
                prompt = f"""
                The current match for "{source_name}" is "{best_match['name']}" with a confidence score of {confidence_score}.
                This score is below the threshold of {self.confidence_threshold}.
                
                Source context: {source_context}
                Target definition: {best_match['definition']}
                
                Please suggest a better name from the first dataset that would be a more accurate match.
                If you cannot think of a better name, suggest how the current names could be standardized or harmonized.
                
                Provide your response as a JSON with the following structure:
                {{
                    "suggestion": "[your suggested name or standardization approach]",
                    "explanation": "[your detailed explanation]"
                }}
                """
                
                # Get suggestion from LLM
                response = self.chatbot.conversation.run(prompt)
                
                # Parse the JSON response
                try:
                    # Extract JSON from the response
                    json_str = response.strip()
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    suggestion_data = json.loads(json_str)
                    return {
                        "suggestion": suggestion_data["suggestion"],
                        "explanation": suggestion_data.get("explanation", "")
                    }
                except Exception as e:
                    logger.error(f"Error parsing LLM suggestion: {e}")
                    return {
                        "suggestion": "Error generating suggestion",
                        "explanation": f"Failed to parse LLM response: {str(e)}"
                    }
            
            def router(state):
                """Route to the next node based on confidence score."""
                confidence_score = state.get("confidence_score")
                
                if confidence_score is None:
                    return "evaluate_match"
                elif confidence_score < self.confidence_threshold:
                    return "suggest_better_name"
                else:
                    return END
            
            # Create the graph
            workflow = StateGraph(AgentState)
            
            # Add nodes
            workflow.add_node("evaluate_match", evaluate_match_quality)
            workflow.add_node("suggest_better_name", suggest_better_name)
            
            # Set the entry point
            workflow.set_entry_point("evaluate_match")
            
            # Add edges
            workflow.add_conditional_edges("evaluate_match", router)
            workflow.add_edge("suggest_better_name", END)
            
            # Compile the graph
            return workflow.compile()
        
        except Exception as e:
            logger.error(f"Error creating agent graph: {e}")
            raise
    
    def evaluate_with_agent(self, source_name: str, source_context: str, match_results: Dict) -> Dict:
        """Use the LangGraph agent to evaluate match quality and provide suggestions."""
        try:
            # Initialize the state
            initial_state = {
                "source_name": source_name,
                "source_context": source_context,
                "match_results": match_results,
                "best_match": None,
                "confidence_score": None,
                "suggestion": None
            }
            
            # Run the graph
            result = self.graph.invoke(initial_state)
            
            return {
                "source_name": source_name,
                "best_match": result["best_match"],
                "confidence_score": result["confidence_score"],
                "suggestion": result.get("suggestion"),
                "explanation": result.get("explanation", ""),
                "reasoning": result.get("reasoning", "")
            }
        except Exception as e:
            logger.error(f"Error evaluating match for {source_name}: {e}")
            raise
    
    def process_second_csv(self, second_df):
        """Process each row from the second CSV individually, without storing in ChromaDB."""
        results = []
        
        for idx, row in tqdm(second_df.iterrows(), total=len(second_df), desc="Mapping names"):
            try:
                # Find matches for this specific row
                match_results, name, context = self.find_match_for_second_csv_row(row)
                
                # Get the code from the row
                code = row['code'] if 'code' in row else ''
                
                # Evaluate match quality using LangGraph agent
                evaluation = self.evaluate_with_agent(name, context, match_results)
                
                # Create result entry
                result = {
                    "code": code,
                    "name": name,
                    "matched_name": evaluation["best_match"]["name"] if evaluation["best_match"] else None,
                    "matched_owned_by": evaluation["best_match"]["owned_by"] if evaluation["best_match"] else None,
                    "matched_definition": evaluation["best_match"]["definition"] if evaluation["best_match"] else None,
                    "confidence_score": evaluation["confidence_score"],
                    "suggestion": evaluation.get("suggestion")
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing row {idx}: {e}")
                results.append({
                    "code": row.get('code', ''),
                    "name": row.get('name', ''),
                    "matched_name": None,
                    "matched_owned_by": None, 
                    "matched_definition": None,
                    "confidence_score": 0.0,
                    "suggestion": f"Error: {str(e)}"
                })
        
        return results

# Main function to run the mapping
def run_mapping(first_csv: str, second_csv: str, config: str, creds: str, cert: str, persist_dir: str, output: str = None):
    """Run the semantic name mapping with only first CSV persisted in ChromaDB."""
    # Create CSV processor and load data
    csv_processor = CSVProcessor(first_csv, second_csv)
    first_df, second_df = csv_processor.load_data()
    
    # Create semantic mapper with LangGraph
    mapper = SemanticMapper(config, creds, cert, persist_dir)
    
    # Index only the first CSV into ChromaDB
    mapper.index_first_csv(first_df)
    
    # Process the second CSV dynamically (without persisting)
    results = mapper.process_second_csv(second_df)
    
    # Create DataFrame and save results
    results_df = pd.DataFrame(results)
    
    # Order columns
    column_order = ['code', 'name', 'matched_name', 'matched_owned_by', 'matched_definition', 'confidence_score', 'suggestion']
    results_df = results_df[column_order]
    
    # Save to CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output or f"name_mapping_results_{timestamp}.csv"
    results_df.to_csv(output_file, index=False)
    
    # Print summary
    good_matches = results_df[results_df['confidence_score'] >= 0.75]
    logger.info(f"Completed semantic name mapping with {len(results_df)} names")
    logger.info(f"Good matches: {len(good_matches)} ({len(good_matches)/len(results_df)*100:.2f}%)")
    logger.info(f"Results saved to {output_file}")
    
    return results_df

# Command-line interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Semantic name mapping between two CSV files")
    parser.add_argument("--first-csv", required=True, help="Path to the first CSV file (name, definition, owned by)")
    parser.add_argument("--second-csv", required=True, help="Path to the second CSV file with taxonomy data")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to the config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to the credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to the certificate file")
    parser.add_argument("--persist-dir", default="./chroma_db", help="Directory to persist vector database")
    parser.add_argument("--output", default=None, help="Path to save output CSV file")
    
    args = parser.parse_args()
    
    run_mapping(
        first_csv=args.first_csv,
        second_csv=args.second_csv,
        config=args.config,
        creds=args.creds,
        cert=args.cert,
        persist_dir=args.persist_dir,
        output=args.output
    )
