#!/usr/bin/env python
"""
Load testing script for PostgreSQL with pgvector.
This script tests the performance of vector search operations with various loads.

Usage:
    python -m tests.load_test_db --terms=10000 --queries=100 --concurrency=5
"""

import os
import sys
import time
import random
import string
import logging
import argparse
import threading
import concurrent.futures
from typing import List, Dict, Any
import numpy as np

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config.environment import get_os_env
from app.core.db_manager import DBManager

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)

logger = logging.getLogger(__name__)

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Load test PostgreSQL with pgvector")
    
    parser.add_argument("--terms", type=int, default=1000,
                        help="Number of terms to generate (default: 1000)")
    
    parser.add_argument("--queries", type=int, default=100,
                        help="Number of queries to perform (default: 100)")
    
    parser.add_argument("--dimensions", type=int, default=1536,
                        help="Embedding dimensions (default: 1536)")
    
    parser.add_argument("--concurrency", type=int, default=1,
                        help="Number of concurrent clients (default: 1)")
    
    parser.add_argument("--batch-size", type=int, default=100,
                        help="Batch size for inserts (default: 100)")
    
    parser.add_argument("--similarity-threshold", type=float, default=0.5,
                        help="Similarity threshold for queries (default: 0.5)")
    
    parser.add_argument("--top-k", type=int, default=3,
                        help="Number of results for queries (default: 3)")
    
    parser.add_argument("--clean", action="store_true",
                        help="Clean up generated data after test")
    
    parser.add_argument("--pg-host", type=str, 
                        help="PostgreSQL host")
    parser.add_argument("--pg-port", type=int, 
                        help="PostgreSQL port")
    parser.add_argument("--pg-user", type=str, 
                        help="PostgreSQL user")
    parser.add_argument("--pg-password", type=str, 
                        help="PostgreSQL password")
    parser.add_argument("--pg-db", type=str, 
                        help="PostgreSQL database name")
    
    return parser.parse_args()

def generate_random_string(length: int = 10) -> str:
    """Generate a random string of given length."""
    return ''.join(random.choices(string.ascii_lowercase, k=length))

def generate_random_embedding(dimensions: int) -> List[float]:
    """Generate a random embedding vector of given dimensions."""
    vector = np.random.rand(dimensions)
    return (vector / np.linalg.norm(vector)).tolist()  # Normalize to unit vector

def generate_test_data(args) -> List[Dict[str, Any]]:
    """Generate test data for load testing."""
    logger.info(f"Generating {args.terms} test terms...")
    
    terms = []
    for i in range(args.terms):
        term_id = f"test_term_{i}"
        name = f"test {generate_random_string(8)} {generate_random_string(5)}"
        description = f"This is a test term for {name} with random words {generate_random_string(20)}"
        
        embedding = generate_random_embedding(args.dimensions)
        
        terms.append({
            "id": term_id,
            "name": name,
            "description": description,
            "embedding": embedding,
            "metadata": {"test": True}
        })
    
    return terms

def insert_test_data(db_manager: DBManager, terms: List[Dict[str, Any]], batch_size: int) -> float:
    """Insert test data into the database."""
    logger.info(f"Inserting {len(terms)} terms in batches of {batch_size}...")
    
    start_time = time.time()
    
    inserted = 0
    for i in range(0, len(terms), batch_size):
        batch = terms[i:i + batch_size]
        
        count = db_manager.batch_store_vectors(batch)
        inserted += count
        
        if (i + batch_size) % 1000 == 0 or (i + batch_size) >= len(terms):
            logger.info(f"Inserted {inserted}/{len(terms)} terms")
    
    duration = time.time() - start_time
    logger.info(f"Inserted {inserted} terms in {duration:.2f} seconds ({inserted/duration:.2f} terms/sec)")
    
    return duration

def perform_queries(db_manager: DBManager, terms: List[Dict[str, Any]], 
                   args, thread_id: int = 0) -> List[Dict[str, Any]]:
    """Perform vector search queries."""
    queries_per_thread = args.queries // args.concurrency
    if thread_id == args.concurrency - 1:
        # Last thread gets any remainder
        queries_per_thread += args.queries % args.concurrency
    
    logger.info(f"Thread {thread_id}: Performing {queries_per_thread} queries...")
    
    results = []
    for i in range(queries_per_thread):
        # Randomly select a term to query
        query_term = random.choice(terms)
        
        # Start timing
        start_time = time.time()
        
        # Perform query
        similar_terms = db_manager.find_similar_vectors(
            query_vector=query_term["embedding"],
            top_k=args.top_k,
            threshold=args.similarity_threshold
        )
        
        # End timing
        duration = time.time() - start_time
        
        # Record results
        results.append({
            "query_id": f"thread_{thread_id}_query_{i}",
            "duration": duration,
            "num_results": len(similar_terms),
            "results": similar_terms
        })
    
    return results

def run_concurrent_queries(db_manager: DBManager, terms: List[Dict[str, Any]], args) -> List[Dict[str, Any]]:
    """Run queries concurrently."""
    logger.info(f"Running {args.queries} queries with {args.concurrency} concurrent clients...")
    
    start_time = time.time()
    
    all_results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=args.concurrency) as executor:
        # Submit tasks
        futures = [
            executor.submit(perform_queries, db_manager, terms, args, thread_id)
            for thread_id in range(args.concurrency)
        ]
        
        # Collect results
        for future in concurrent.futures.as_completed(futures):
            all_results.extend(future.result())
    
    total_duration = time.time() - start_time
    
    # Calculate statistics
    query_times = [result["duration"] for result in all_results]
    avg_query_time = sum(query_times) / len(query_times)
    min_query_time = min(query_times)
    max_query_time = max(query_times)
    p95_query_time = sorted(query_times)[int(len(query_times) * 0.95)]
    
    queries_per_second = len(all_results) / total_duration
    
    logger.info(f"Completed {len(all_results)} queries in {total_duration:.2f} seconds")
    logger.info(f"Queries per second: {queries_per_second:.2f}")
    logger.info(f"Average query time: {avg_query_time*1000:.2f} ms")
    logger.info(f"Min query time: {min_query_time*1000:.2f} ms")
    logger.info(f"Max query time: {max_query_time*1000:.2f} ms")
    logger.info(f"95th percentile query time: {p95_query_time*1000:.2f} ms")
    
    return all_results

def clean_up_test_data(db_manager: DBManager, terms: List[Dict[str, Any]]) -> None:
    """Clean up test data from the database."""
    logger.info(f"Cleaning up {len(terms)} test terms...")
    
    try:
        with db_manager.get_connection() as conn:
            with conn.cursor() as cursor:
                # Delete all test terms
                cursor.execute("""
                DELETE FROM business_terms 
                WHERE id LIKE 'test_term_%' AND metadata->>'test' = 'true'
                """)
                
                deleted = cursor.rowcount
                conn.commit()
                
                logger.info(f"Deleted {deleted} test terms")
    except Exception as e:
        logger.error(f"Error cleaning up test data: {e}")

def main():
    """Run the load test."""
    args = parse_args()
    
    # Set environment variables from command line
    if args.pg_host:
        os.environ["PG_HOST"] = args.pg_host
    if args.pg_port:
        os.environ["PG_PORT"] = str(args.pg_port)
    if args.pg_user:
        os.environ["PG_USER"] = args.pg_user
    if args.pg_password:
        os.environ["PG_PASSWORD"] = args.pg_password
    if args.pg_db:
        os.environ["PG_DB"] = args.pg_db
    
    # Get environment
    env = get_os_env()
    
    # Initialize DB manager
    logger.info("Initializing PostgreSQL connection...")
    db_manager = DBManager()
    
    # Check database connection
    db_health = db_manager.health_check()
    if db_health["status"] != "healthy":
        logger.error(f"PostgreSQL connection failed: {db_health.get('error', 'Unknown error')}")
        sys.exit(1)
    
    logger.info(f"Connected to PostgreSQL ({db_health['version']})")
    logger.info(f"pgvector extension: {'Enabled' if db_health['vector_enabled'] else 'Disabled'}")
    
    # Generate test data
    terms = generate_test_data(args)
    
    try:
        # Insert test data
        insert_duration = insert_test_data(db_manager, terms, args.batch_size)
        
        # Run queries
        results = run_concurrent_queries(db_manager, terms, args)
        
        # Print summary
        print("\n=== Load Test Summary ===")
        print(f"Database: PostgreSQL with pgvector")
        print(f"Host: {env.get('PG_HOST')}")
        print(f"Number of terms: {args.terms}")
        print(f"Vector dimensions: {args.dimensions}")
        print(f"Number of queries: {args.queries}")
        print(f"Concurrency: {args.concurrency}")
        print(f"Insert performance: {args.terms/insert_duration:.2f} terms/sec")
        print(f"Query performance: {args.queries/(sum(r['duration'] for r in results)/args.concurrency):.2f} queries/sec")
        print("============================\n")
        
    finally:
        # Clean up test data if requested
        if args.clean:
            clean_up_test_data(db_manager, terms)

if __name__ == "__main__":
    main()
