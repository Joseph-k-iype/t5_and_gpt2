import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
from typing import Optional, Any, Dict, List, Union, Tuple
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# Import from your existing code
from paste import (
    OSEnv, MyDocument, EmbeddingClient, AzureChatbot,
    CONFIG_PATH, CREDS_PATH, CERT_PATH, logger
)

# Import ChromaDB
import chromadb
from chromadb.config import Settings

# Import LangGraph and LangChain
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain.chains import ConversationChain

# Simplified CSV data processor
class CSVProcessor:
    def __init__(self, first_csv_path: str, second_csv_path: str):
        self.first_csv_path = first_csv_path
        self.second_csv_path = second_csv_path
        self.first_df = None
        self.second_df = None
    
    def _detect_encoding(self, file_path: str) -> str:
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        return result['encoding']
    
    def load_data(self):
        # Load CSVs with proper encoding
        first_encoding = self._detect_encoding(self.first_csv_path)
        second_encoding = self._detect_encoding(self.second_csv_path)
        
        self.first_df = pd.read_csv(self.first_csv_path, encoding=first_encoding)
        self.second_df = pd.read_csv(self.second_csv_path, encoding=second_encoding)
        
        # Clean and normalize columns
        self.first_df.columns = [col.strip().lower() for col in self.first_df.columns]
        self.second_df.columns = [col.strip().lower() for col in self.second_df.columns]
        
        # Fill missing values
        self.first_df = self.first_df.fillna('')
        self.second_df = self.second_df.fillna('')
        
        logger.info(f"First CSV loaded with {len(self.first_df)} rows")
        logger.info(f"Second CSV loaded with {len(self.second_df)} rows")
        
        # Return a tuple of both dataframes
        return self.first_df, self.second_df

# Semantic Mapper with LangGraph integration
class SemanticMapper:
    def __init__(self, config_file: str, creds_file: str, cert_file: str, persist_dir: str):
        """Initialize the semantic mapper with required components."""
        # Setup environment
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.persist_dir = persist_dir
        
        # Create directory if it doesn't exist
        os.makedirs(self.persist_dir, exist_ok=True)
        
        # Initialize embedding client
        self.embedding_client = EmbeddingClient(
            azure_api_version=self.env.get("AZURE_API_VERSION", "2023-05-15"),
            embeddings_model=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
        )
        
        # Initialize ChromaDB client
        self.chroma_client = chromadb.PersistentClient(
            path=self.persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Initialize LLM for confidence scoring
        self.chatbot = AzureChatbot(config_file, creds_file, cert_file)
        
        # Collection names
        self.first_collection_name = "first_csv_data"
        self.second_collection_name = "second_csv_data"
        
        # Create LangGraph for the matching workflow
        self.graph = self._create_agent_graph()
        
        # Confidence threshold
        self.confidence_threshold = 0.75
    
    def _create_collection(self, name: str):
        """Create or get a collection."""
        try:
            # Try to get the collection
            collection = self.chroma_client.get_collection(name=name)
            logger.info(f"Retrieved existing collection: {name}")
        except Exception:
            # Create if it doesn't exist
            collection = self.chroma_client.create_collection(name=name)
            logger.info(f"Created new collection: {name}")
        
        return collection
    
    def _create_document_with_embedding(self, doc_id: str, text: str, metadata: Dict[str, Any]) -> Tuple[str, List[float], Dict[str, Any]]:
        """Create a document and generate its embedding using the EmbeddingClient."""
        # Create MyDocument instance
        doc = MyDocument(id=doc_id, text=text, metadata=metadata)
        
        # Generate embedding
        embedded_doc = self.embedding_client.generate_embeddings(doc)
        
        return doc_id, embedded_doc.embedding, metadata
    
    def _prepare_first_csv_documents(self, df):
        """Prepare documents from the first CSV with embeddings."""
        docs = []
        embeddings = []
        ids = []
        metadatas = []
        
        logger.info("Generating embeddings for first CSV data...")
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing first CSV"):
            # Create document text
            doc_id = f"first_{idx}"
            text = f"Name: {row['name']}\nDefinition: {row['definition']}\nOwned by: {row['owned by']}"
            
            # Create metadata
            metadata = {
                "source": "first_csv",
                "name": row['name'],
                "definition": row['definition'],
                "owned_by": row['owned by'],
                "original_index": int(idx)
            }
            
            # Generate embedding
            doc_id, embedding, metadata = self._create_document_with_embedding(doc_id, text, metadata)
            
            # Store the results
            docs.append(text)
            embeddings.append(embedding)
            ids.append(doc_id)
            metadatas.append(metadata)
        
        return ids, docs, embeddings, metadatas
    
    def _prepare_second_csv_documents(self, df):
        """Prepare documents from the second CSV with embeddings."""
        docs = []
        embeddings = []
        ids = []
        metadatas = []
        
        logger.info("Generating embeddings for second CSV data...")
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing second CSV"):
            # Create document text
            doc_id = f"second_{idx}"
            text = f"Name: {row['name']}\nCode: {row['code']}\n"
            if 'taxonomy path 1' in row:
                text += f"Taxonomy Path 1: {row['taxonomy path 1']}\n"
            if 'taxonomy path 2' in row:
                text += f"Taxonomy Path 2: {row['taxonomy path 2']}\n"
            if 'parent' in row:
                text += f"Parent: {row['parent']}\n"
            if 'description' in row:
                text += f"Description: {row['description']}\n"
            if 'record examples' in row:
                text += f"Record Examples: {row['record examples']}"
            
            # Create metadata
            metadata = {
                "source": "second_csv",
                "name": row['name'],
                "code": row['code'],
                "original_index": int(idx)
            }
            
            # Add optional fields to metadata
            for field in ['taxonomy path 1', 'taxonomy path 2', 'parent', 'description', 'record examples']:
                if field in row:
                    metadata[field.replace(' ', '_')] = row[field]
            
            # Generate embedding
            doc_id, embedding, metadata = self._create_document_with_embedding(doc_id, text, metadata)
            
            # Store the results
            docs.append(text)
            embeddings.append(embedding)
            ids.append(doc_id)
            metadatas.append(metadata)
        
        return ids, docs, embeddings, metadatas
    
    def index_data(self, first_df, second_df):
        """Index data from both dataframes into ChromaDB."""
        # Get or create collections
        first_collection = self._create_collection(self.first_collection_name)
        second_collection = self._create_collection(self.second_collection_name)
        
        # Process first CSV
        ids1, docs1, embeddings1, metadatas1 = self._prepare_first_csv_documents(first_df)
        
        # Process second CSV
        ids2, docs2, embeddings2, metadatas2 = self._prepare_second_csv_documents(second_df)
        
        # Add documents to collections in batches
        batch_size = 100
        
        # Add first CSV documents
        for i in range(0, len(ids1), batch_size):
            end = min(i + batch_size, len(ids1))
            first_collection.add(
                ids=ids1[i:end],
                documents=docs1[i:end],
                embeddings=embeddings1[i:end],
                metadatas=metadatas1[i:end]
            )
        
        # Add second CSV documents
        for i in range(0, len(ids2), batch_size):
            end = min(i + batch_size, len(ids2))
            second_collection.add(
                ids=ids2[i:end],
                documents=docs2[i:end],
                embeddings=embeddings2[i:end],
                metadatas=metadatas2[i:end]
            )
        
        logger.info(f"Indexed {len(ids1)} documents from first CSV")
        logger.info(f"Indexed {len(ids2)} documents from second CSV")
    
    def find_match(self, name: str, context: str = "", n_results: int = 3) -> Dict:
        """Find matching names in the first CSV collection."""
        # Create query document
        query_text = f"Name: {name}"
        if context:
            query_text += f"\nContext: {context}"
        
        # Generate embedding for query
        query_doc = MyDocument(id="query", text=query_text)
        embedded_query = self.embedding_client.generate_embeddings(query_doc)
        query_embedding = embedded_query.embedding
        
        # Query the collection
        first_collection = self.chroma_client.get_collection(self.first_collection_name)
        results = first_collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        return results
    
    def _create_agent_graph(self):
        """Create the LangGraph for the name matching workflow."""
        try:
            # Define state type for the graph
            class AgentState(dict):
                """State for the name matching agent graph."""
                pass
            
            # Define the nodes in our graph
            def evaluate_match_quality(state):
                """Evaluate the quality of the best match and assign a confidence score."""
                source_name = state["source_name"]
                match_results = state["match_results"]
                source_context = state["source_context"]
                
                if not match_results["ids"] or len(match_results["ids"]) == 0:
                    return {
                        "best_match": None,
                        "confidence_score": 0.0,
                        "suggestion": None
                    }
                
                # Get best match
                best_match = {
                    "id": match_results["ids"][0],
                    "name": match_results["metadatas"][0]["name"],
                    "definition": match_results["metadatas"][0]["definition"],
                    "owned_by": match_results["metadatas"][0]["owned_by"],
                    "distance": match_results["distances"][0] if "distances" in match_results else None,
                }
                
                # Create prompt for LLM
                prompt = f"""
                Evaluate the quality of this semantic name match and provide a confidence score between 0 and 1:
                
                Source name: {source_name}
                Target match: {best_match['name']}
                
                Source context: {source_context}
                Target definition: {best_match['definition']}
                
                Based on semantic meaning, how confident are you that these names refer to the same concept?
                Provide your response as a JSON with the following structure:
                {{
                    "confidence_score": [score between 0 and 1],
                    "reasoning": "[your detailed reasoning]"
                }}
                """
                
                # Get evaluation from LLM
                response = self.chatbot.conversation.run(prompt)
                
                # Parse the JSON response
                try:
                    # Extract JSON from the response
                    json_str = response.strip()
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    evaluation = json.loads(json_str)
                    confidence_score = float(evaluation["confidence_score"])
                    
                    return {
                        "best_match": best_match,
                        "confidence_score": confidence_score,
                        "reasoning": evaluation.get("reasoning", "")
                    }
                except Exception as e:
                    logger.error(f"Error parsing LLM response: {e}")
                    # Default to using similarity score if parsing fails
                    confidence_score = 1 - min(1, best_match.get("distance", 0.5) / 2)
                    return {
                        "best_match": best_match,
                        "confidence_score": confidence_score,
                        "reasoning": f"Failed to parse LLM response, using similarity score instead: {confidence_score}"
                    }
            
            def suggest_better_name(state):
                """Suggest a better name if the match quality is poor."""
                confidence_score = state["confidence_score"]
                source_name = state["source_name"]
                best_match = state["best_match"]
                source_context = state["source_context"]
                
                if confidence_score >= self.confidence_threshold or not best_match:
                    # No need for suggestion if confidence is high enough
                    return {"suggestion": None}
                
                # Create a prompt for the LLM to suggest a better name
                prompt = f"""
                The current match for "{source_name}" is "{best_match['name']}" with a confidence score of {confidence_score}.
                This score is below the threshold of {self.confidence_threshold}.
                
                Source context: {source_context}
                Target definition: {best_match['definition']}
                
                Please suggest a better name from the first dataset that would be a more accurate match.
                If you cannot think of a better name, suggest how the current names could be standardized or harmonized.
                
                Provide your response as a JSON with the following structure:
                {{
                    "suggestion": "[your suggested name or standardization approach]",
                    "explanation": "[your detailed explanation]"
                }}
                """
                
                # Get suggestion from LLM
                response = self.chatbot.conversation.run(prompt)
                
                # Parse the JSON response
                try:
                    # Extract JSON from the response
                    json_str = response.strip()
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    suggestion_data = json.loads(json_str)
                    return {
                        "suggestion": suggestion_data["suggestion"],
                        "explanation": suggestion_data.get("explanation", "")
                    }
                except Exception as e:
                    logger.error(f"Error parsing LLM suggestion: {e}")
                    return {
                        "suggestion": "Error generating suggestion",
                        "explanation": f"Failed to parse LLM response: {str(e)}"
                    }
            
            def router(state):
                """Route to the next node based on confidence score."""
                confidence_score = state.get("confidence_score")
                
                if confidence_score is None:
                    return "evaluate_match"
                elif confidence_score < self.confidence_threshold:
                    return "suggest_better_name"
                else:
                    return END
            
            # Create the graph
            workflow = StateGraph(AgentState)
            
            # Add nodes
            workflow.add_node("evaluate_match", evaluate_match_quality)
            workflow.add_node("suggest_better_name", suggest_better_name)
            
            # Set the entry point
            workflow.set_entry_point("evaluate_match")
            
            # Add edges
            workflow.add_conditional_edges("evaluate_match", router)
            workflow.add_edge("suggest_better_name", END)
            
            # Compile the graph
            return workflow.compile()
        
        except Exception as e:
            logger.error(f"Error creating agent graph: {e}")
            raise
    
    def evaluate_with_agent(self, source_name: str, source_context: str, match_results: Dict) -> Dict:
        """Use the LangGraph agent to evaluate match quality and provide suggestions."""
        try:
            # Initialize the state
            initial_state = {
                "source_name": source_name,
                "source_context": source_context,
                "match_results": match_results,
                "best_match": None,
                "confidence_score": None,
                "suggestion": None
            }
            
            # Run the graph
            result = self.graph.invoke(initial_state)
            
            return {
                "source_name": source_name,
                "best_match": result["best_match"],
                "confidence_score": result["confidence_score"],
                "suggestion": result.get("suggestion"),
                "explanation": result.get("explanation", ""),
                "reasoning": result.get("reasoning", "")
            }
        except Exception as e:
            logger.error(f"Error evaluating match for {source_name}: {e}")
            raise
    
    def process_mappings(self, second_df):
        """Process all rows from the second CSV and find mappings using LangGraph agent."""
        results = []
        
        for idx, row in tqdm(second_df.iterrows(), total=len(second_df), desc="Mapping names"):
            try:
                # Get data from row
                name = row['name']
                code = row['code'] if 'code' in row else ''
                
                # Build context string
                context_parts = []
                for field in ['taxonomy path 1', 'taxonomy path 2', 'parent', 'description', 'record examples']:
                    if field in row and row[field]:
                        context_parts.append(f"{field.capitalize()}: {row[field]}")
                
                context = "; ".join(context_parts)
                
                # Find matches
                match_results = self.find_match(name, context)
                
                # Evaluate match quality using LangGraph agent
                evaluation = self.evaluate_with_agent(name, context, match_results)
                
                # Create result entry
                result = {
                    "code": code,
                    "name": name,
                    "matched_name": evaluation["best_match"]["name"] if evaluation["best_match"] else None,
                    "matched_owned_by": evaluation["best_match"]["owned_by"] if evaluation["best_match"] else None,
                    "matched_definition": evaluation["best_match"]["definition"] if evaluation["best_match"] else None,
                    "confidence_score": evaluation["confidence_score"],
                    "suggestion": evaluation.get("suggestion")
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing row {idx}: {e}")
                results.append({
                    "code": row.get('code', ''),
                    "name": row.get('name', ''),
                    "matched_name": None,
                    "matched_owned_by": None, 
                    "matched_definition": None,
                    "confidence_score": 0.0,
                    "suggestion": f"Error: {str(e)}"
                })
        
        return results

# Main function to run the mapping
def run_mapping(first_csv: str, second_csv: str, config: str, creds: str, cert: str, persist_dir: str, output: str = None):
    """Run the semantic name mapping with LangGraph agent."""
    # Create CSV processor and load data
    csv_processor = CSVProcessor(first_csv, second_csv)
    first_df, second_df = csv_processor.load_data()
    
    # Create semantic mapper with LangGraph
    mapper = SemanticMapper(config, creds, cert, persist_dir)
    
    # Index the data
    mapper.index_data(first_df, second_df)
    
    # Process all mappings using LangGraph agent
    results = mapper.process_mappings(second_df)
    
    # Create DataFrame and save results
    results_df = pd.DataFrame(results)
    
    # Order columns
    column_order = ['code', 'name', 'matched_name', 'matched_owned_by', 'matched_definition', 'confidence_score', 'suggestion']
    results_df = results_df[column_order]
    
    # Save to CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output or f"name_mapping_results_{timestamp}.csv"
    results_df.to_csv(output_file, index=False)
    
    # Print summary
    good_matches = results_df[results_df['confidence_score'] >= 0.75]
    logger.info(f"Completed semantic name mapping with {len(results_df)} names")
    logger.info(f"Good matches: {len(good_matches)} ({len(good_matches)/len(results_df)*100:.2f}%)")
    logger.info(f"Results saved to {output_file}")
    
    return results_df

# Command-line interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Semantic name mapping between two CSV files")
    parser.add_argument("--first-csv", required=True, help="Path to the first CSV file (name, definition, owned by)")
    parser.add_argument("--second-csv", required=True, help="Path to the second CSV file with taxonomy data")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to the config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to the credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to the certificate file")
    parser.add_argument("--persist-dir", default="./chroma_db", help="Directory to persist vector database")
    parser.add_argument("--output", default=None, help="Path to save output CSV file")
    
    args = parser.parse_args()
    
    run_mapping(
        first_csv=args.first_csv,
        second_csv=args.second_csv,
        config=args.config,
        creds=args.creds,
        cert=args.cert,
        persist_dir=args.persist_dir,
        output=args.output
    )
