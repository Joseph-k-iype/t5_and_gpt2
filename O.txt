import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.subplots as sp
from scipy import stats
import os

def analyze_data_quality(df):
    """
    Perform statistical analysis on data quality issues in the incident dataframe
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing incident data with columns:
        incident_summary, incident_description, incident_resolution, 
        Id, is_data_issue, data_quality_dimension, confidence_score
    
    Returns:
    --------
    results : dict
        Dictionary of analysis results
    """
    results = {}
    
    # Basic counts and percentages
    total_incidents = len(df)
    data_issues = df[df['is_data_issue'] == True].shape[0]
    non_data_issues = total_incidents - data_issues
    data_issue_pct = (data_issues / total_incidents) * 100 if total_incidents > 0 else 0
    
    results['counts'] = {
        'total_incidents': total_incidents,
        'data_issues': data_issues,
        'non_data_issues': non_data_issues,
        'data_issue_percentage': data_issue_pct
    }
    
    # Data quality dimension analysis
    if data_issues > 0:
        # Get dimension distribution
        dimension_counts = df[df['is_data_issue'] == True]['data_quality_dimension'].value_counts()
        dimension_pct = dimension_counts / data_issues * 100
        
        results['dimensions'] = {
            'counts': dimension_counts.to_dict(),
            'percentages': dimension_pct.to_dict()
        }
        
        # Check for missing dimension values for data issues
        missing_dimensions = df[(df['is_data_issue'] == True) & (df['data_quality_dimension'].isnull())].shape[0]
        results['dimensions']['missing_count'] = missing_dimensions
        results['dimensions']['missing_percentage'] = (missing_dimensions / data_issues * 100) if data_issues > 0 else 0
    
    # Confidence score analysis
    confidence_stats = {
        'overall': {
            'mean': df['confidence_score'].mean(),
            'median': df['confidence_score'].median(),
            'std': df['confidence_score'].std(),
            'min': df['confidence_score'].min(),
            'max': df['confidence_score'].max(),
            'count': df['confidence_score'].count()
        }
    }
    
    if data_issues > 0:
        confidence_stats['data_issues'] = {
            'mean': df[df['is_data_issue'] == True]['confidence_score'].mean(),
            'median': df[df['is_data_issue'] == True]['confidence_score'].median(),
            'std': df[df['is_data_issue'] == True]['confidence_score'].std(),
            'min': df[df['is_data_issue'] == True]['confidence_score'].min(),
            'max': df[df['is_data_issue'] == True]['confidence_score'].max(),
            'count': df[df['is_data_issue'] == True]['confidence_score'].count()
        }
    
    if non_data_issues > 0:
        confidence_stats['non_data_issues'] = {
            'mean': df[df['is_data_issue'] == False]['confidence_score'].mean(),
            'median': df[df['is_data_issue'] == False]['confidence_score'].median(),
            'std': df[df['is_data_issue'] == False]['confidence_score'].std(),
            'min': df[df['is_data_issue'] == False]['confidence_score'].min(),
            'max': df[df['is_data_issue'] == False]['confidence_score'].max(),
            'count': df[df['is_data_issue'] == False]['confidence_score'].count()
        }
    
    results['confidence'] = confidence_stats
    
    # Statistical significance test for difference in confidence scores
    if data_issues > 0 and non_data_issues > 0:
        data_issue_scores = df[df['is_data_issue'] == True]['confidence_score']
        non_data_issue_scores = df[df['is_data_issue'] == False]['confidence_score']
        
        if len(data_issue_scores) > 1 and len(non_data_issue_scores) > 1:
            t_stat, p_value = stats.ttest_ind(
                data_issue_scores,
                non_data_issue_scores,
                equal_var=False  # Welch's t-test (doesn't assume equal variances)
            )
            
            results['confidence']['t_test'] = {
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
    
    # Confidence scores by data quality dimension
    if data_issues > 0:
        dimension_confidence = df[df['is_data_issue'] == True].groupby('data_quality_dimension')['confidence_score'].agg(['mean', 'median', 'std', 'count']).to_dict()
        results['dimension_confidence'] = dimension_confidence
    
    # Correlation analysis
    if data_issues > 0:
        # Create a copy of dataframe for correlation analysis
        df_corr = df.copy()
        df_corr['is_data_issue_num'] = df_corr['is_data_issue'].astype(int)
        
        # One-hot encode data quality dimensions if there are multiple dimensions
        unique_dimensions = df[df['is_data_issue'] == True]['data_quality_dimension'].dropna().unique()
        if len(unique_dimensions) > 1:
            dummies = pd.get_dummies(df_corr['data_quality_dimension'], prefix='dim')
            df_corr = pd.concat([df_corr, dummies], axis=1)
        
        # Calculate correlations with confidence score
        corr_columns = ['confidence_score', 'is_data_issue_num'] + [col for col in df_corr.columns if col.startswith('dim_')]
        corr_matrix = df_corr[corr_columns].corr()
        
        # Store correlation with confidence score
        results['correlations'] = corr_matrix['confidence_score'].to_dict()
    
    return results

def create_visualizations(df, results, output_dir=None):
    """
    Create visualizations using Plotly based on the statistical analysis
    
    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame containing incident data
    results : dict
        Dictionary of analysis results from analyze_data_quality function
    output_dir : str, optional
        Directory to save the visualizations as HTML files
    
    Returns:
    --------
    figures : dict
        Dictionary of Plotly figures
    """
    figures = {}
    
    # Create output directory if specified
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    # 1. Pie chart for data issues vs. non-data issues
    labels = ['Data Issues', 'Non-Data Issues']
    values = [results['counts']['data_issues'], results['counts']['non_data_issues']]
    
    fig_pie = px.pie(
        values=values,
        names=labels,
        title='Distribution of Incidents',
        color_discrete_sequence=px.colors.qualitative.Set3,
        hole=0.4
    )
    fig_pie.update_traces(textposition='inside', textinfo='percent+label')
    figures['issue_distribution'] = fig_pie
    
    if output_dir:
        fig_pie.write_html(os.path.join(output_dir, 'issue_distribution.html'))
    
    # 2. Bar chart for data quality dimensions (if data issues exist)
    if 'dimensions' in results and results['counts']['data_issues'] > 0:
        dimension_df = pd.DataFrame({
            'Dimension': list(results['dimensions']['counts'].keys()),
            'Count': list(results['dimensions']['counts'].values()),
            'Percentage': list(results['dimensions']['percentages'].values())
        })
        
        if not dimension_df.empty:
            fig_bar = px.bar(
                dimension_df,
                x='Dimension',
                y='Count',
                color='Dimension',
                title='Data Quality Dimension Distribution',
                text='Percentage',
                labels={'Count': 'Number of Incidents', 'Dimension': 'Data Quality Dimension'},
                color_discrete_sequence=px.colors.qualitative.Plotly
            )
            fig_bar.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            figures['dimension_distribution'] = fig_bar
            
            if output_dir:
                fig_bar.write_html(os.path.join(output_dir, 'dimension_distribution.html'))
    
    # 3. Box plot for confidence scores comparison
    if results['counts']['data_issues'] > 0 and results['counts']['non_data_issues'] > 0:
        # Prepare data for box plot
        box_data = []
        
        # Overall confidence
        box_data.append({
            'group': 'Overall',
            'confidence': df['confidence_score'].tolist()
        })
        
        # Data issues confidence
        box_data.append({
            'group': 'Data Issues',
            'confidence': df[df['is_data_issue'] == True]['confidence_score'].tolist()
        })
        
        # Non-data issues confidence
        box_data.append({
            'group': 'Non-Data Issues',
            'confidence': df[df['is_data_issue'] == False]['confidence_score'].tolist()
        })
        
        # Convert to long format for Plotly
        box_df = pd.DataFrame()
        for item in box_data:
            temp_df = pd.DataFrame({
                'Group': [item['group']] * len(item['confidence']),
                'Confidence Score': item['confidence']
            })
            box_df = pd.concat([box_df, temp_df])
        
        fig_box = px.box(
            box_df, 
            x='Group', 
            y='Confidence Score',
            color='Group',
            title='Confidence Score Comparison',
            points='all',
            color_discrete_sequence=px.colors.qualitative.G10
        )
        figures['confidence_comparison'] = fig_box
        
        if output_dir:
            fig_box.write_html(os.path.join(output_dir, 'confidence_comparison.html'))
    
    # 4. Bar chart for confidence by data quality dimension
    if 'dimension_confidence' in results and results['counts']['data_issues'] > 0:
        means = results['dimension_confidence']['mean']
        counts = results['dimension_confidence']['count']
        stds = results['dimension_confidence']['std']
        
        # Convert to DataFrame
        dimension_conf_df = pd.DataFrame({
            'Dimension': list(means.keys()),
            'Mean Confidence': list(means.values()),
            'Count': list(counts.values()),
            'Std': list(stds.values())
        })
        
        if not dimension_conf_df.empty:
            fig_dim_conf = px.bar(
                dimension_conf_df,
                x='Dimension',
                y='Mean Confidence',
                error_y='Std',
                color='Dimension',
                title='Average Confidence Score by Data Quality Dimension',
                labels={
                    'Mean Confidence': 'Average Confidence Score',
                    'Dimension': 'Data Quality Dimension'
                },
                text='Count',
                color_discrete_sequence=px.colors.qualitative.Dark24
            )
            fig_dim_conf.update_traces(texttemplate='n=%{text}', textposition='outside')
            figures['dimension_confidence'] = fig_dim_conf
            
            if output_dir:
                fig_dim_conf.write_html(os.path.join(output_dir, 'dimension_confidence.html'))
    
    # 5. Correlation heatmap
    if 'correlations' in results:
        # Extract relevant correlations
        corr_data = results['correlations']
        # Filter out self-correlation
        corr_data = {k: v for k, v in corr_data.items() if k != 'confidence_score'}
        
        corr_df = pd.DataFrame({
            'Factor': list(corr_data.keys()),
            'Correlation': list(corr_data.values())
        })
        
        # Sort by absolute correlation value
        corr_df['Abs_Correlation'] = corr_df['Correlation'].abs()
        corr_df = corr_df.sort_values('Abs_Correlation', ascending=False).drop('Abs_Correlation', axis=1)
        
        if not corr_df.empty:
            fig_corr = px.bar(
                corr_df,
                x='Factor',
                y='Correlation',
                title='Correlation with Confidence Score',
                labels={'Factor': 'Factor', 'Correlation': 'Correlation Coefficient'},
                color='Correlation',
                color_continuous_scale='RdBu_r',
                range_color=[-1, 1]
            )
            figures['correlations'] = fig_corr
            
            if output_dir:
                fig_corr.write_html(os.path.join(output_dir, 'correlations.html'))
    
    # 6. Summary dashboard with multiple charts
    if len(figures) > 1:
        # Create subplots
        num_figs = len(figures)
        rows = (num_figs + 1) // 2  # Ceiling division
        
        fig_dashboard = sp.make_subplots(
            rows=rows,
            cols=2,
            subplot_titles=list(figures.keys()),
            specs=[[{'type': 'domain'}, {'type': 'xy'}]] + [[{'type': 'xy'}, {'type': 'xy'}]] * (rows - 1),
            vertical_spacing=0.1
        )
        
        # Add traces from individual figures to dashboard
        row, col = 1, 1
        for name, fig in figures.items():
            if name == 'issue_distribution':
                for trace in fig.data:
                    fig_dashboard.add_trace(trace, row=row, col=col)
                col += 1
            else:
                for trace in fig.data:
                    fig_dashboard.add_trace(trace, row=row, col=col)
                col += 1
                if col > 2:
                    col = 1
                    row += 1
        
        fig_dashboard.update_layout(
            title_text='Data Quality Analysis Dashboard',
            height=300 * rows,
            width=1000,
            showlegend=False
        )
        figures['dashboard'] = fig_dashboard
        
        if output_dir:
            fig_dashboard.write_html(os.path.join(output_dir, 'dashboard.html'))
    
    return figures

def main(file_path, output_dir=None):
    """
    Main function to run the entire analysis
    
    Parameters:
    -----------
    file_path : str
        Path to the CSV file containing the incident data
    output_dir : str, optional
        Directory to save the visualizations
    """
    # Load data
    df = pd.read_csv(file_path)
    
    # Print basic information
    print(f"Dataset shape: {df.shape}")
    print(f"Columns: {', '.join(df.columns)}")
    print(f"Missing values:\n{df.isnull().sum()}")
    
    # Run analysis
    results = analyze_data_quality(df)
    
    # Create and save visualizations
    figures = create_visualizations(df, results, output_dir)
    
    # Print key statistics
    print("\n----- STATISTICAL ANALYSIS RESULTS -----")
    
    # Basic counts
    print(f"\nTotal incidents: {results['counts']['total_incidents']}")
    print(f"Data issues: {results['counts']['data_issues']} ({results['counts']['data_issue_percentage']:.2f}%)")
    print(f"Non-data issues: {results['counts']['non_data_issues']} ({100 - results['counts']['data_issue_percentage']:.2f}%)")
    
    # Data quality dimensions
    if 'dimensions' in results and results['counts']['data_issues'] > 0:
        print("\nData Quality Dimension Distribution:")
        for dim, count in results['dimensions']['counts'].items():
            print(f"  {dim}: {count} ({results['dimensions']['percentages'][dim]:.2f}%)")
        
        if results['dimensions']['missing_count'] > 0:
            print(f"  Missing dimensions: {results['dimensions']['missing_count']} ({results['dimensions']['missing_percentage']:.2f}%)")
    
    # Confidence scores
    print("\nConfidence Score Statistics:")
    print(f"  Overall: mean={results['confidence']['overall']['mean']:.4f}, median={results['confidence']['overall']['median']:.4f}, std={results['confidence']['overall']['std']:.4f}")
    
    if 'data_issues' in results['confidence']:
        print(f"  Data issues: mean={results['confidence']['data_issues']['mean']:.4f}, median={results['confidence']['data_issues']['median']:.4f}, std={results['confidence']['data_issues']['std']:.4f}")
    
    if 'non_data_issues' in results['confidence']:
        print(f"  Non-data issues: mean={results['confidence']['non_data_issues']['mean']:.4f}, median={results['confidence']['non_data_issues']['median']:.4f}, std={results['confidence']['non_data_issues']['std']:.4f}")
    
    # T-test results
    if 't_test' in results['confidence']:
        print(f"\nT-test for difference in confidence scores:")
        print(f"  t-statistic: {results['confidence']['t_test']['t_statistic']:.4f}")
        print(f"  p-value: {results['confidence']['t_test']['p_value']:.4f}")
        print(f"  Statistically significant difference: {results['confidence']['t_test']['significant']}")
    
    # Dimension confidence
    if 'dimension_confidence' in results:
        print("\nConfidence by Data Quality Dimension:")
        for dim in results['dimension_confidence']['mean'].keys():
            mean = results['dimension_confidence']['mean'][dim]
            count = results['dimension_confidence']['count'][dim]
            std = results['dimension_confidence']['std'][dim]
            print(f"  {dim}: mean={mean:.4f}, count={count}, std={std:.4f}")
    
    # Correlations
    if 'correlations' in results:
        print("\nCorrelations with Confidence Score:")
        for factor, corr in sorted(
            [(k, v) for k, v in results['correlations'].items() if k != 'confidence_score'],
            key=lambda x: abs(x[1]),
            reverse=True
        ):
            print(f"  {factor}: {corr:.4f}")
    
    print("\nAnalysis complete.")
    
    if output_dir:
        print(f"Visualizations saved to: {output_dir}")
    
    return df, results, figures

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Analyze data quality issues in incident data.')
    parser.add_argument('file_path', type=str, help='Path to the CSV file')
    parser.add_argument('--output_dir', type=str, default='visualizations', help='Directory to save visualizations')
    
    args = parser.parse_args()
    
    # Run the analysis
    main(args.file_path, args.output_dir)


# Example usage:
# python data_quality_analysis.py your_incidents_data.csv --output_dir visualizations
