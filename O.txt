import os
import sys
import uuid
import json
import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, TypeVar, Generic, Type, Union, Literal
from pathlib import Path
from dotenv import load_dotenv, dotenv_values
from pydantic import BaseModel, Field, ValidationError, field_validator
import instructor
from instructor.function_calling import OpenAISchema
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import AzureChatOpenAI
from langchain.chains import ConversationChain
from collections import namedtuple
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint import MemorySaver

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Define paths
ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

# Create ENV_DIR if it doesn't exist
os.makedirs(ENV_DIR, exist_ok=True)

# Type variable for generic response model
T = TypeVar('T', bound=BaseModel)

# Utility functions
def is_file_readable(filepath: str) -> bool:
    """Check if a file is readable."""
    return os.path.isfile(filepath) and os.access(filepath, os.R_OK)

def str_to_bool(s: str) -> bool:
    """Convert a string to a boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

class OSEnv:
    """Class to manage environment variables"""
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        
        if is_file_readable(certificate_path):
            self.set_certificate_path(certificate_path)
        
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
        self.credential = self._get_credential()
    
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"), 
                client_id=self.get("AZURE_CLIENT_ID"), 
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False) -> None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            if not is_file_readable(dotenvfile):
                logger.warning(f"Environment file not found or not readable: {dotenvfile}")
                return
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False) -> None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None) -> str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self) -> None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")

class AzureChatbot:
    """Azure OpenAI chatbot with Instructor integration"""
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        self._setup_instructor_client()
    
    def _setup_chat_model(self):
        try:
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider
            )
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise
    
    def _setup_instructor_client(self):
        try:
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            api_version = self.env.get("API_VERSION", "2023-05-15")
            direct_client = AzureOpenAI(
                azure_endpoint=azure_endpoint,
                api_version=api_version,
                azure_ad_token_provider=get_bearer_token_provider(
                    self.env.credential,
                    "https://cognitiveservices.azure.com/.default"
                )
            )
            self.instructor_client = instructor.from_openai(direct_client)
            logger.info("Instructor client set up successfully")
        except Exception as e:
            logger.error(f"Error setting up instructor client: {e}")
            raise
    
    def query(self, user_input: str) -> str:
        try:
            return self.conversation.predict(input=user_input)
        except Exception as e:
            logger.error(f"Error in chat query: {e}")
            return f"Error: {str(e)}"
    
    def structured_query(self, user_input: str, response_model: Type[T]) -> T:
        try:
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            result = self.instructor_client.chat.completions.create(
                model=model_name,
                response_model=response_model,
                messages=[{"role": "user", "content": user_input}]
            )
            self.memory.save_context(
                {"input": user_input}, 
                {"output": f"Extracted structured data: {result.model_dump_json()}"}
            )
            return result
        except Exception as e:
            logger.error(f"Error in structured query: {e}")
            raise

# Define Pydantic models for structured outputs
class IncidentFeatures(OpenAISchema):
    """Extracted relevant features from an incident"""
    id: str
    summary: str
    description: str
    resolution_name: str
    resolution_details: str
    category: str
    subcategory: str
    contains_data_terms: bool = Field(
        description="Whether the incident contains terms related to data issues"
    )
    contains_system_terms: bool = Field(
        description="Whether the incident contains terms related to system issues"
    )
    is_categorized_as_data: bool = Field(
        description="Whether the incident is explicitly categorized as a data-related issue"
    )

class ClassificationReason(OpenAISchema):
    """A single reason supporting the classification with its impact on confidence"""
    reason: str = Field(description="Specific evidence supporting the classification")
    impact: float = Field(description="Impact on confidence (0.0 to 1.0)", ge=0.0, le=1.0)
    
class IncidentClassification(OpenAISchema):
    """The final classification of an incident"""
    incident_id: str
    is_data_issue: bool
    confidence_score: float = Field(description="Confidence score (0.0 to 1.0)", ge=0.0, le=1.0)
    supporting_reasons: List[ClassificationReason] = Field(
        description="Reasons supporting this classification"
    )
    contrary_reasons: List[ClassificationReason] = Field(
        description="Reasons against this classification"
    )
    
class AgentMessage(BaseModel):
    """Message passed between agents"""
    content: str
    metadata: Dict[str, Any] = {}

# Define LangGraph state model using Pydantic
class ClassificationState(BaseModel):
    """State maintained during the classification process"""
    incident: Optional[IncidentFeatures] = None
    feature_extraction_complete: bool = False
    preliminary_classification: Optional[IncidentClassification] = None
    final_classification: Optional[IncidentClassification] = None
    messages: List[AgentMessage] = Field(default_factory=list)
    current_agent: str = "feature_extractor"
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    def to_dict(self) -> dict:
        return self.model_dump()
    
    @classmethod
    def from_dict(cls, data: Union[Dict[str, Any], "ClassificationState"]) -> 'ClassificationState':
        if isinstance(data, cls):
            return data
        return cls(**data)

# Multi-agent system for incident classification
class IncidentClassifier:
    """Multi-agent system for classifying IT incidents as data issues"""
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        try:
            self.chatbot = AzureChatbot(config_file=config_file, creds_file=creds_file, cert_file=cert_file)
        except Exception as e:
            logger.error(f"Error initializing AzureChatbot: {e}")
            raise
        try:
            self.graph = self._build_graph()
        except Exception as e:
            logger.error(f"Error building graph: {e}")
            raise
        
    def _build_graph(self):
        # Use ClassificationState directly as the state schema
        graph = StateGraph(ClassificationState)
        graph.add_node("feature_extractor", self.feature_extractor)
        graph.add_node("classifier", self.classifier)
        graph.add_node("reasoning_agent", self.reasoning_agent)
        graph.add_node("confidence_evaluator", self.confidence_evaluator)
        graph.add_edge(START, "feature_extractor")
        graph.add_edge("feature_extractor", "classifier")
        graph.add_edge("classifier", "reasoning_agent")
        graph.add_edge("reasoning_agent", "confidence_evaluator")
        graph.add_edge("confidence_evaluator", END)
        memory = MemorySaver()
        return graph.compile(checkpointer=memory)
    
    def feature_extractor(self, state: Dict[str, Any]) -> Dict[str, Any]:
        current_state = ClassificationState.from_dict(state)
        if current_state.feature_extraction_complete:
            return current_state.to_dict()
        try:
            metadata = current_state.metadata
            features = self.chatbot.structured_query(
                f"""
                Extract key features from this IT incident:
                ID: {metadata.get("id", "Unknown")}
                Summary: {metadata.get("summary", "")}
                Description: {metadata.get("description", "")}
                Resolution Name: {metadata.get("resolution_name", "")}
                Resolution Details: {metadata.get("resolution_details", "")}
                Category: {metadata.get("category", "")}
                Subcategory: {metadata.get("subcategory", "")}
                Determine if the incident contains data-related or system-related terms.
                """,
                IncidentFeatures
            )
            current_state.incident = features
            current_state.feature_extraction_complete = True
            current_state.current_agent = "classifier"
            current_state.messages.append(
                AgentMessage(
                    content="Successfully extracted features from the incident",
                    metadata={"agent": "feature_extractor", "success": True}
                )
            )
            logger.info(f"Feature extraction complete for incident {metadata.get('id', 'Unknown')}")
        except Exception as e:
            logger.error(f"Error in feature extraction: {e}")
            current_state.messages.append(
                AgentMessage(
                    content=f"Error extracting features: {str(e)}",
                    metadata={"agent": "feature_extractor", "success": False}
                )
            )
        return current_state.to_dict()
    
    def classifier(self, state: Dict[str, Any]) -> Dict[str, Any]:
        current_state = ClassificationState.from_dict(state)
        if current_state.preliminary_classification:
            return current_state.to_dict()
        if not current_state.incident:
            current_state.messages.append(
                AgentMessage(
                    content="Cannot classify without extracted features",
                    metadata={"agent": "classifier", "success": False}
                )
            )
            return current_state.to_dict()
        try:
            is_data_issue = (
                current_state.incident.is_categorized_as_data or 
                current_state.incident.contains_data_terms
            )
            classification = IncidentClassification(
                incident_id=current_state.incident.id,
                is_data_issue=is_data_issue,
                confidence_score=0.7 if is_data_issue else 0.3,
                supporting_reasons=[
                    ClassificationReason(
                        reason=f"Incident {'is' if current_state.incident.is_categorized_as_data else 'is not'} explicitly categorized as data-related",
                        impact=0.6
                    )
                ],
                contrary_reasons=[]
            )
            current_state.preliminary_classification = classification
            current_state.current_agent = "reasoning_agent"
            current_state.messages.append(
                AgentMessage(
                    content=f"Initial classification: {'Data issue' if is_data_issue else 'Not a data issue'}",
                    metadata={"agent": "classifier", "success": True}
                )
            )
            logger.info(f"Initial classification complete for incident {current_state.incident.id}")
        except Exception as e:
            logger.error(f"Error in classification: {e}")
            current_state.messages.append(
                AgentMessage(
                    content=f"Error in classification: {str(e)}",
                    metadata={"agent": "classifier", "success": False}
                )
            )
        return current_state.to_dict()
    
    def reasoning_agent(self, state: Dict[str, Any]) -> Dict[str, Any]:
        current_state = ClassificationState.from_dict(state)
        if not current_state.preliminary_classification or not current_state.incident:
            current_state.messages.append(
                AgentMessage(
                    content="Cannot provide reasoning without preliminary classification and incident data",
                    metadata={"agent": "reasoning_agent", "success": False}
                )
            )
            return current_state.to_dict()
        try:
            improved_classification = self.chatbot.structured_query(
                f"""
                Analyze this IT incident and provide reasoning for its classification:
                ID: {current_state.incident.id}
                Summary: {current_state.incident.summary}
                Description: {current_state.incident.description}
                Resolution Name: {current_state.incident.resolution_name}
                Resolution Details: {current_state.incident.resolution_details}
                Category: {current_state.incident.category}
                Subcategory: {current_state.incident.subcategory}
                Current classification - Is data issue: {current_state.preliminary_classification.is_data_issue}, Confidence: {current_state.preliminary_classification.confidence_score}
                Provide a list of supporting and contrary reasons, each with an impact score (0.0 to 1.0).
                """,
                IncidentClassification
            )
            current_state.final_classification = improved_classification
            current_state.current_agent = "confidence_evaluator"
            current_state.messages.append(
                AgentMessage(
                    content="Developed detailed reasoning for classification",
                    metadata={"agent": "reasoning_agent", "success": True}
                )
            )
            logger.info(f"Reasoning complete for incident {current_state.incident.id}")
        except Exception as e:
            logger.error(f"Error in reasoning: {e}")
            current_state.messages.append(
                AgentMessage(
                    content=f"Error in reasoning: {str(e)}",
                    metadata={"agent": "reasoning_agent", "success": False}
                )
            )
            current_state.final_classification = current_state.preliminary_classification
            current_state.current_agent = "confidence_evaluator"
        return current_state.to_dict()
    
    def confidence_evaluator(self, state: Dict[str, Any]) -> Dict[str, Any]:
        current_state = ClassificationState.from_dict(state)
        if not current_state.final_classification:
            if current_state.preliminary_classification:
                current_state.final_classification = current_state.preliminary_classification
                current_state.messages.append(
                    AgentMessage(
                        content="Using preliminary classification as final classification",
                        metadata={"agent": "confidence_evaluator", "success": True}
                    )
                )
            else:
                incident_id = current_state.metadata.get("id", "Unknown")
                if current_state.incident:
                    incident_id = current_state.incident.id
                current_state.final_classification = IncidentClassification(
                    incident_id=incident_id,
                    is_data_issue=False,
                    confidence_score=0.5,
                    supporting_reasons=[ClassificationReason(reason="Insufficient data for classification", impact=1.0)],
                    contrary_reasons=[]
                )
                current_state.messages.append(
                    AgentMessage(
                        content="Created default classification due to missing data",
                        metadata={"agent": "confidence_evaluator", "success": False}
                    )
                )
        try:
            supporting_impact = sum(reason.impact for reason in current_state.final_classification.supporting_reasons)
            contrary_impact = sum(reason.impact for reason in current_state.final_classification.contrary_reasons)
            total_impact = supporting_impact + contrary_impact
            if total_impact > 0:
                if supporting_impact > contrary_impact:
                    confidence = 0.5 + (0.5 * (supporting_impact - contrary_impact) / total_impact)
                else:
                    confidence = 0.5 - (0.5 * (contrary_impact - supporting_impact) / total_impact)
            else:
                confidence = 0.5
            current_state.final_classification.confidence_score = round(confidence, 2)
            current_state.messages.append(
                AgentMessage(
                    content=f"Final confidence score: {current_state.final_classification.confidence_score}",
                    metadata={"agent": "confidence_evaluator", "success": True}
                )
            )
            incident_id = current_state.metadata.get("id", "Unknown")
            if current_state.incident:
                incident_id = current_state.incident.id
            logger.info(f"Confidence evaluation complete for incident {incident_id}")
        except Exception as e:
            logger.error(f"Error in confidence evaluation: {e}")
            current_state.messages.append(
                AgentMessage(
                    content=f"Error in confidence evaluation: {str(e)}",
                    metadata={"agent": "confidence_evaluator", "success": False}
                )
            )
        assert current_state.final_classification is not None, "Final classification is still None"
        return current_state.to_dict()
    
    def classify_incident(self, incident_data: Dict[str, str]) -> IncidentClassification:
        initial_state = ClassificationState(
            metadata={
                "id": incident_data.get("Id", "Unknown"),
                "summary": incident_data.get("IT_INCIDENT_SUMMARY", ""),
                "description": incident_data.get("IT_INCIDENT_DESC", ""),
                "resolution_name": incident_data.get("IT_INCIDENT_RESOLUTION_DETAILS_NAME", ""),
                "resolution_details": incident_data.get("IT_INCIDENT_RESOLUTION_DESC", ""),
                "category": incident_data.get("IT_INCIDENT_AREA_CATEGORY", ""),
                "subcategory": incident_data.get("IT_INCIDENT_AREA_SUBCATEGORY", "")
            }
        )
        try:
            initial_dict = initial_state.to_dict()
            logger.info(f"Running classification for incident {incident_data.get('Id', 'Unknown')}")
            # Provide a default configuration with a 'thread_id'
            config = {"configurable": {"thread_id": "default_thread"}}
            final_state_dict = self.graph.invoke(initial_dict, config=config)
            final_state = ClassificationState.from_dict(final_state_dict)
            if final_state.final_classification is not None:
                return final_state.final_classification
            else:
                logger.error("Classification graph did not produce a final classification")
                raise ValueError("Classification process did not produce a final classification")
        except Exception as e:
            logger.error(f"Error running classification graph: {e}")
            raise
    
    def process_csv(self, csv_path: str, output_path: str = None) -> List[IncidentClassification]:
        try:
            df = pd.read_csv(csv_path)
            logger.info(f"Loaded {len(df)} incidents from {csv_path}")
            results = []
            for i, row in df.iterrows():
                incident_id = row.get('Id', f"Unknown-{i}")
                logger.info(f"Processing incident {i+1}/{len(df)}: {incident_id}")
                try:
                    incident_data = row.to_dict()
                    classification = self.classify_incident(incident_data)
                    if not isinstance(classification, IncidentClassification):
                        logger.error(f"Invalid classification type: {type(classification)}")
                        continue
                    results.append(classification)
                    logger.info(
                        f"Successfully classified incident {classification.incident_id}: "
                        f"{'Data issue' if classification.is_data_issue else 'Not a data issue'} "
                        f"(Confidence: {classification.confidence_score})"
                    )
                except Exception as e:
                    logger.error(f"Error processing incident {incident_id}: {e}")
                    continue
            if output_path and results:
                with open(output_path, 'w') as f:
                    json.dump([r.model_dump() for r in results], f, indent=2)
                logger.info(f"Saved {len(results)} results to {output_path}")
            return results
        except Exception as e:
            logger.error(f"Error processing CSV: {e}")
            raise

def create_sample_data(csv_path: str):
    sample_data = [
        {
            "Id": "INC001",
            "IT_INCIDENT_SUMMARY": "Database query timeout affecting customer reports",
            "IT_INCIDENT_DESC": "Users reported that customer reports are taking more than 30 minutes to generate.",
            "IT_INCIDENT_RESOLUTION_DETAILS_NAME": "Database Optimization",
            "IT_INCIDENT_RESOLUTION_DESC": "Optimized the slow query.",
            "IT_INCIDENT_AREA_CATEGORY": "Database",
            "IT_INCIDENT_AREA_SUBCATEGORY": "Query Performance"
        },
        {
            "Id": "INC002",
            "IT_INCIDENT_SUMMARY": "Network connectivity issues in Building B",
            "IT_INCIDENT_DESC": "Employees in Building B experience intermittent network issues.",
            "IT_INCIDENT_RESOLUTION_DETAILS_NAME": "Router Replacement",
            "IT_INCIDENT_RESOLUTION_DESC": "Replaced faulty router.",
            "IT_INCIDENT_AREA_CATEGORY": "Network",
            "IT_INCIDENT_AREA_SUBCATEGORY": "Hardware Failure"
        },
        {
            "Id": "INC003",
            "IT_INCIDENT_SUMMARY": "Missing customer records in monthly report",
            "IT_INCIDENT_DESC": "The monthly report is missing 15% of records.",
            "IT_INCIDENT_RESOLUTION_DETAILS_NAME": "Data Correction",
            "IT_INCIDENT_RESOLUTION_DESC": "Fixed data filtering issue.",
            "IT_INCIDENT_AREA_CATEGORY": "Business Process or Usage",
            "IT_INCIDENT_AREA_SUBCATEGORY": "Data Integrity"
        },
        {
            "Id": "INC004",
            "IT_INCIDENT_SUMMARY": "Application crashing during end-of-month processing",
            "IT_INCIDENT_DESC": "Application crashes with memory errors during processing.",
            "IT_INCIDENT_RESOLUTION_DETAILS_NAME": "Memory Allocation",
            "IT_INCIDENT_RESOLUTION_DESC": "Increased memory allocation and optimized algorithms.",
            "IT_INCIDENT_AREA_CATEGORY": "Application",
            "IT_INCIDENT_AREA_SUBCATEGORY": "Performance"
        },
        {
            "Id": "INC005",
            "IT_INCIDENT_SUMMARY": "Duplicate transactions in financial database",
            "IT_INCIDENT_DESC": "Duplicate transaction entries causing financial discrepancies.",
            "IT_INCIDENT_RESOLUTION_DETAILS_NAME": "Transaction Deduplication",
            "IT_INCIDENT_RESOLUTION_DESC": "Implemented idempotency checks.",
            "IT_INCIDENT_AREA_CATEGORY": "Database",
            "IT_INCIDENT_AREA_SUBCATEGORY": "Data Consistency"
        }
    ]
    df = pd.DataFrame(sample_data)
    df.to_csv(csv_path, index=False)
    logger.info(f"Created sample data with {len(df)} incidents")
    return df

def create_config_files():
    if not os.path.exists(CONFIG_PATH):
        with open(CONFIG_PATH, 'w') as f:
            f.write("""MODEL_NAME=gpt-4o-mini
TEMPERATURE=0.1
MAX_TOKENS=1000
API_VERSION=2023-05-15
AZURE_ENDPOINT=https://your-azure-endpoint.openai.azure.com/
""")
        logger.info(f"Created sample config file at {CONFIG_PATH}")
    if not os.path.exists(CREDS_PATH):
        with open(CREDS_PATH, 'w') as f:
            f.write("""AZURE_TENANT_ID=your-tenant-id
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret
USE_MANAGED_IDENTITY=False
SECURED_ENDPOINTS=True
""")
        logger.info(f"Created sample credentials file at {CREDS_PATH}")

def print_classification_results(results: List[IncidentClassification]):
    if not results:
        print("No classification results to display")
        return
    data_issues = [r for r in results if r.is_data_issue]
    high_confidence = [r for r in results if r.confidence_score >= 0.8]
    print("\n===== CLASSIFICATION RESULTS =====")
    print(f"Total incidents analyzed: {len(results)}")
    print(f"Data issues found: {len(data_issues)} ({len(data_issues)/len(results)*100:.1f}%)")
    print(f"High confidence classifications: {len(high_confidence)} ({len(high_confidence)/len(results)*100:.1f}%)")
    print("\n===== SAMPLE CLASSIFICATIONS =====")
    for i, result in enumerate(results[:min(3, len(results))]):
        print(f"\nIncident ID: {result.incident_id}")
        print(f"Classification: {'DATA ISSUE' if result.is_data_issue else 'NOT DATA ISSUE'}")
        print(f"Confidence: {result.confidence_score:.2f}")
        print("\nSupporting Reasons:")
        for reason in result.supporting_reasons[:min(2, len(result.supporting_reasons))]:
            print(f"- {reason.reason} (Impact: {reason.impact:.2f})")
        print("\nContrary Reasons:")
        for reason in result.contrary_reasons[:min(2, len(result.contrary_reasons))]:
            print(f"- {reason.reason} (Impact: {reason.impact:.2f})")
        if i < min(2, len(results) - 1):
            print("\n" + "-" * 50)

def main():
    print("IT Incident Data Issue Classifier")
    print("=" * 40)
    create_config_files()
    csv_path = "it_incidents.csv"
    output_path = "classification_results.json"
    if not os.path.exists(csv_path):
        print(f"Sample CSV not found at {csv_path}, creating a sample dataset...")
        create_sample_data(csv_path)
    if not os.path.exists(CONFIG_PATH) or not os.path.exists(CREDS_PATH):
        print("Error: Configuration files not found.")
        print(f"Please configure {CONFIG_PATH} and {CREDS_PATH} with your Azure OpenAI credentials.")
        return
    print("\nThis script will:")
    print("1. Connect to Azure OpenAI using your credentials")
    print("2. Process IT incidents from the CSV file")
    print("3. Classify each incident as a data issue or not")
    print("4. Save results to a JSON file")
    proceed = input("\nDo you want to proceed? (y/n): ").lower()
    if proceed != 'y':
        print("Exiting...")
        return
    try:
        print("\nInitializing IT Incident Classifier...")
        classifier = IncidentClassifier()
        print(f"Processing incidents from {csv_path}...")
        results = classifier.process_csv(csv_path, output_path)
        print_classification_results(results)
        print(f"\nResults saved to {output_path}")
    except Exception as e:
        logger.error(f"Error running classifier: {e}")
        print(f"\nError: {str(e)}")
        print("\nTroubleshooting tips:")
        print("1. Check your Azure OpenAI credentials in the env files")
        print("2. Verify that your Azure OpenAI service is set up correctly")
        print("3. Ensure you have the required Python packages installed")
        import traceback
        print("\nDetailed error:")
        traceback.print_exc()

if __name__ == "__main__":
    main()
