## CSV Processing and Semantic Matching with NLP, Hybrid Strategies, and REST API

from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain_core.runnables import RunnableParallel
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain_community.vectorstores import Chroma as LangChainChroma
from chromadb.config import Settings
from chromadb.config import Settings
from langchain_core.messages import HumanMessage, AIMessage
import chromadb
import uuid
from operator import itemgetter
from langchain_core.retrievers import BaseRetriever
from typing import List, Tuple, Dict, Any, Optional
from langchain_community.embeddings import AzureOpenAIEmbeddings
from langgraph.graph import END, StateGraph
import pandas as pd
import sys
import os
import logging
from pydantic import BaseModel, Field
import fastapi
from fastapi import FastAPI, File, UploadFile, Depends, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from starlette.responses import StreamingResponse
import uvicorn
import io
import tempfile
import shutil
import time
import asyncio
from datetime import datetime
from enum import Enum
import itertools
import difflib
import jellyfish  # For Levenshtein distance, Jaro-Winkler, etc.
from fuzzywuzzy import fuzz  # For fuzzy string matching
import statistics
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# Download required NLTK resources
try:
    nltk.download('wordnet', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except:
    logger.warning("Could not download NLTK resources. If offline, ensure they're pre-downloaded.")

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Try to load spaCy model, with fallback options
try:
    nlp = spacy.load("en_core_web_lg")  # Larger model with word vectors
except:
    try:
        nlp = spacy.load("en_core_web_md")  # Medium model
    except:
        try:
            nlp = spacy.load("en_core_web_sm")  # Small model as fallback
        except:
            logger.warning("Could not load any spaCy model. Installing minimal model.")
            import subprocess
            try:
                subprocess.run([sys.executable, "-m", "spacy", "download", "en_core_web_sm"], 
                              check=True, capture_output=True)
                nlp = spacy.load("en_core_web_sm")
            except:
                logger.error("Failed to load or install spaCy model. Text similarity will be limited.")
                # Create minimal blank model as fallback
                nlp = spacy.blank("en")


# Pydantic models for structured outputs
class ColumnMatch(BaseModel):
    name: str = Field(description="The matched target column name")
    score: float = Field(description="Confidence score for this match (0.0 to 1.0)")
    justification: str = Field(description="Detailed justification for why this is a good match")
    suggested_name: Optional[str] = Field(None, description="Suggested improved name if current match isn't ideal")
    suggested_name_reason: Optional[str] = Field(None, description="Reason for suggesting an improved name")

class MatchingResult(BaseModel):
    source_column: str = Field(description="The source column name being matched")
    matches: List[ColumnMatch] = Field(description="The top matches for this source column")
    reasoning: str = Field(description="Detailed reasoning process for all matches")


class MatchingStrategy(Enum):
    """Enumeration of available matching strategies"""
    SEMANTIC = "semantic"           # Vector embedding similarity
    LINGUISTIC = "linguistic"       # NLP-based comparison
    LEVENSHTEIN = "levenshtein"     # Edit distance
    JARO_WINKLER = "jaro_winkler"   # String similarity favoring shared prefixes
    TFIDF = "tfidf"                 # Term frequency-inverse document frequency
    FUZZY = "fuzzy"                 # Fuzzy string matching
    HYBRID = "hybrid"               # Combination of multiple strategies


class HybridMatcher:
    """Implements multiple matching strategies and ensembles them for better results"""
    
    def __init__(self, strategies=None, weights=None):
        """
        Initialize with selected strategies and their weights
        
        Args:
            strategies: List of MatchingStrategy enums to use
            weights: Dictionary mapping strategies to their weights
        """
        self.available_strategies = {
            MatchingStrategy.SEMANTIC: self.semantic_similarity,
            MatchingStrategy.LINGUISTIC: self.linguistic_similarity,
            MatchingStrategy.LEVENSHTEIN: self.levenshtein_similarity,
            MatchingStrategy.JARO_WINKLER: self.jaro_winkler_similarity,
            MatchingStrategy.TFIDF: self.tfidf_similarity,
            MatchingStrategy.FUZZY: self.fuzzy_similarity
        }
        
        # Default to using all strategies with equal weights if none specified
        if strategies is None:
            self.strategies = list(MatchingStrategy)
            # Remove HYBRID to avoid recursion
            if MatchingStrategy.HYBRID in self.strategies:
                self.strategies.remove(MatchingStrategy.HYBRID)
        else:
            self.strategies = strategies
            
        # Set default equal weights if none provided
        if weights is None:
            total_strategies = len(self.strategies)
            self.weights = {strategy: 1.0/total_strategies for strategy in self.strategies}
        else:
            self.weights = weights
            
        # Normalize weights to sum to 1.0
        weight_sum = sum(self.weights.values())
        if weight_sum != 1.0:
            self.weights = {k: v/weight_sum for k, v in self.weights.items()}
            
        # Initialize TF-IDF vectorizer
        self.tfidf = None
        self.tfidf_fit = False
        
        # List to store rules for rule-based matching
        self.rules = []
    
    def fit_tfidf(self, corpus):
        """Fit the TF-IDF vectorizer on a corpus of column names"""
        self.tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))
        self.tfidf.fit(corpus)
        self.tfidf_fit = True
        return self
    
    def semantic_similarity(self, text1, text2, nlp_model=None):
        """Compute semantic similarity using word embeddings"""
        if nlp_model is None:
            nlp_model = nlp  # Use global spaCy model
            
        # Check if model has word vectors
        if not nlp_model.has_vector:
            # Fallback to character n-gram comparison
            return self.tfidf_similarity(text1, text2)
            
        doc1 = nlp_model(text1.lower())
        doc2 = nlp_model(text2.lower())
        
        # If vectors are available, use them
        if doc1.has_vector and doc2.has_vector:
            return max(0.0, min(1.0, doc1.similarity(doc2)))
        
        # Fallback to token-level similarity
        tokens1 = [token for token in doc1 if not token.is_stop and not token.is_punct]
        tokens2 = [token for token in doc2 if not token.is_stop and not token.is_punct]
        
        if not tokens1 or not tokens2:
            return 0.0
            
        # Cross-compare tokens and average
        similarities = []
        for token1 in tokens1:
            for token2 in tokens2:
                if token1.has_vector and token2.has_vector:
                    similarities.append(token1.similarity(token2))
        
        if similarities:
            return max(0.0, min(1.0, sum(similarities) / len(similarities)))
        else:
            return 0.0
    
    def linguistic_similarity(self, text1, text2):
        """Compute linguistic similarity using NLP features"""
        return NLPHelper.compute_similarity(text1, text2)
    
    def levenshtein_similarity(self, text1, text2):
        """Compute similarity based on Levenshtein edit distance"""
        if not text1 or not text2:
            return 0.0
            
        # Convert Levenshtein distance to similarity score
        max_len = max(len(text1), len(text2))
        if max_len == 0:
            return 1.0
            
        distance = jellyfish.levenshtein_distance(text1.lower(), text2.lower())
        similarity = 1.0 - (distance / max_len)
        return max(0.0, similarity)
    
    def jaro_winkler_similarity(self, text1, text2):
        """Compute Jaro-Winkler similarity, which favors strings with common prefixes"""
        if not text1 or not text2:
            return 0.0
            
        return jellyfish.jaro_winkler_similarity(text1.lower(), text2.lower())
    
    def tfidf_similarity(self, text1, text2):
        """Compute similarity using TF-IDF vectors"""
        if not text1 or not text2:
            return 0.0
            
        if not self.tfidf_fit:
            # Create on-the-fly for this comparison only
            temp_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))
            matrix = temp_tfidf.fit_transform([text1.lower(), text2.lower()])
            return (matrix * matrix.T)[0, 1]
        else:
            # Use pre-fitted vectorizer
            vec1 = self.tfidf.transform([text1.lower()])
            vec2 = self.tfidf.transform([text2.lower()])
            return (vec1 * vec2.T)[0, 0]
    
    def fuzzy_similarity(self, text1, text2):
        """Compute fuzzy string similarity ratio"""
        if not text1 or not text2:
            return 0.0
            
        # Use token_set_ratio to handle word reordering and partial matches
        return fuzz.token_set_ratio(text1.lower(), text2.lower()) / 100.0
    
    def compute_similarity(self, text1, text2, metadata1=None, metadata2=None):
        """
        Compute similarity using ensemble of selected strategies
        
        Args:
            text1: First text to compare
            text2: Second text to compare
            metadata1: Optional metadata for first text
            metadata2: Optional metadata for second text
            
        Returns:
            Dictionary with overall score and individual strategy scores
        """
        results = {}
        
        # Compute similarity for each selected strategy
        for strategy in self.strategies:
            score = self.available_strategies[strategy](text1, text2)
            results[strategy.value] = score
            
        # Compute weighted ensemble score
        weighted_score = 0.0
        for strategy in self.strategies:
            weighted_score += results[strategy.value] * self.weights[strategy]
            
        results['ensemble'] = weighted_score
        
        # Include individual strategy contributions to final score
        results['contributions'] = {
            strategy.value: results[strategy.value] * self.weights[strategy] 
            for strategy in self.strategies
        }
        
        # Add metadata about strategies and weights used
        results['strategies_used'] = [s.value for s in self.strategies]
        results['weights_used'] = {s.value: self.weights[s] for s in self.strategies}
        
        return results


class NLPHelper:
    """Helper class for NLP operations using spaCy and NLTK"""
    
    @staticmethod
    def tokenize_and_clean(text):
        """Tokenize, lemmatize and clean text"""
        doc = nlp(text.lower())
        # Remove stop words and punctuation, then lemmatize
        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]
        return tokens
    
    @staticmethod
    def compute_similarity(text1, text2):
        """Compute semantic similarity between two texts using spaCy"""
        if not text1 or not text2:
            return 0.0
        
        doc1 = nlp(text1.lower())
        doc2 = nlp(text2.lower())
        
        # If the model has vectors, use them
        if doc1.has_vector and doc2.has_vector:
            return doc1.similarity(doc2)
        
        # Fallback to token overlap if vectors not available
        tokens1 = set(NLPHelper.tokenize_and_clean(text1))
        tokens2 = set(NLPHelper.tokenize_and_clean(text2))
        
        if not tokens1 or not tokens2:
            return 0.0
            
        # Jaccard similarity as fallback
        intersection = len(tokens1.intersection(tokens2))
        union = len(tokens1.union(tokens2))
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def find_synonyms(word):
        """Find synonyms for a word using WordNet"""
        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().replace('_', ' '))
        return list(synonyms)
    
    @staticmethod
    def extract_column_features(column_name):
        """Extract features from a column name"""
        # Handle camelCase, snake_case, etc.
        # First, split by underscore
        parts = column_name.split('_')
        
        # Then handle camelCase within each part
        all_parts = []
        for part in parts:
            # Add spaces before capital letters and split
            spaced = ''.join([' ' + c if c.isupper() else c for c in part]).strip()
            all_parts.extend(spaced.split())
        
        # Clean and normalize
        normalized_parts = [part.lower() for part in all_parts if part]
        
        # Get lemmatized form of each part
        lemmatized = [lemmatizer.lemmatize(part) for part in normalized_parts]
        
        return {
            'original': column_name,
            'tokens': normalized_parts,
            'lemmatized': lemmatized
        }
    
    @staticmethod
    def compare_column_names(source_name, target_name):
        """Compare column names using multiple techniques"""
        source_features = NLPHelper.extract_column_features(source_name)
        target_features = NLPHelper.extract_column_features(target_name)
        
        # Direct similarity using spaCy
        direct_sim = NLPHelper.compute_similarity(source_name, target_name)
        
        # Token overlap score
        source_tokens = set(source_features['lemmatized'])
        target_tokens = set(target_features['lemmatized'])
        
        if not source_tokens or not target_tokens:
            token_sim = 0.0
        else:
            intersection = len(source_tokens.intersection(target_tokens))
            union = len(source_tokens.union(target_tokens))
            token_sim = intersection / union if union > 0 else 0.0
        
        # Check for synonym overlap
        source_synonyms = set()
        for token in source_features['lemmatized']:
            source_synonyms.update(NLPHelper.find_synonyms(token))
        
        target_synonyms = set()
        for token in target_features['lemmatized']:
            target_synonyms.update(NLPHelper.find_synonyms(token))
        
        synonym_overlap = len(source_synonyms.intersection(target_synonyms))
        synonym_sim = synonym_overlap / max(len(source_synonyms), len(target_synonyms)) if max(len(source_synonyms), len(target_synonyms)) > 0 else 0.0
        
        # Combine scores (weights can be adjusted)
        combined_score = 0.5 * direct_sim + 0.3 * token_sim + 0.2 * synonym_sim
        
        return {
            'score': combined_score,
            'direct_similarity': direct_sim,
            'token_similarity': token_sim,
            'synonym_similarity': synonym_sim
        }


class SemanticColumnMatcher:
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH, 
                 matching_strategies=None, strategy_weights=None, nltk_path=None):
        self.env = OSEnv(config_file, creds_file, cert_file)
        
        # Initialize custom NLTK path if provided
        if nltk_path:
            logger.info(f"Using provided NLTK path: {nltk_path}")
            # Update environment variables for NLTK
            if os.path.exists(nltk_path):
                os.environ['NLTK_DATA'] = nltk_path
                logger.info(f"Set NLTK_DATA environment variable to: {nltk_path}")
        
        # Initialize hybrid matcher with specified strategies or defaults
        if matching_strategies is None:
            matching_strategies = [
                MatchingStrategy.SEMANTIC,     # 0.4 weight by default
                MatchingStrategy.LINGUISTIC,   # 0.3 weight by default
                MatchingStrategy.FUZZY,        # 0.2 weight by default
                MatchingStrategy.JARO_WINKLER  # 0.1 weight by default
            ]
            
        # Default weights that favor semantic and linguistic approaches
        if strategy_weights is None:
            strategy_weights = {
                MatchingStrategy.SEMANTIC: 0.4,
                MatchingStrategy.LINGUISTIC: 0.3,
                MatchingStrategy.FUZZY: 0.2,
                MatchingStrategy.JARO_WINKLER: 0.1
            }
        
        self.hybrid_matcher = HybridMatcher(
            strategies=matching_strategies,
            weights=strategy_weights
        )
        
        # Setup other components
        self.setup_components()
        
    def setup_components(self):
        """Initialize all components needed for semantic matching"""
        # Set up embedding client
        model_name = self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
        api_version = self.env.get("API_VERSION", "2023-05-15")
        azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
        azure_deployment = self.env.get("AZURE_DEPLOYMENT", "text-embedding-3-large")
        
        # Set up LangChain Azure embeddings
        token_provider = get_bearer_token_provider(
            self._get_credential(),
            "https://cognitiveservices.azure.com/.default"
        )
        
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment=azure_deployment,
            azure_endpoint=azure_endpoint,
            api_version=api_version,
            azure_ad_token_provider=token_provider
        )
        
        # Set up Azure Chat LLM
        model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
        temperature = float(self.env.get("TEMPERATURE", "0.7"))
        max_tokens = int(self.env.get("MAX_TOKENS", "800"))
        
        self.llm = AzureChatOpenAI(
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            api_version=api_version,
            azure_endpoint=azure_endpoint,
            azure_ad_token_provider=token_provider
        )
        
        # Set up Pydantic output parser
        self.parser = PydanticOutputParser(pydantic_object=MatchingResult)
        
        # Initialize job tracking for async processing
        self.jobs = {}
    
    def _get_credential(self):
        if str_to_bool(self.env.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(
                tenant_id=self.env.get("AZURE_TENANT_ID"), 
                client_id=self.env.get("AZURE_CLIENT_ID"), 
                client_secret=self.env.get("AZURE_CLIENT_SECRET")
            )
        
    def load_csv_data(self, source_csv_path: str, target_csv_path: str):
        """Load the two CSV files and prepare them for processing"""
        try:
            # Load source CSV (with id, appname, colname)
            self.source_df = pd.read_csv(source_csv_path)
            logger.info(f"Loaded source CSV with {len(self.source_df)} rows")
            
            # Load target CSV (with name and definition)
            self.target_df = pd.read_csv(target_csv_path)
            logger.info(f"Loaded target CSV with {len(self.target_df)} rows")
            
            # Verify required columns exist
            if not all(col in self.source_df.columns for col in ['id', 'appname', 'colname']):
                raise ValueError("Source CSV missing required columns: id, appname, colname")
                
            if not all(col in self.target_df.columns for col in ['name', 'definition']):
                raise ValueError("Target CSV missing required columns: name, definition")
                
            # Clean data
            self.source_df['colname'] = self.source_df['colname'].str.strip()
            self.target_df['name'] = self.target_df['name'].str.strip()
            self.target_df['definition'] = self.target_df['definition'].fillna('').str.strip()
            
            # Create enriched target data for better matching
            self.target_df['text_for_embedding'] = self.target_df.apply(
                lambda row: f"Column name: {row['name']}. Definition: {row['definition']}", 
                axis=1
            )
            
            # Add NLP features
            logger.info("Generating NLP features for target columns...")
            self.target_df['nlp_features'] = self.target_df['name'].apply(NLPHelper.extract_column_features)
            
            return True
        except Exception as e:
            logger.error(f"Error loading CSV data: {e}")
            raise
    
    def setup_vector_store(self):
        """Set up ChromaDB vector store with target column data"""
        try:
            # Create documents for each target column
            documents = []
            metadatas = []
            ids = []
            
            for _, row in self.target_df.iterrows():
                doc_id = str(uuid.uuid4())
                doc_text = row['text_for_embedding']
                doc_metadata = {
                    'name': row['name'],
                    'definition': row['definition']
                }
                
                documents.append(doc_text)
                metadatas.append(doc_metadata)
                ids.append(doc_id)
            
            # Create in-memory Chroma collection with telemetry disabled
            chroma_settings = Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
            
            self.vectorstore = LangChainChroma.from_texts(
                documents,
                self.embeddings,
                metadatas=metadatas,
                ids=ids,
                client_settings=chroma_settings
            )
            
            logger.info(f"Vector store created with {len(documents)} documents")
            return True
        except Exception as e:
            logger.error(f"Error setting up vector store: {e}")
            raise
    
    def enhance_retrieval(self, source_column, source_app, k=8):
        """Enhance vector retrieval with hybrid matching strategies"""
        # Get initial candidates from vector store
        query = f"Column name: {source_column} from application: {source_app}"
        initial_docs = self.vectorstore.as_retriever(search_kwargs={"k": k*2}).get_relevant_documents(query)
        
        # If TF-IDF vectorizer not fit yet, fit on all column names
        if not self.hybrid_matcher.tfidf_fit and len(initial_docs) > 0:
            all_column_names = [doc.metadata['name'] for doc in initial_docs]
            self.hybrid_matcher.fit_tfidf(all_column_names)
        
        # Calculate hybrid similarity scores for each candidate
        scores = []
        for doc in initial_docs:
            target_name = doc.metadata['name']
            definition = doc.metadata.get('definition', '')
            
            # Calculate hybrid similarity using multiple strategies
            hybrid_results = self.hybrid_matcher.compute_similarity(
                source_column, 
                target_name
            )
            
            # Extract ensemble score
            combined_score = hybrid_results['ensemble']
            
            # Also compute similarity between source column and definition
            # if definition is available
            if definition:
                definition_sim = self.hybrid_matcher.compute_similarity(
                    source_column, 
                    definition
                )
                
                # Blend vector similarity with definition similarity
                # Weight name matching higher than definition matching
                final_score = 0.7 * combined_score + 0.3 * definition_sim['ensemble']
            else:
                final_score = combined_score
                
            # Store document, score, and detailed results
            scores.append((doc, final_score, hybrid_results))
        
        # Sort by combined score and take top k
        scores.sort(key=lambda x: x[1], reverse=True)
        top_docs = [item[0] for item in scores[:k]]
        
        # Return the documents with their scores
        return top_docs, scores[:k]
    
    def create_rag_chain(self):
        """Create the RAG prompt chain for semantic matching"""
        # Define the semantic matching prompt template with format instructions
        template = """You are a semantic column matching expert. You need to find the most semantically similar matches 
between a source column name and potential target columns.

SOURCE COLUMN: {source_column}
SOURCE APPLICATION: {source_app}

POTENTIAL MATCHES:
{retrieved_documents}

INSTRUCTIONS:
1. Carefully analyze the semantic meaning of the source column name.
2. Consider the application context: {source_app}
3. Evaluate each potential match based on:
   - Semantic similarity to the source column name
   - How well the definition aligns with the likely purpose of the source column
   - Common database naming conventions and abbreviations
4. Think step-by-step to determine each match's relevance.
5. For each match, provide:
   - A confidence score between 0.0 and 1.0
   - A detailed justification
   - If the match isn't ideal, suggest a better name and explain why
6. Rank the top 4 matches in order of confidence score.

{format_instructions}
"""
        
        # Format instructions for Pydantic output
        format_instructions = self.parser.get_format_instructions()
        
        # Format the retrieved documents for the prompt
        def format_docs(docs_with_scores):
            docs = [doc for doc, _, _ in docs_with_scores]
            result = []
            for i, (doc, combined_score, similarity_details) in enumerate(docs_with_scores):
                name = doc.metadata['name']
                definition = doc.metadata.get('definition', '')
                
                # Format similarity metrics
                sim_text = f"Similarity Score: {combined_score:.3f}"
                
                entry = f"MATCH {i+1}:\nName: {name}\nDefinition: {definition}\n{sim_text}\n"
                result.append(entry)
            
            return "\n".join(result)
        
        # Create the RAG chain
        self.rag_chain = (
            {
                "source_column": itemgetter("source_column"),
                "source_app": itemgetter("source_app"),
                "retrieved_documents": itemgetter("docs_with_scores") | format_docs,
                "format_instructions": lambda _: format_instructions
            }
            | ChatPromptTemplate.from_template(template)
            | self.llm
            | self.parser
        )
        
        logger.info("RAG chain created for semantic matching")
        return True

    def setup_agent_graph(self):
        """Create a LangGraph workflow for agent-based selection"""
        
        # Define state
        class AgentState(BaseModel):
            source_column: str
            source_app: str
            source_id: str
            retrieved_docs: List[Document] = []
            docs_with_scores: List[Tuple[Document, float, Dict]] = []
            matching_result: Optional[MatchingResult] = None
            
        # Define the retrieval node
        def retrieve_and_analyze(state):
            # Enhanced retrieval using both vector similarity and NLP
            docs, docs_with_scores = self.enhance_retrieval(
                state.source_column, 
                state.source_app
            )
            
            return {
                "retrieved_docs": docs,
                "docs_with_scores": docs_with_scores
            }
        
        # Define the reasoning and selection node
        def reason_and_select(state):
            # Use the RAG chain to analyze and select matches
            matching_result = self.rag_chain.invoke({
                "source_column": state.source_column,
                "source_app": state.source_app,
                "docs_with_scores": state.docs_with_scores
            })
            
            return {"matching_result": matching_result}
        
        # Build the graph
        workflow = StateGraph(AgentState)
        
        # Add nodes
        workflow.add_node("retrieve_and_analyze", retrieve_and_analyze)
        workflow.add_node("reason_and_select", reason_and_select)
        
        # Add edges
        workflow.add_edge("retrieve_and_analyze", "reason_and_select")
        workflow.add_edge("reason_and_select", END)
        
        # Set entry point
        workflow.set_entry_point("retrieve_and_analyze")
        
        # Compile the graph
        self.agent_graph = workflow.compile()
        
        logger.info("Agent graph created for column matching")
        return True
    
    def match_columns(self):
        """Process all source columns and find their semantic matches"""
        try:
            results = []
            
            for _, row in self.source_df.iterrows():
                source_id = row['id']
                source_app = row['appname']
                source_column = row['colname']
                
                logger.info(f"Processing column: {source_column} from app: {source_app}")
                
                # Execute the agent graph
                final_state = self.agent_graph.invoke({
                    "source_column": source_column,
                    "source_app": source_app,
                    "source_id": source_id
                })
                
                # Store results
                result = {
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "matching_result": final_state.matching_result
                }
                
                results.append(result)
                
            self.matching_results = results
            logger.info(f"Completed matching for {len(results)} source columns")
            return results
        except Exception as e:
            logger.error(f"Error during column matching: {e}")
            raise
    
    def format_results(self):
        """Format the matching results into a pandas DataFrame"""
        formatted_results = []
        
        for result in self.matching_results:
            source_id = result["source_id"]
            source_app = result["source_app"]
            source_column = result["source_column"]
            matching_result = result["matching_result"]
            
            for i, match in enumerate(matching_result.matches):
                formatted_results.append({
                    "source_id": source_id,
                    "source_app": source_app,
                    "source_column": source_column,
                    "target_column": match.name,
                    "rank": i + 1,
                    "score": match.score,
                    "justification": match.justification,
                    "suggested_name": match.suggested_name if match.suggested_name else "",
                    "suggested_name_reason": match.suggested_name_reason if match.suggested_name_reason else ""
                })
        
        return pd.DataFrame(formatted_results)
    
    def save_results(self, output_path: str):
        """Save the matching results to a CSV file"""
        results_df = self.format_results()
        results_df.to_csv(output_path, index=False)
        logger.info(f"Results saved to {output_path}")
        return True
    
    def run_full_pipeline(self, source_csv_path: str, target_csv_path: str, output_path: str):
        """Run the complete semantic column matching pipeline"""
        try:
            logger.info("Starting semantic column matching pipeline")
            
            # Step 1: Load CSV data
            self.load_csv_data(source_csv_path, target_csv_path)
            
            # Step 2: Set up vector store
            self.setup_vector_store()
            
            # Step 3: Create RAG chain
            self.create_rag_chain()
            
            # Step 4: Set up agent graph
            self.setup_agent_graph()
            
            # Step 5: Run matching process
            self.match_columns()
            
            # Step 6: Save results
            self.save_results(output_path)
            
            logger.info("Pipeline completed successfully")
            return self.format_results()
        except Exception as e:
            logger.error(f"Error in pipeline: {e}")
            raise


# REST API Models
class APIConfig(BaseModel):
    """API configuration settings"""
    api_title: str = "Column Matcher API"
    api_description: str = "API for matching column names semantically"
    api_version: str = "1.0.0"
    allowed_origins: List[str] = ["*"]
    host: str = "0.0.0.0"
    port: int = 8000
    nltk_path: Optional[str] = None  # Custom path to NLTK data


class ColumnTestRequest(BaseModel):
    """Model for testing a column name"""
    column_name: str = Field(..., description="Column name to test")
    application_name: Optional[str] = Field(None, description="Source application name for context")
    top_k: Optional[int] = Field(4, description="Number of matches to return")


class ColumnMatchResponse(BaseModel):
    """Response model for column match"""
    target_column: str
    score: float
    justification: str
    suggested_name: Optional[str] = None
    suggested_name_reason: Optional[str] = None


class ColumnTestResponse(BaseModel):
    """Response model for column test results"""
    source_column: str
    matches: List[ColumnMatchResponse]


class RestAPI:
    """REST API implementation using FastAPI"""
    
    def __init__(self, config: APIConfig = None):
        """Initialize the REST API"""
        if config is None:
            config = APIConfig()
            
        self.config = config
        self.app = FastAPI(
            title=config.api_title,
            description=config.api_description,
            version=config.api_version
        )
        
        # Configure CORS
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=config.allowed_origins,
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
        
        # Set up routes
        self.setup_routes()
        
        # Initialize matcher with custom NLTK path if provided
        if config.nltk_path:
            self.matcher = SemanticColumnMatcher(
                nltk_path=config.nltk_path
            )
        else:
            self.matcher = None
            
        self.vectorstore = None
        self.initialized = False
    
    def initialize_matcher(self, source_csv_path: str, target_csv_path: str):
        """Initialize the matcher with data sources"""
        try:
            if self.matcher is None:
                self.matcher = SemanticColumnMatcher()
            
            # Load CSV data
            self.matcher.load_csv_data(source_csv_path, target_csv_path)
            
            # Set up vector store
            self.matcher.setup_vector_store()
            self.vectorstore = self.matcher.vectorstore
            
            # Create RAG chain
            self.matcher.create_rag_chain()
            
            # Set up agent graph
            self.matcher.setup_agent_graph()
            
            self.initialized = True
            logger.info("Matcher initialized successfully with provided data sources")
            
            return True
        except Exception as e:
            logger.error(f"Error initializing matcher: {e}")
            raise
    
    def setup_routes(self):
        """Set up API routes"""
        
        @self.app.get("/", tags=["Health"])
        async def root():
            """API health check endpoint"""
            status_msg = "initialized" if self.initialized else "not initialized"
            return {
                "status": "healthy", 
                "version": self.config.api_version,
                "matcher_status": status_msg
            }
        
        @self.app.post("/initialize", tags=["Setup"])
        async def initialize_api(
            source_file: UploadFile = File(...),
            target_file: UploadFile = File(...)
        ):
            """
            Initialize the matcher with data sources
            
            - **source_file**: CSV file with columns id, appname, colname
            - **target_file**: CSV file with columns name, definition
            """
            try:
                # Save uploaded files to temp directory
                temp_dir = tempfile.mkdtemp()
                source_path = os.path.join(temp_dir, "source.csv")
                target_path = os.path.join(temp_dir, "target.csv")
                
                # Save source file
                with open(source_path, "wb") as buffer:
                    shutil.copyfileobj(source_file.file, buffer)
                
                # Save target file
                with open(target_path, "wb") as buffer:
                    shutil.copyfileobj(target_file.file, buffer)
                
                # Initialize matcher
                self.initialize_matcher(source_path, target_path)
                
                # Clean up temp files
                shutil.rmtree(temp_dir)
                
                return {"status": "success", "message": "Matcher initialized successfully"}
                
            except Exception as e:
                logger.error(f"Error initializing API: {e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Error initializing API: {str(e)}"
                )
        
        @self.app.post("/test-column", response_model=ColumnTestResponse, tags=["Matching"])
        async def test_column(
            request: ColumnTestRequest
        ):
            """
            Test a new column name against the existing embeddings
            
            Matches the provided column name against the previously loaded target columns
            and returns the best semantic matches with confidence scores.
            
            - **column_name**: The column name to test
            - **application_name**: Optional application context
            - **top_k**: Number of matches to return (default: 4)
            """
            if not self.initialized:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Matcher not initialized. Call /initialize endpoint first."
                )
                
            try:
                source_column = request.column_name
                source_app = request.application_name or "Unknown"
                top_k = request.top_k or 4
                
                # Use agent to process the column
                final_state = self.matcher.agent_graph.invoke({
                    "source_column": source_column,
                    "source_app": source_app,
                    "source_id": str(uuid.uuid4())  # Generate a dummy ID
                })
                
                # Extract matches from result
                matching_result = final_state.matching_result
                
                # Format response
                matches = []
                for match in matching_result.matches[:top_k]:
                    matches.append(ColumnMatchResponse(
                        target_column=match.name,
                        score=match.score,
                        justification=match.justification,
                        suggested_name=match.suggested_name,
                        suggested_name_reason=match.suggested_name_reason
                    ))
                
                return ColumnTestResponse(
                    source_column=source_column,
                    matches=matches
                )
                
            except Exception as e:
                logger.error(f"Error testing column: {e}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Error testing column: {str(e)}"
                )
    
    def run(self):
        """Run the FastAPI server using uvicorn"""
        uvicorn.run(
            self.app,
            host=self.config.host,
            port=self.config.port
        )


# Command-line interface for running either as API or direct script
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Financial column name matching')
    parser.add_argument('--mode', type=str, choices=['api', 'cli'], default='cli',
                       help='Run as API server or CLI tool')
    
    # CLI mode arguments
    parser.add_argument('--source', type=str, help='Path to source CSV with id, appname, colname')
    parser.add_argument('--target', type=str, help='Path to target CSV with name, definition')
    parser.add_argument('--output', type=str, default='column_matches.csv', 
                       help='Path to save results')
    
    # API mode arguments
    parser.add_argument('--host', type=str, default='0.0.0.0', help='API server host')
    parser.add_argument('--port', type=int, default=8000, help='API server port')
    
    # Custom path for NLTK
    parser.add_argument('--nltk-path', type=str, default='library/nltk', 
                      help='Custom path to NLTK resources')
    
    args = parser.parse_args()
    
    # Update the paths with command-line arguments
    nltk_path = args.nltk_path
    
    if args.mode == 'api':
        # Run as API server
        config = APIConfig(
            host=args.host,
            port=args.port,
            nltk_path=nltk_path
        )
            
        api = RestAPI(config)
        
        # If source and target files are provided, initialize the matcher
        if args.source and args.target:
            try:
                api.matcher = SemanticColumnMatcher(
                    nltk_path=nltk_path
                )
                api.initialize_matcher(args.source, args.target)
                logger.info("API initialized with provided data sources")
            except Exception as e:
                logger.error(f"Failed to initialize matcher: {e}")
                logger.info("API will start, but matcher is not initialized")
        
        # Run the API server using uvicorn
        logger.info(f"Starting API server on {args.host}:{args.port}")
        logger.info(f"Using NLTK path: {nltk_path}")
        api.run()
        
    else:
        # Run as CLI tool
        if not args.source or not args.target:
            parser.error("CLI mode requires --source and --target arguments")
        
        logger.info(f"Using NLTK path: {nltk_path}")
        
        matcher = SemanticColumnMatcher(
            nltk_path=nltk_path
        )
        results = matcher.run_full_pipeline(args.source, args.target, args.output)
        
        # Print summary of results
        print(f"\nMatching Results Summary:")
        print(f"========================")
        
        # Group by source column
        by_source = results.groupby(['source_column'])
        
        for source_col, group in by_source:
            # Get top match
            top_match = group.iloc[0]
            print(f"\nSource Column: {source_col}")
            print(f"  Top Match: {top_match['target_column']} (Score: {top_match['score']:.2f})")
            
            # Show suggested name if available
            if top_match['suggested_name']:
                print(f"  Suggested Better Name: {top_match['suggested_name']}")
                print(f"  Reason: {top_match['suggested_name_reason']}")
            
            print(f"  Justification: {top_match['justification']}")
