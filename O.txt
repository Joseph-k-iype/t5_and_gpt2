import re
import logging
from typing import Dict, Any, List, Optional
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import AzureChatOpenAI
from app.core.models import EnhancementResult, TaggingResult, DataElement

logger = logging.getLogger(__name__)

class ConfidenceEvaluator:
    """Evaluates confidence scores for enhancement and tagging results."""
    
    def __init__(self, llm: AzureChatOpenAI):
        self.llm = llm
        self._setup_evaluation_chain()
    
    def _setup_evaluation_chain(self):
        template = """
        You are an expert in data governance and ISO/IEC 11179 metadata standards. Your task is to evaluate 
        the confidence in the enhancement and tagging of data elements.
        
        Data Element:
        - ID: {id}
        - Original Name: {original_name}
        - Original Description: {original_description}
        - Enhanced Name: {enhanced_name}
        - Enhanced Description: {enhanced_description}
        
        Enhancement Feedback: {enhancement_feedback}
        Validation Feedback: {validation_feedback}
        
        ISO/IEC 11179 standards for data element names (adapted for business-friendly format):
        - Names MUST be in lowercase with spaces between words.
        - Names MUST NOT use technical formatting like camelCase, snake_case or PascalCase
        - Names MUST NOT contain underscores, hyphens, or special characters
        - Names should be clear, unambiguous and self-describing
        - Names should not use acronyms or abbreviations unless they are universally understood
        - Names should be concise yet descriptive
        - Names should use standard terminology in the domain
        - Names should use business language that non-technical users can understand
        
        ISO/IEC 11179 standards for data element descriptions:
        - Descriptions should clearly define what the data element represents
        - Descriptions should be complete, covering the concept fully
        - Descriptions should be precise, specific enough to distinguish from other concepts
        - Descriptions should be objective and factual, not opinion-based
        - Descriptions should use complete sentences with proper grammar and punctuation
        - Descriptions should be written in business language, not technical jargon
        
        Based on the ISO/IEC 11179 standards, evaluate the confidence in the enhancement.
        
        Provide your evaluation as follows:
        1. Overall confidence score (0.0-1.0): [provide a confidence score, must be between 0.0 and 1.0]
        2. Detailed justification for the score: [explain your reasoning]
        """
        
        self.evaluation_prompt = PromptTemplate(
            input_variables=["id", "original_name", "original_description", "enhanced_name", 
                           "enhanced_description", "enhancement_feedback", "validation_feedback"],
            template=template)
        self.evaluation_chain = self.evaluation_prompt | self.llm | StrOutputParser()
        
        # Setup tagging evaluation chain
        tagging_template = """
        You are an expert in data governance and business terminology. Your task is to evaluate 
        the confidence in the tagging of data elements to business terms.
        
        Data Element:
        - ID: {id}
        - Name: {name}
        - Description: {description}
        
        Matched Business Terms:
        {matched_terms}
        
        Matching Confidence Scores:
        {confidence_scores}
        
        Based on the data element and matched business terms, evaluate the confidence in the tagging.
        Consider whether the business terms accurately represent the data element's semantics.
        
        Provide your evaluation as follows:
        1. Overall confidence score (0.0-1.0): [provide a confidence score, must be between 0.0 and 1.0]
        2. Detailed justification for the score: [explain your reasoning]
        """
        
        self.tagging_evaluation_prompt = PromptTemplate(
            input_variables=["id", "name", "description", "matched_terms", "confidence_scores"],
            template=tagging_template)
        self.tagging_evaluation_chain = self.tagging_evaluation_prompt | self.llm | StrOutputParser()
    
    def _parse_evaluation_result(self, result: str) -> float:
        lines = result.strip().split("\n")
        confidence = 0.5  # Default confidence
        
        for line in lines:
            if "confidence score" in line.lower():
                # Extract the confidence score
                match = re.search(r"\d+\.\d+", line)
                if match:
                    try:
                        confidence = float(match.group(0))
                        confidence = max(0.0, min(1.0, confidence))  # Ensure within bounds
                    except ValueError:
                        logger.warning(f"Could not parse confidence score from line: {line}")
                        pass
        
        return confidence
    
    async def evaluate_enhancement(self, 
                                 original_element: DataElement, 
                                 enhanced_result: EnhancementResult,
                                 validation_feedback: str) -> float:
        """Evaluate the confidence in the enhancement."""
        try:
            result = await self.evaluation_chain.ainvoke({
                "id": original_element.id,
                "original_name": original_element.existing_name,
                "original_description": original_element.existing_description,
                "enhanced_name": enhanced_result.enhanced_name,
                "enhanced_description": enhanced_result.enhanced_description,
                "enhancement_feedback": enhanced_result.feedback,
                "validation_feedback": validation_feedback
            })
            
            return self._parse_evaluation_result(result)
        except Exception as e:
            logger.error(f"Error evaluating enhancement confidence: {e}")
            return 0.5  # Default confidence in case of error
    
    async def evaluate_tagging(self, tagging_result: TaggingResult) -> float:
        """Evaluate the confidence in the tagging."""
        try:
            # Skip evaluation if modeling is required
            if tagging_result.modeling_required:
                return 0.0
                
            matched_terms_str = ""
            for i, term in enumerate(tagging_result.matching_terms):
                matched_terms_str += f"{i+1}. Term: {term['name']}\n   Description: {term['description']}\n"
            
            confidence_scores_str = ", ".join([f"{score:.2f}" for score in tagging_result.confidence_scores])
            
            result = await self.tagging_evaluation_chain.ainvoke({
                "id": tagging_result.element_id,
                "name": tagging_result.element_name,
                "description": tagging_result.element_description,
                "matched_terms": matched_terms_str,
                "confidence_scores": confidence_scores_str
            })
            
            return self._parse_evaluation_result(result)
        except Exception as e:
            logger.error(f"Error evaluating tagging confidence: {e}")
            return 0.5  # Default confidence in case of error
