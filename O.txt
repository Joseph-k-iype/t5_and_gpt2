import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
from typing import Optional, Any, Dict, List, Union, Tuple
from pathlib import Path
from datetime import datetime
from tqdm import tqdm

# Import from your existing code
from paste import (
    OSEnv, MyDocument, EmbeddingClient, AzureChatbot,
    CONFIG_PATH, CREDS_PATH, CERT_PATH, logger
)

# Import ChromaDB
import chromadb
from chromadb.config import Settings

# Import LangGraph and LangChain
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain.chains import ConversationChain

# Simplified CSV data processor
class CSVProcessor:
    def __init__(self, first_csv_path: str, second_csv_path: str):
        self.first_csv_path = first_csv_path
        self.second_csv_path = second_csv_path
        self.first_df = None
        self.second_df = None
    
    def _detect_encoding(self, file_path: str) -> str:
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        return result['encoding']
    
    def load_data(self):
        # Load CSVs with proper encoding
        first_encoding = self._detect_encoding(self.first_csv_path)
        second_encoding = self._detect_encoding(self.second_csv_path)
        
        self.first_df = pd.read_csv(self.first_csv_path, encoding=first_encoding)
        self.second_df = pd.read_csv(self.second_csv_path, encoding=second_encoding)
        
        # Clean and normalize columns
        self.first_df.columns = [col.strip().lower() for col in self.first_df.columns]
        self.second_df.columns = [col.strip().lower() for col in self.second_df.columns]
        
        # Fill missing values
        self.first_df = self.first_df.fillna('')
        self.second_df = self.second_df.fillna('')
        
        logger.info(f"First CSV loaded with {len(self.first_df)} rows")
        logger.info(f"Second CSV loaded with {len(self.second_df)} rows")
        
        # Return a tuple of both dataframes
        return self.first_df, self.second_df

# Semantic Mapper with LangGraph integration - only persisting first CSV vectors
class SemanticMapper:
    def __init__(self, config_file: str, creds_file: str, cert_file: str, persist_dir: str):
        """Initialize the semantic mapper with required components."""
        # Setup environment
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.persist_dir = persist_dir
        
        # Create directory if it doesn't exist
        os.makedirs(self.persist_dir, exist_ok=True)
        
        # Initialize embedding client
        self.embedding_client = EmbeddingClient(
            azure_api_version=self.env.get("AZURE_API_VERSION", "2023-05-15"),
            embeddings_model=self.env.get("EMBEDDINGS_MODEL", "text-embedding-3-large")
        )
        
        # Initialize ChromaDB client - only for persisting first CSV
        self.chroma_client = chromadb.PersistentClient(
            path=self.persist_dir,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Initialize LLM for confidence scoring
        self.chatbot = AzureChatbot(config_file, creds_file, cert_file)
        
        # Collection name - only need one for the first CSV
        self.collection_name = "first_csv_vectors"
        
        # Create LangGraph for the matching workflow
        self.graph = self._create_agent_graph()
        
        # Confidence threshold
        self.confidence_threshold = 0.75
    
    def _create_or_get_collection(self):
        """Create or get the collection for first CSV vectors."""
        try:
            # Try to get the collection
            collection = self.chroma_client.get_collection(name=self.collection_name)
            logger.info(f"Retrieved existing collection: {self.collection_name}")
        except Exception:
            # Create if it doesn't exist
            collection = self.chroma_client.create_collection(name=self.collection_name)
            logger.info(f"Created new collection: {self.collection_name}")
        
        return collection
    
    def _create_document_with_embedding(self, doc_id: str, text: str, metadata: Dict[str, Any]) -> Tuple[str, List[float], Dict[str, Any]]:
        """Create a document and generate its embedding using the EmbeddingClient."""
        # Create MyDocument instance
        doc = MyDocument(id=doc_id, text=text, metadata=metadata)
        
        # Generate embedding
        embedded_doc = self.embedding_client.generate_embeddings(doc)
        
        return doc_id, embedded_doc.embedding, metadata
    
    def index_first_csv(self, first_df):
        """Index only the first CSV data into ChromaDB for persistence."""
        # Get or create collection
        collection = self._create_or_get_collection()
        
        # Check if collection already has data
        if collection.count() > 0:
            logger.info(f"Collection already contains {collection.count()} documents")
            # Optionally check if update is needed based on the first CSV size
            if collection.count() >= len(first_df):
                logger.info("Collection already contains all the first CSV data, skipping indexing")
                return
        
        # Process first CSV
        docs = []
        embeddings = []
        ids = []
        metadatas = []
        
        logger.info("Generating embeddings for first CSV data...")
        for idx, row in tqdm(first_df.iterrows(), total=len(first_df), desc="Processing first CSV"):
            # Create document text
            doc_id = f"first_{idx}"
            
            # Make sure the required columns exist
            name = row['name'] if 'name' in row else ''
            definition = row['definition'] if 'definition' in row else ''
            owned_by = row['owned by'] if 'owned by' in row else ''
            
            text = f"Name: {name}\nDefinition: {definition}\nOwned by: {owned_by}"
            
            # Create metadata
            metadata = {
                "name": name,
                "definition": definition,
                "owned_by": owned_by,
                "original_index": int(idx)
            }
            
            # Generate embedding
            doc_id, embedding, metadata = self._create_document_with_embedding(doc_id, text, metadata)
            
            # Verify embedding was created successfully
            if not embedding or len(embedding) == 0:
                logger.warning(f"Empty embedding generated for document {doc_id}, skipping")
                continue
            
            # Store the results
            docs.append(text)
            embeddings.append(embedding)
            ids.append(doc_id)
            metadatas.append(metadata)
        
        # Make sure we have data to add
        if not ids:
            logger.warning("No valid documents to add to collection")
            return
            
        # Add documents to collection in batches
        batch_size = 100
        for i in range(0, len(ids), batch_size):
            end = min(i + batch_size, len(ids))
            try:
                collection.add(
                    ids=ids[i:end],
                    documents=docs[i:end],
                    embeddings=embeddings[i:end],
                    metadatas=metadatas[i:end]
                )
            except Exception as e:
                logger.error(f"Error adding batch to collection: {e}")
        
        logger.info(f"Indexed {len(ids)} documents from first CSV")
        logger.info(f"Collection now contains {collection.count()} documents")
    
    def find_match_for_second_csv_row(self, row, n_results: int = 3) -> Dict:
        """
        Find top matching names in the first CSV collection for a single row from the second CSV.
        Always returns the top n_results without applying any threshold.
        """
        # Extract data from second CSV row
        name = row['name']
        
        # Build context string from available metadata
        context_parts = []
        for field in ['code', 'taxonomy path 1', 'taxonomy path 2', 'parent', 'description', 'record examples']:
            if field in row and row[field]:
                context_parts.append(f"{field.capitalize()}: {row[field]}")
        
        context = "; ".join(context_parts)
        
        # Create query text
        query_text = f"Name: {name}"
        if context:
            query_text += f"\nContext: {context}"
        
        # Generate embedding for query
        query_doc = MyDocument(id="query", text=query_text)
        embedded_query = self.embedding_client.generate_embeddings(query_doc)
        query_embedding = embedded_query.embedding
        
        # Debug the embedding
        if not query_embedding or len(query_embedding) == 0:
            logger.warning(f"Empty embedding generated for query: {query_text}")
            # Return empty results structure
            return {"ids": [], "distances": [], "metadatas": []}, name, context
        
        # Query the collection
        collection = self._create_or_get_collection()
        
        # Make sure collection has documents
        if collection.count() == 0:
            logger.warning("Collection is empty, no matches possible")
            return {"ids": [], "distances": [], "metadatas": []}, name, context
        
        try:
            # Get the top n matches without any threshold
            num_results = min(n_results, collection.count())
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=num_results
            )
            return results, name, context
        except Exception as e:
            logger.error(f"Error querying collection: {e}")
            # Return empty results
            return {"ids": [], "distances": [], "metadatas": []}, name, context
    
    def _create_agent_graph(self):
        """Create the LangGraph for the name matching workflow."""
        try:
            # Define state type for the graph
            class AgentState(dict):
                """State for the name matching agent graph."""
                pass
            
            # Define the nodes in our graph
            def evaluate_match_quality(state):
                """
                Evaluate the quality of the matches using the agent, considering all top matches.
                The agent will score each match and either select the best one or suggest a better match.
                """
                source_name = state["source_name"]
                match_results = state["match_results"]
                source_context = state["source_context"]
                
                if not match_results["ids"] or len(match_results["ids"]) == 0:
                    return {
                        "best_match": None,
                        "confidence_score": 0.0,
                        "suggestion": None
                    }
                
                # Process all matches - we'll show all top matches to the agent
                matches = []
                for i in range(len(match_results["ids"])):
                    match = {
                        "id": match_results["ids"][i],
                        "name": match_results["metadatas"][i]["name"],
                        "definition": match_results["metadatas"][i]["definition"],
                        "owned_by": match_results["metadatas"][i]["owned_by"],
                        "distance": match_results["distances"][i] if "distances" in match_results else None,
                    }
                    matches.append(match)
                
                # Create prompt for LLM, showing all top matches
                matches_text = "\n".join([
                    f"Match {i+1}:\n  Name: {m['name']}\n  Definition: {m['definition']}\n  Owned by: {m['owned_by']}"
                    for i, m in enumerate(matches)
                ])
                
                prompt = f"""
                You are an expert in semantic name matching. Your task is to evaluate the quality of potential matches
                for a source name, considering its context.

                Source name: {source_name}
                Source context: {source_context}

                Top potential matches from vector similarity search:
                {matches_text}

                Please evaluate these matches and decide:
                1. Which match (if any) is the best semantic match for the source name?
                2. Assign a confidence score between 0 and 1 to indicate how certain you are that it's a good match
                3. If none of the matches are good enough, suggest an alternative name that would be a better match

                Provide your response as a JSON with the following structure:
                {{
                    "best_match_index": [index of best match (0, 1, 2, etc.) or -1 if none are good],
                    "confidence_score": [score between 0 and 1],
                    "reasoning": "[your detailed reasoning]",
                    "suggestion": "[optional: your suggested better name if none are good]"
                }}
                """
                
                # Get evaluation from LLM using the AzureChatbot
                response = self.chatbot.conversation.run(prompt)
                
                # Parse the JSON response
                try:
                    # Extract JSON from the response
                    json_str = response.strip()
                    if "```json" in json_str:
                        json_str = json_str.split("```json")[1].split("```")[0].strip()
                    elif "```" in json_str:
                        json_str = json_str.split("```")[1].split("```")[0].strip()
                    
                    evaluation = json.loads(json_str)
                    best_match_index = evaluation.get("best_match_index", -1)
                    confidence_score = float(evaluation.get("confidence_score", 0.0))
                    
                    # Get the best match if a valid index was returned
                    if best_match_index >= 0 and best_match_index < len(matches):
                        best_match = matches[best_match_index]
                    else:
                        best_match = None
                    
                    return {
                        "best_match": best_match,
                        "confidence_score": confidence_score,
                        "reasoning": evaluation.get("reasoning", ""),
                        "suggestion": evaluation.get("suggestion", None)
                    }
                except Exception as e:
                    logger.error(f"Error parsing LLM response: {e}")
                    
                    # Default to using the first match as a fallback
                    if matches:
                        confidence_score = 1 - min(1, matches[0].get("distance", 0.5) / 2)
                        return {
                            "best_match": matches[0],
                            "confidence_score": confidence_score,
                            "reasoning": f"Failed to parse LLM response, using best vector similarity match.",
                            "suggestion": None
                        }
                    else:
                        return {
                            "best_match": None,
                            "confidence_score": 0.0,
                            "reasoning": "No matches found.",
                            "suggestion": None
                        }
            
            # Single-node graph - we directly use the agent's evaluation and suggestion
            workflow = StateGraph(AgentState)
            workflow.add_node("evaluate_match", evaluate_match_quality)
            workflow.set_entry_point("evaluate_match")
            workflow.add_edge("evaluate_match", END)
            
            # Compile the graph
            return workflow.compile()
        
        except Exception as e:
            logger.error(f"Error creating agent graph: {e}")
            raise
    
    def evaluate_with_agent(self, source_name: str, source_context: str, match_results: Dict) -> Dict:
        """
        Use the LangGraph agent to evaluate all top matches and either select the best one
        or suggest a better match if none are good enough.
        """
        try:
            # Check if results are empty
            if not match_results["ids"] or len(match_results["ids"]) == 0:
                logger.info(f"No matches found for {source_name}")
                return {
                    "source_name": source_name,
                    "best_match": None,
                    "confidence_score": 0.0,
                    "suggestion": "No matches found in the database.",
                    "explanation": "The source name could not be matched with any entry in the first CSV.",
                    "reasoning": "No matching entries found"
                }
            
            # Initialize the state with all matches
            initial_state = {
                "source_name": source_name,
                "source_context": source_context,
                "match_results": match_results
            }
            
            # Run the graph - this will now evaluate all matches
            result = self.graph.invoke(initial_state)
            
            # Return the agent's evaluation, which could include the best match
            # or a suggestion for a better match
            return {
                "source_name": source_name,
                "best_match": result.get("best_match"),
                "confidence_score": result.get("confidence_score", 0.0),
                "suggestion": result.get("suggestion"),
                "reasoning": result.get("reasoning", "")
            }
        except Exception as e:
            logger.error(f"Error evaluating match for {source_name}: {e}")
            # Return a default result if there's an error
            return {
                "source_name": source_name,
                "best_match": None,
                "confidence_score": 0.0,
                "suggestion": f"Error during evaluation: {str(e)}",
                "reasoning": "Error in agent processing"
            }
    
    def process_second_csv(self, second_df):
        """Process each row from the second CSV individually, without storing in ChromaDB."""
        results = []
        
        for idx, row in tqdm(second_df.iterrows(), total=len(second_df), desc="Mapping names"):
            try:
                # Find top matches for this specific row (always returns top 3 without threshold)
                match_results, name, context = self.find_match_for_second_csv_row(row, n_results=3)
                
                # Get the code from the row
                code = row['code'] if 'code' in row else ''
                
                # Evaluate all matches using the agent, which will select the best one
                # or suggest a better match if none are good enough
                evaluation = self.evaluate_with_agent(name, context, match_results)
                
                # Create result entry
                result = {
                    "code": code,
                    "name": name,
                    "matched_name": evaluation["best_match"]["name"] if evaluation["best_match"] else None,
                    "matched_owned_by": evaluation["best_match"]["owned_by"] if evaluation["best_match"] else None,
                    "matched_definition": evaluation["best_match"]["definition"] if evaluation["best_match"] else None,
                    "confidence_score": evaluation["confidence_score"],
                    "suggestion": evaluation.get("suggestion")
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error processing row {idx}: {e}")
                results.append({
                    "code": row.get('code', ''),
                    "name": row.get('name', ''),
                    "matched_name": None,
                    "matched_owned_by": None, 
                    "matched_definition": None,
                    "confidence_score": 0.0,
                    "suggestion": f"Error: {str(e)}"
                })
        
        return results

# Main function to run the mapping
def run_mapping(first_csv: str, second_csv: str, config: str, creds: str, cert: str, persist_dir: str, output: str = None):
    """Run the semantic name mapping with only first CSV persisted in ChromaDB."""
    # Create CSV processor and load data
    csv_processor = CSVProcessor(first_csv, second_csv)
    first_df, second_df = csv_processor.load_data()
    
    # Create semantic mapper with LangGraph
    mapper = SemanticMapper(config, creds, cert, persist_dir)
    
    # Index only the first CSV into ChromaDB
    mapper.index_first_csv(first_df)
    
    # Process the second CSV dynamically (without persisting)
    results = mapper.process_second_csv(second_df)
    
    # Create DataFrame and save results
    results_df = pd.DataFrame(results)
    
    # Order columns
    column_order = ['code', 'name', 'matched_name', 'matched_owned_by', 'matched_definition', 'confidence_score', 'suggestion']
    results_df = results_df[column_order]
    
    # Save to CSV
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output or f"name_mapping_results_{timestamp}.csv"
    results_df.to_csv(output_file, index=False)
    
    # Print summary
    good_matches = results_df[results_df['confidence_score'] >= 0.75]
    logger.info(f"Completed semantic name mapping with {len(results_df)} names")
    logger.info(f"Good matches: {len(good_matches)} ({len(good_matches)/len(results_df)*100:.2f}%)")
    logger.info(f"Results saved to {output_file}")
    
    return results_df

# Command-line interface
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Semantic name mapping between two CSV files")
    parser.add_argument("--first-csv", required=True, help="Path to the first CSV file (name, definition, owned by)")
    parser.add_argument("--second-csv", required=True, help="Path to the second CSV file with taxonomy data")
    parser.add_argument("--config", default=CONFIG_PATH, help="Path to the config file")
    parser.add_argument("--creds", default=CREDS_PATH, help="Path to the credentials file")
    parser.add_argument("--cert", default=CERT_PATH, help="Path to the certificate file")
    parser.add_argument("--persist-dir", default="./chroma_db", help="Directory to persist vector database")
    parser.add_argument("--output", default=None, help="Path to save output CSV file")
    
    args = parser.parse_args()
    
    run_mapping(
        first_csv=args.first_csv,
        second_csv=args.second_csv,
        config=args.config,
        creds=args.creds,
        cert=args.cert,
        persist_dir=args.persist_dir,
        output=args.output
    )
