#!/usr/bin/env python
"""
Business Terms Import Script

This script imports business terms from a CSV file into the vector database.
It uses the BusinessTermManager to handle the import process.
"""

import os
import sys
import logging
import argparse
from typing import Optional, Tuple
import pandas as pd
import uuid
import csv
import time

# Add parent directory to path to import app modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.core.business_terms import BusinessTermManager, ConceptRelationship
from app.core.embedding import MyDocument
from app.config.environment import get_os_env

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def check_dependencies():
    """Check if required dependencies are installed."""
    try:
        import chardet
        return True
    except ImportError:
        logger.error("chardet library not installed. Installing now...")
        try:
            import subprocess
            subprocess.check_call([sys.executable, "-m", "pip", "install", "chardet"])
            import chardet
            logger.info("chardet successfully installed")
            return True
        except Exception as e:
            logger.error(f"Failed to install chardet: {e}")
            logger.error("Please install it manually: pip install chardet")
            return False

def detect_encoding(file_path: str) -> Tuple[str, float]:
    """
    Detect the encoding of a file using chardet.
    
    Args:
        file_path: Path to the file
        
    Returns:
        Tuple containing the detected encoding and confidence
    """
    try:
        import chardet
    except ImportError:
        logger.warning("chardet not available. Defaulting to UTF-8.")
        return "utf-8", 0.0
        
    try:
        with open(file_path, 'rb') as f:
            # Read a sample of the file for detection
            # For large files, we don't need to read the whole file
            file_size = os.path.getsize(file_path)
            sample = f.read(min(1024 * 1024, file_size))
        
        result = chardet.detect(sample)
        encoding = result['encoding']
        confidence = result['confidence']
        
        logger.info(f"Detected encoding: {encoding} (confidence: {confidence:.2f})")
        return encoding, confidence
    except Exception as e:
        logger.error(f"Error detecting encoding: {e}")
        return "utf-8", 0.0

def validate_csv(file_path: str) -> bool:
    """
    Validate the CSV file has the required columns.
    
    Args:
        file_path: Path to the CSV file
        
    Returns:
        bool: True if valid, False otherwise
    """
    try:
        # Detect file encoding
        encoding, confidence = detect_encoding(file_path)
        
        if confidence < 0.7:
            logger.warning(f"Low confidence in encoding detection ({confidence:.2f})")
        
        # Try with detected encoding first
        try:
            df = pd.read_csv(file_path, nrows=1, encoding=encoding)
        except Exception as e:
            logger.warning(f"Failed to read with detected encoding {encoding}: {e}")
            
            # Fallback encodings to try
            fallback_encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
            for enc in fallback_encodings:
                if enc != encoding:  # Skip if already tried
                    try:
                        logger.info(f"Trying with encoding: {enc}")
                        df = pd.read_csv(file_path, nrows=1, encoding=enc)
                        logger.info(f"Successfully read with encoding: {enc}")
                        encoding = enc  # Update to the working encoding
                        break
                    except Exception:
                        continue
            else:
                logger.error("Failed to read CSV with all attempted encodings")
                return False
        
        required_columns = ['name', 'description']
        
        # Check if all required columns exist
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"Missing required columns: {', '.join(missing_columns)}")
            return False
        
        # Store the detected encoding in the environment for later use
        os.environ["CSV_DETECTED_ENCODING"] = encoding
        return True
    except Exception as e:
        logger.error(f"Error validating CSV file: {e}")
        return False

def clean_csv(input_file: str, output_file: Optional[str] = None) -> Optional[str]:
    """
    Clean and prepare CSV file for import.
    
    Args:
        input_file: Path to the input CSV file
        output_file: Path to save the cleaned CSV file, if not provided, a temp file is created
        
    Returns:
        str: Path to the cleaned CSV file, or None if cleaning failed
    """
    try:
        if not os.path.exists(input_file):
            logger.error(f"Input file doesn't exist: {input_file}")
            return None
            
        # Create output file path if not provided
        if not output_file:
            basename = os.path.basename(input_file)
            dirname = os.path.dirname(input_file)
            filename, ext = os.path.splitext(basename)
            output_file = os.path.join(dirname, f"{filename}_cleaned{ext}")
        
        # Get the detected encoding from environment or detect it
        encoding = os.environ.get("CSV_DETECTED_ENCODING")
        if not encoding:
            encoding, _ = detect_encoding(input_file)
        
        logger.info(f"Reading CSV with encoding: {encoding}")
        
        # Read the CSV file with detected encoding
        try:
            df = pd.read_csv(input_file, encoding=encoding)
        except Exception as e:
            logger.warning(f"Error reading with encoding {encoding}: {e}")
            # Fallback to latin1 which rarely fails (but might not interpret characters correctly)
            logger.info("Falling back to latin1 encoding")
            df = pd.read_csv(input_file, encoding='latin1')
        
        # Basic cleaning: remove extra whitespace, drop completely empty rows
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].str.strip()
        
        df = df.dropna(how='all')
        
        # Ensure ID column exists
        if 'id' not in df.columns:
            logger.info("Adding 'id' column with generated UUIDs")
            df['id'] = [f"term-{uuid.uuid4()}" for _ in range(len(df))]
        
        # Ensure required columns exist
        for col in ['name', 'description']:
            if col not in df.columns:
                logger.error(f"Required column '{col}' missing from CSV")
                return None
        
        # Handle missing values 
        df['name'] = df['name'].fillna('')
        df['description'] = df['description'].fillna('')
        
        # Filter rows with empty name or description
        valid_rows = (df['name'] != '') & (df['description'] != '')
        invalid_count = (~valid_rows).sum()
        
        if invalid_count > 0:
            logger.warning(f"Removing {invalid_count} rows with empty name or description")
            df = df[valid_rows]
        
        # Save cleaned CSV with UTF-8 encoding for consistency
        df.to_csv(output_file, index=False, encoding='utf-8')
        logger.info(f"Cleaned CSV saved to {output_file}")
        
        return output_file
    except Exception as e:
        logger.error(f"Error cleaning CSV: {e}")
        return None

def import_terms_row_by_row(file_path: str, encoding: str = 'utf-8') -> int:
    """
    Import business terms from a CSV file, processing each row individually.
    This method is more memory-efficient for large CSV files.
    
    Args:
        file_path: Path to the CSV file
        encoding: File encoding
        
    Returns:
        Number of terms imported/updated
    """
    try:
        # Initialize business term manager
        btm = BusinessTermManager()
        
        # Get existing terms for tracking what needs to be deleted later
        existing_terms = {}
        for term in btm.get_all_terms():
            term_key = f"{term.name}::{term.description}"
            existing_terms[term_key] = term.id
        
        # Track terms found in CSV
        csv_term_keys = set()
        
        # Counters for tracking progress
        added_count = 0
        updated_count = 0
        skipped_count = 0
        row_count = 0
        start_time = time.time()
        
        # Open and process CSV file
        try:
            with open(file_path, 'r', encoding=encoding) as csvfile:
                reader = csv.DictReader(csvfile)
                
                # Validate required columns
                if not reader.fieldnames:
                    raise ValueError("CSV file has no headers or is empty")
                    
                required_columns = ['name', 'description']
                missing_columns = [col for col in required_columns if col not in reader.fieldnames]
                if missing_columns:
                    raise ValueError(f"Missing required columns: {', '.join(missing_columns)}")
                
                # Process each row individually
                for row in reader:
                    row_count += 1
                    
                    # Skip rows with missing required fields
                    if 'name' not in row or 'description' not in row:
                        logger.warning(f"Skipping row {row_count} with missing required fields")
                        skipped_count += 1
                        continue
                    
                    # Get and clean the data
                    term_id = row.get('id', '').strip()
                    name = row['name'].strip()
                    description = row['description'].strip()
                    
                    # Skip rows with empty name or description
                    if not name or not description:
                        logger.warning(f"Skipping row {row_count} with empty name or description")
                        skipped_count += 1
                        continue
                    
                    # Generate a unique ID if not provided
                    if not term_id:
                        term_id = f"term-{uuid.uuid4()}"
                    
                    # Create term key for tracking
                    term_key = f"{name}::{description}"
                    csv_term_keys.add(term_key)
                    
                    # Skip if term already exists and is unchanged
                    if term_key in existing_terms and existing_terms[term_key] == term_id:
                        skipped_count += 1
                        continue
                    
                    # Extract metadata
                    metadata = {}
                    for key, value in row.items():
                        if key not in ['id', 'name', 'description'] and value:
                            metadata[key] = value
                    
                    # Create document for embedding
                    doc = MyDocument(
                        id=term_id,
                        text=f"{name}. {description}"
                    )
                    
                    # Generate embedding
                    doc_with_embedding = btm.embedding_client.generate_embeddings(doc)
                    
                    if not doc_with_embedding.embedding:
                        logger.warning(f"Skipping row {row_count}: could not generate embedding for '{name}'")
                        skipped_count += 1
                        continue
                    
                    # Store the term in the vector store
                    success = btm.vector_store.store_vector(
                        id=term_id,
                        name=name,
                        description=description,
                        embedding=doc_with_embedding.embedding,
                        metadata=metadata
                    )
                    
                    if success:
                        if term_key in existing_terms:
                            updated_count += 1
                        else:
                            added_count += 1
                    
                    # Log progress periodically
                    if row_count % 10 == 0:
                        elapsed = time.time() - start_time
                        rate = row_count / elapsed if elapsed > 0 else 0
                        logger.info(f"Processed {row_count} rows: added {added_count}, updated {updated_count}, " +
                                   f"skipped {skipped_count}, rate: {rate:.2f} rows/sec")
        
        except UnicodeDecodeError as e:
            logger.error(f"Error reading CSV with encoding {encoding}: {e}")
            raise
        
        # Handle deletion of terms not in the CSV
        deleted_count = 0
        terms_to_delete = []
        
        for term_key, term_id in existing_terms.items():
            if term_key not in csv_term_keys:
                terms_to_delete.append(term_id)
        
        # Delete in batches of 10 to avoid overwhelming the system
        for i in range(0, len(terms_to_delete), 10):
            batch = terms_to_delete[i:i+10]
            for term_id in batch:
                if btm.vector_store.delete_term(term_id):
                    deleted_count += 1
            
            if deleted_count % 10 == 0 and deleted_count > 0:
                logger.info(f"Deleted {deleted_count}/{len(terms_to_delete)} terms not found in CSV")
        
        # Re-initialize concept relationships
        btm.concept_relationships = ConceptRelationship()
        
        # Log summary
        total_time = time.time() - start_time
        logger.info(f"Import summary: Processed {row_count} rows in {total_time:.2f}s ({row_count/total_time:.2f} rows/sec)")
        logger.info(f"Added: {added_count}, Updated: {updated_count}, Skipped: {skipped_count}, Deleted: {deleted_count}")
        
        return added_count + updated_count
    
    except Exception as e:
        logger.error(f"Error importing terms row by row: {e}")
        raise

def import_terms(file_path: str, batch_size: int = 100, row_by_row: bool = False) -> bool:
    """
    Import business terms from a CSV file.
    
    Args:
        file_path: Path to the CSV file
        batch_size: Number of terms to process in each batch (used only if row_by_row=False)
        row_by_row: Whether to process the file row by row instead of in batches
        
    Returns:
        bool: True if import was successful, False otherwise
    """
    try:
        # Get the detected encoding
        encoding = os.environ.get("CSV_DETECTED_ENCODING", "utf-8")
        logger.info(f"Importing terms with encoding: {encoding}")
        
        if row_by_row:
            # Import terms row by row
            terms_added = import_terms_row_by_row(file_path, encoding=encoding)
        else:
            # Initialize business term manager
            btm = BusinessTermManager()
            # Import terms in batches (original method)
            terms_added = btm.import_terms_from_csv(file_path, encoding=encoding, batch_size=batch_size)
        
        logger.info(f"Successfully imported {terms_added} business terms")
        return True
    except Exception as e:
        logger.error(f"Error importing business terms: {e}")
        return False

def main():
    # Ensure all dependencies are installed
    if not check_dependencies():
        return 1
        
    parser = argparse.ArgumentParser(description="Import business terms from a CSV file")
    parser.add_argument("csv_file", help="Path to the CSV file containing business terms")
    parser.add_argument("--batch-size", type=int, default=100, help="Number of terms to process in each batch")
    parser.add_argument("--clean", action="store_true", help="Clean and prepare the CSV file before import")
    parser.add_argument("--validate-only", action="store_true", help="Only validate the CSV file, don't import")
    parser.add_argument("--output", help="Path to save the cleaned CSV file")
    parser.add_argument("--chroma-dir", help="Override the ChromaDB directory")
    parser.add_argument("--chroma-collection", help="Override the ChromaDB collection name")
    parser.add_argument("--encoding", help="Force a specific encoding instead of auto-detection")
    # Add the new row-by-row option
    parser.add_argument("--row-by-row", action="store_true", 
                        help="Process the CSV row by row instead of in batches (recommended for large files)")
    
    args = parser.parse_args()
    
    # Set environment variables if overrides provided
    if args.chroma_dir:
        os.environ["CHROMA_PERSIST_DIR"] = args.chroma_dir
        logger.info(f"Using ChromaDB directory: {args.chroma_dir}")
    
    if args.chroma_collection:
        os.environ["CHROMA_COLLECTION"] = args.chroma_collection
        logger.info(f"Using ChromaDB collection: {args.chroma_collection}")
        
    # Set encoding if provided
    if args.encoding:
        os.environ["CSV_DETECTED_ENCODING"] = args.encoding
        logger.info(f"Using forced encoding: {args.encoding}")
    
    # Validate the CSV file
    if not validate_csv(args.csv_file):
        logger.error("CSV file validation failed")
        return 1
    
    if args.validate_only:
        logger.info("CSV file validation successful")
        return 0
    
    # Clean the CSV file if requested
    csv_file_to_import = args.csv_file
    if args.clean:
        cleaned_file = clean_csv(args.csv_file, args.output)
        if not cleaned_file:
            logger.error("CSV cleaning failed")
            return 1
        csv_file_to_import = cleaned_file
    
    # Import the terms
    success = import_terms(csv_file_to_import, args.batch_size, args.row_by_row)
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())
