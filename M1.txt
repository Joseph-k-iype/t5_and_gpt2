import os
import sys
import uuid
import json
import logging
import chardet
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel
from langchain_community.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.schema import Document as LC_DOCUMENT
from langchain.embeddings.base import Embeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
from pydantic import BaseModel, ValidationError, field_validator
from langchain.agents import AgentExecutor, create_react_agent
from langchain.schema import HumanMessage, SystemMessage
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from langchain.tools import BaseTool
from langchain_community.tools import Tool
from langchain_community.output_parsers import ResponseSchema, StructuredOutputParser

logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        raise FileNotFoundError(f"The file '{filepath}' does not exist or is not readable")
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s== 'True':
        return True
    elif s== 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class

class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.credential = None
        self.bulk_set(config_file, True)
        self.bulk_set(creds_file, False)
        self.set_certificate_path(certificate_path)
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            self.set_proxy()
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            self.token = self.get_azure_token()
        else:
            self.token = None
        
    def _get_credential(self):
        if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
            return DefaultAzureCredential()
        else:
            return ClientSecretCredential(tenant_id=self.get("AZURE_TENANT_ID"), client_id=self.get("AZURE_CLIENT_ID"), client_secret=self.get("AZURE_CLIENT_SECRET"))
    
    def set_certificate_path(self, path: str):
        try:
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            if not is_file_readable(path):
                raise FileNotFoundError(f"The file '{path}' does not exist or is not readable")
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
            raise
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        try:
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            
            # Check if file exists and is readable
            try:
                is_file_readable(dotenvfile)
                # If we get here, the file exists and is readable
                temp_dict = dotenv_values(dotenvfile)
                for key, value in temp_dict.items():
                    self.set(key, value, print_val)
                logger.info(f"Loaded {len(temp_dict)} environment variables from {dotenvfile}")
                del temp_dict
            except FileNotFoundError as e:
                logger.error(f"File not found or not readable: {dotenvfile}")
                raise
                
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
            raise
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        try:
            os.environ[key] = value
            if key not in self.var_list:
                self.var_list.append(key)
            if print_val:
                logger.info(f"{key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
            raise
    
    def get(self, key: str, default: Optional[str] = None)->str:
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            raise
    
    def set_proxy(self) -> None:
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            if not all([ad_username, ad_password, proxy_domain]):
                raise ValueError("Proxy settings are incomplete")
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
            raise
    
    def get_azure_token(self) -> str:
        try:
            credential = ClientSecretCredential(
                tenant_id=self.get("AZURE_TENANT_ID"),
                client_id=self.get("AZURE_CLIENT_ID"),
                client_secret=self.get("AZURE_CLIENT_SECRET")
            )
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token set")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## embedding class + Document class

class MyDocument(BaseModel):
    id: str
    text: str
    embedding: List[float]
    metadata: Dict[str, Any]
    
    def __init__(self, **data):
        super().__init__(
            id=data.get("id", ""),
            text=data.get("text", ""),
            embedding=data.get("embedding", []),
            metadata=data.get("metadata", {})
        )

class EmbeddingClient:
    def __init__(self, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = self._get_direct_azure_client()
    
    def _get_direct_azure_client(self):
        token_provider = get_bearer_token_provider(
            DefaultAzureCredential(),
            "https://cognitiveservices.azure.com/.default"
        )
        azure_endpoint = os.environ.get("AZURE_ENDPOINT", "")
        if not azure_endpoint:
            raise ValueError("AZURE_ENDPOINT environment variable must be set")
            
        return AzureOpenAI(
            azure_endpoint=azure_endpoint, 
            api_version=self.azure_api_version,
            azure_ad_token_provider=token_provider
        )
    
    def generate_embeddings(self, doc: MyDocument)->MyDocument:
        try:
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc

## LangChain components
## AzureChatbot components

class AzureChatbot:
    def __init__(self, config_file=str, creds_file=str, cert_file=str):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self._setup_chat_model()
        self.memory = ConversationBufferMemory()
        self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
    
    def _setup_chat_model(self):
        try:
            # Create the credential
            if str_to_bool(self.env.get("USE_MANAGED_IDENTITY", "False")):
                credential = DefaultAzureCredential()
            else:
                tenant_id = self.env.get("AZURE_TENANT_ID")
                client_id = self.env.get("AZURE_CLIENT_ID")
                client_secret = self.env.get("AZURE_CLIENT_SECRET")
                
                if not all([tenant_id, client_id, client_secret]):
                    raise ValueError("AZURE_TENANT_ID, AZURE_CLIENT_ID, and AZURE_CLIENT_SECRET must be set")
                
                credential = ClientSecretCredential(
                    tenant_id=tenant_id,
                    client_id=client_id,
                    client_secret=client_secret
                )
            
            # Get the token provider using the credential
            token_provider = get_bearer_token_provider(
                credential,
                "https://cognitiveservices.azure.com/.default"
            )
            
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            
            if not azure_endpoint:
                raise ValueError("AZURE_ENDPOINT must be set")
            
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=token_provider
            )
        except Exception as e:
            logger.error(f"Error setting up chatbot: {e}")
            raise


## New classes for agent-based mapping

class IncidentDimensionResult(BaseModel):
    """Output model for incident dimension mapping."""
    incident_id: str
    selected_dimensions: List[Dict[str, str]]
    
    class Config:
        arbitrary_types_allowed = True

class DimensionInfoTool(BaseTool):
    """Tool for retrieving dimension information."""
    name: str = "dimension_info_tool"
    description: str = "Use this tool to get information about data quality dimensions."
    
    def __init__(self, dimensions_df: pd.DataFrame):
        super().__init__()
        self.dimensions_df = dimensions_df
        
    def _run(self, dimension_name: Optional[str] = None) -> str:
        """Run the tool to get dimension information."""
        try:
            if dimension_name:
                # Get specific dimension info
                dim_rows = self.dimensions_df[self.dimensions_df['Dimension'] == dimension_name]
                if dim_rows.empty:
                    # Try partial match
                    for col in self.dimensions_df.columns:
                        if col == 'Dimension':
                            dim_rows = self.dimensions_df[self.dimensions_df[col].str.contains(dimension_name, case=False, na=False)]
                            if not dim_rows.empty:
                                break
                
                if dim_rows.empty:
                    return f"No dimension found with name '{dimension_name}'"
                else:
                    # Return the first matching dimension's details
                    dim_row = dim_rows.iloc[0]
                    return (
                        f"Dimension: {dim_row['Dimension']}\n"
                        f"Control Type: {dim_row['Control Type']}\n"
                        f"Preventative/Detective: {dim_row['Preventative/Detective']}\n"
                        f"Manual/Automated: {dim_row['Manual/Automated']}\n"
                        f"Data Quality Risks Mitigated: {dim_row['Data Quality Risks Mitigated']}\n"
                        f"Guidance Definition: {dim_row['Guidance Definition']}\n"
                        f"DQ Requirements: {dim_row['DQ Requirements']}\n"
                        f"Logical DQ Rules: {dim_row['Logical DQ Rules']}"
                    )
            else:
                # Return all dimension names
                dimension_list = self.dimensions_df['Dimension'].tolist()
                return f"Available dimensions: {', '.join(dimension_list)}"
        except Exception as e:
            return f"Error retrieving dimension information: {str(e)}"
    
    async def _arun(self, dimension_name: Optional[str] = None) -> str:
        """Run the tool asynchronously."""
        return self._run(dimension_name)

class ListAllDimensionsTool(BaseTool):
    """Tool for listing all available dimensions with details."""
    name: str = "list_all_dimensions_tool"
    description: str = "Use this tool to get detailed information about all data quality dimensions."
    
    def __init__(self, dimensions_df: pd.DataFrame):
        super().__init__()
        self.dimensions_df = dimensions_df
        
    def _run(self) -> str:
        """Run the tool to get all dimensions information."""
        try:
            all_dims_info = ""
            for _, dim_row in self.dimensions_df.iterrows():
                dim_info = (
                    f"=== DIMENSION: {dim_row['Dimension']} ===\n"
                    f"Control Type: {dim_row['Control Type']}\n"
                    f"Preventative/Detective: {dim_row['Preventative/Detective']}\n"
                    f"Manual/Automated: {dim_row['Manual/Automated']}\n"
                    f"Data Quality Risks Mitigated: {dim_row['Data Quality Risks Mitigated']}\n"
                    f"Guidance Definition: {dim_row['Guidance Definition']}\n"
                    f"DQ Requirements: {dim_row['DQ Requirements']}\n"
                    f"Logical DQ Rules: {dim_row['Logical DQ Rules']}\n\n"
                )
                all_dims_info += dim_info
            
            return all_dims_info
        except Exception as e:
            return f"Error retrieving all dimensions information: {str(e)}"
    
    async def _arun(self) -> str:
        """Run the tool asynchronously."""
        return self._run()

class IncidentInfoTool(BaseTool):
    """Tool for retrieving incident information."""
    name: str = "incident_info_tool"
    description: str = "Use this tool to get information about an IT incident."
    
    def __init__(self, incidents_df: pd.DataFrame):
        super().__init__()
        self.incidents_df = incidents_df
        
    def _run(self, incident_id: str) -> str:
        """Run the tool to get incident information."""
        try:
            # Find the incident by ID
            incident_rows = self.incidents_df[self.incidents_df['Id'] == incident_id]
            
            if incident_rows.empty:
                return f"No incident found with ID '{incident_id}'"
            else:
                # Return the incident details
                incident = incident_rows.iloc[0]
                return (
                    f"Incident ID: {incident['Id']}\n"
                    f"Summary: {incident['IT_INCIDENT_SUMMARY']}\n"
                    f"Resolution Details Name: {incident['IT_INCIDENT_RESOLUTION_DETAILS_NAME']}\n"
                    f"Resolution Description: {incident['IT_INCIDENT_RESOLUTION_DESC']}\n"
                    f"Area Category: {incident['IT_INCIDENT_AREA_CATEGORY']}\n"
                    f"Area Subcategory: {incident['IT_INCIDENT_AREA_SUBCATEGORY']}"
                )
        except Exception as e:
            return f"Error retrieving incident information: {str(e)}"
    
    async def _arun(self, incident_id: str) -> str:
        """Run the tool asynchronously."""
        return self._run(incident_id)

class AgentMapper:
    """Main class for agent-based mapping of incidents to dimensions."""
    
    def __init__(self, config_file: str, creds_file: str, cert_file: str, 
                 incidents_file: str, dimensions_file: str, output_file: str):
        self.config_file = config_file
        self.creds_file = creds_file
        self.cert_file = cert_file
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.incidents_file = incidents_file
        self.dimensions_file = dimensions_file
        self.output_file = output_file
        self.incidents_df = None
        self.dimensions_df = None
        self.results_df = None
        self.agent_executor = None
        
        # Initialize the chatbot after environment is set up
        self.chatbot = AzureChatbot(config_file, creds_file, cert_file)
    
    def detect_encoding(self, file_path: str) -> str:
        """Detect file encoding."""
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        return result['encoding']
    
    def load_data(self) -> None:
        """Load incident and dimension data from CSV files."""
        try:
            incidents_encoding = self.detect_encoding(self.incidents_file)
            dimensions_encoding = self.detect_encoding(self.dimensions_file)
            
            self.incidents_df = pd.read_csv(self.incidents_file, encoding=incidents_encoding)
            self.dimensions_df = pd.read_csv(self.dimensions_file, encoding=dimensions_encoding)
            
            logger.info(f"Loaded {len(self.incidents_df)} incidents and {len(self.dimensions_df)} dimensions")
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            raise
    
    def setup_agent(self) -> None:
        """Set up the LangChain agent for mapping incidents to dimensions."""
        try:
            # Create tools
            dimension_info_tool = DimensionInfoTool(self.dimensions_df)
            list_all_dimensions_tool = ListAllDimensionsTool(self.dimensions_df)
            incident_info_tool = IncidentInfoTool(self.incidents_df)
            
            tools = [dimension_info_tool, list_all_dimensions_tool, incident_info_tool]
            
            # Define the agent prompt
            system_message = """
            You are an expert data quality analyst. Your task is to analyze IT incidents and identify which data quality dimensions are most relevant to each incident. 
            
            For a given incident ID, you will:
            1. Use the incident_info_tool to retrieve information about the incident
            2. Use the list_all_dimensions_tool to review all available dimensions
            3. Use the dimension_info_tool to get more details about specific dimensions if needed
            4. Analyze the incident and determine which dimension(s) apply, with detailed reasoning
            
            Your final output should be in JSON format with this structure:
            {
                "incident_id": "The incident ID",
                "selected_dimensions": [
                    {
                        "dimension": "Name of Dimension 1",
                        "reasoning": "Detailed explanation of why this dimension applies to the incident"
                    },
                    {
                        "dimension": "Name of Dimension 2",
                        "reasoning": "Detailed explanation of why this dimension applies to the incident"
                    }
                ]
            }
            
            Be thorough in your analysis. You can select multiple dimensions if appropriate, but only select dimensions that are truly relevant to the incident.
            """
            
            # Create the agent
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_message),
                ("human", "{input}")
            ])
            
            agent = create_react_agent(self.chatbot.llm, tools, prompt)
            self.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
            
            logger.info("Agent setup completed successfully")
        except Exception as e:
            logger.error(f"Error setting up agent: {e}")
            raise
    
    def map_incident_to_dimensions(self, incident_id: str) -> Dict:
        """Map a single incident to appropriate dimensions using the agent."""
        try:
            # Run the agent
            agent_input = f"Analyze incident with ID {incident_id} and determine the most appropriate data quality dimensions."
            result = self.agent_executor.invoke({"input": agent_input})
            output = result.get("output", "")
            
            # Extract JSON from output
            json_match = re.search(r'({.*})', output.replace('\n', ' '), re.DOTALL)
            if not json_match:
                logger.warning(f"Could not extract JSON from agent output for incident {incident_id}")
                return {"incident_id": incident_id, "selected_dimensions": []}
            
            try:
                json_str = json_match.group(1)
                parsed_result = json.loads(json_str)
                
                # Validate the structure
                if "incident_id" not in parsed_result or "selected_dimensions" not in parsed_result:
                    raise ValueError("Missing required fields in result")
                
                return parsed_result
            except json.JSONDecodeError:
                logger.warning(f"Invalid JSON in agent output for incident {incident_id}")
                return {"incident_id": incident_id, "selected_dimensions": []}
        except Exception as e:
            logger.error(f"Error mapping incident {incident_id}: {e}")
            return {"incident_id": incident_id, "selected_dimensions": []}
    
    def process_all_incidents(self) -> None:
        """Process all incidents and create mapping results."""
        results = []
        
        total_incidents = len(self.incidents_df)
        for idx, incident_row in self.incidents_df.iterrows():
            incident_id = incident_row['Id']
            logger.info(f"Processing incident {idx+1}/{total_incidents}: {incident_id}")
            
            mapping_result = self.map_incident_to_dimensions(incident_id)
            selected_dimensions = mapping_result.get("selected_dimensions", [])
            
            if not selected_dimensions:
                # Create a single row with no dimensions
                results.append({
                    "Id": incident_id,
                    "Dimension": "",
                    "Reasoning": "No appropriate dimensions found"
                })
            else:
                # Create a row for each selected dimension
                for dim_info in selected_dimensions:
                    results.append({
                        "Id": incident_id,
                        "Dimension": dim_info.get("dimension", ""),
                        "Reasoning": dim_info.get("reasoning", "")
                    })
        
        self.results_df = pd.DataFrame(results)
        logger.info(f"Created mapping results with {len(self.results_df)} rows")
    
    def save_results(self) -> None:
        """Save mapping results to CSV file."""
        try:
            self.results_df.to_csv(self.output_file, index=False)
            logger.info(f"Results saved to {self.output_file}")
        except Exception as e:
            logger.error(f"Error saving results: {e}")
            raise
    
    def run(self) -> None:
        """Run the complete mapping process."""
        logger.info("Starting incident-to-dimension mapping process")
        
        logger.info("Loading data...")
        self.load_data()
        
        logger.info("Setting up agent...")
        self.setup_agent()
        
        logger.info("Processing incidents...")
        self.process_all_incidents()
        
        logger.info("Saving results...")
        self.save_results()
        
        logger.info("Mapping process completed successfully")


def main():
    """Main entry point for the application."""
    try:
        # Get file paths from environment or use defaults
        env = OSEnv(CONFIG_PATH, CREDS_PATH, CERT_PATH)
        incidents_file = env.get("INCIDENTS_FILE", "incidents.csv")
        dimensions_file = env.get("DIMENSIONS_FILE", "dimensions.csv")
        output_file = env.get("OUTPUT_FILE", "incident_dimension_mapping.csv")
        
        # Create and run the mapper
        mapper = AgentMapper(
            config_file=CONFIG_PATH,
            creds_file=CREDS_PATH,
            cert_file=CERT_PATH,
            incidents_file=incidents_file,
            dimensions_file=dimensions_file,
            output_file=output_file
        )
        
        mapper.run()
        
    except Exception as e:
        logger.error(f"An error occurred in main: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()
