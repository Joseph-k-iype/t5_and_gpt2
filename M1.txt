import os
import sys
import uuid
import json
import logging
import pandas as pd
import networkx as nx
import numpy as np
from typing import Optional, Any, Dict, List, Union, Tuple, Callable
from pathlib import Path
from dotenv import dotenv_values
from azure.identity import DefaultAzureCredential, get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
from pydantic import BaseModel
from langchain.chat_models import AzureChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain, LLMChain
from langchain.docstore import Document as LC_DOCUMENT
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from chromadb.config import Settings
from chromadb.utils import embedding_functions
from collections import namedtuple
import re
from pydantic import BaseModel, ValidationError, field_validator
import traceback

# Import necessary RDF libraries
import rdflib
from rdflib import Graph as RDFGraph, URIRef, Literal, BNode, ConjunctiveGraph
from rdflib.namespace import RDF, RDFS, OWL
from rdflib.plugins.stores.sparqlstore import SPARQLStore, SPARQLUpdateStore

# Import LangGraph for agent orchestration
from langgraph.graph import StateGraph, END

# Additional LangChain components for structured output
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel as LCBaseModel, Field
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

ENV_DIR = "env"
CONFIG_PATH = f"{ENV_DIR}/config.env"
CREDS_PATH = f"{ENV_DIR}/credentials.env"
CERT_PATH = f"{ENV_DIR}/cacert.pem"

Triple = namedtuple("Triple", ["subject", "predicate", "object"])

## utility functions
def is_file_readable(filepath: str)->bool:
    """Check if a file is readable."""
    if not os.path.isfile(filepath) or not os.access(filepath, os.R_OK):
        logger.warning(f"The file '{filepath}' does not exist or is not readable")
        return False
    return True

def str_to_bool(s: str)->bool:
    """Convert a string to a boolean."""
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError(f"Invalid boolean value: {s}")

## OSEnv class
class OSEnv:
    def __init__(self, config_file: str, creds_file: str, certificate_path: str):
        self.var_list = []
        self.credential = None
        
        try:
            self.bulk_set(config_file, True)
        except Exception as e:
            logger.warning(f"Error loading config file: {e}")
            
        try:
            self.bulk_set(creds_file, False)
        except Exception as e:
            logger.warning(f"Error loading creds file: {e}")
            
        try:
            self.set_certificate_path(certificate_path)
        except Exception as e:
            logger.warning(f"Error setting certificate path: {e}")
            
        if str_to_bool(self.get("PROXY_ENABLED", "False")):
            try:
                self.set_proxy()
            except Exception as e:
                logger.warning(f"Error setting proxy: {e}")
        
        if str_to_bool(self.get("SECURED_ENDPOINTS", "False")):
            try:
                self.token = self.get_azure_token()
            except Exception as e:
                logger.warning(f"Error getting Azure token: {e}")
                self.token = None
        else:
            self.token = None
        
        # Initialize credential
        self.credential = self._get_credential()
    
    def _get_credential(self):
        """Get the appropriate Azure credential"""
        try:
            if str_to_bool(self.get("USE_MANAGED_IDENTITY", "False")):
                return DefaultAzureCredential()
            else:
                tenant_id = self.get("AZURE_TENANT_ID")
                client_id = self.get("AZURE_CLIENT_ID")
                client_secret = self.get("AZURE_CLIENT_SECRET")
                
                # Check if required values are present
                if not all([tenant_id, client_id, client_secret]):
                    logger.warning("Missing Azure credentials, returning DefaultAzureCredential")
                    return DefaultAzureCredential()
                    
                return ClientSecretCredential(
                    tenant_id=tenant_id, 
                    client_id=client_id, 
                    client_secret=client_secret
                )
        except Exception as e:
            logger.warning(f"Error creating credential: {e}")
            # Return default credential as fallback
            return DefaultAzureCredential()
    
    def set_certificate_path(self, path: str):
        """Set the certificate path for SSL connections"""
        try:
            if not path:
                logger.warning("Certificate path is empty")
                return
                
            if not os.path.isabs(path):
                path = os.path.abspath(path)
            
            if not is_file_readable(path):
                logger.warning(f"Certificate file not readable: {path}")
                return
            
            self.set("REQUESTS_CA_BUNDLE", path)
            self.set("SSL_CERT_FILE", path)
            self.set("CURL_CA_BUNDLE", path)
            logger.info(f"Set certificate path: {path}")
        except Exception as e:
            logger.error(f"Error setting certificate path: {e}")
    
    def bulk_set(self, dotenvfile: str, print_val: bool = False)->None:
        """Load and set environment variables from a .env file"""
        try:
            if not dotenvfile:
                logger.warning("Dotenv file path is empty")
                return
                
            if not os.path.isabs(dotenvfile):
                dotenvfile = os.path.abspath(dotenvfile)
            
            if not is_file_readable(dotenvfile):
                logger.warning(f"Dotenv file not readable: {dotenvfile}")
                return
            
            temp_dict = dotenv_values(dotenvfile)
            for key, value in temp_dict.items():
                self.set(key, value, print_val)
            
            logger.info(f"Loaded {len(temp_dict)} environment variables from {dotenvfile}")
            del temp_dict
        except Exception as e:
            logger.error(f"Error loading environment variables from {dotenvfile}: {e}")
    
    def set(self, key: str, value: str, print_val: bool = False)->None:
        """Set an environment variable"""
        try:
            if key and value is not None:  # Allow empty string values but not None
                os.environ[key] = value
                if key not in self.var_list:
                    self.var_list.append(key)
                if print_val:
                    logger.info(f"Set env var: {key}: {value}")
        except Exception as e:
            logger.error(f"Error setting environment variable {key}: {e}")
    
    def get(self, key: str, default: Optional[str] = None)->str:
        """Get an environment variable with optional default"""
        try:
            return os.environ.get(key, default)
        except Exception as e:
            logger.error(f"Error getting environment variable {key}: {e}")
            return default
    
    def set_proxy(self) -> None:
        """Configure proxy settings from environment variables"""
        try:
            ad_username = self.get("AD_USERNAME")
            ad_password = self.get("AD_USER_PW")
            proxy_domain = self.get("HTTPS_PROXY_DOMAIN")
            
            if not all([ad_username, ad_password, proxy_domain]):
                logger.warning("Proxy settings are incomplete")
                return
                
            proxy_url = f"https://{ad_username}:{ad_password}@{proxy_domain}"
            self.set("HTTP_PROXY", proxy_url, print_val=False)
            self.set("HTTPS_PROXY", proxy_url, print_val=False)
            
            no_proxy_domains = [
                'cognitiveservices.azure.com',
                'search.windows.net',
                'openai.azure.com',
                'core.windows.net',
                'azurewebsites.net'
            ]
            self.set("NO_PROXY", ",".join(no_proxy_domains), print_val=False)
            logger.info("Proxy settings configured")
        except Exception as e:
            logger.error(f"Error setting proxy: {e}")
    
    def get_azure_token(self) -> str:
        """Get an Azure authentication token"""
        try:
            tenant_id = self.get("AZURE_TENANT_ID")
            client_id = self.get("AZURE_CLIENT_ID")
            client_secret = self.get("AZURE_CLIENT_SECRET")
            
            if not all([tenant_id, client_id, client_secret]):
                logger.warning("Missing Azure credentials for token")
                return None
                
            credential = ClientSecretCredential(
                tenant_id=tenant_id,
                client_id=client_id,
                client_secret=client_secret
            )
            
            token = credential.get_token("https://cognitiveservices.azure.com/.default")
            self.set("AZURE_TOKEN", token.token, print_val=False)
            logger.info("Azure token acquired")
            return token.token
        except Exception as e:
            logger.error(f"Error retrieving Azure token: {e}")
            return None
    
    def list_env_vars(self)->None:
        """List all environment variables that have been set"""
        for var in self.var_list:
            if var in {'AZURE_TOKEN', 'AD_USER_PW', 'AZURE_CLIENT_SECRET'}:
                logger.info(f"{var}: [REDACTED]")
            else:
                logger.info(f"{var}: {os.getenv(var)}")


## embedding class + Document class
class MyDocument(BaseModel):
    """Document model for embeddings"""
    id: str = ""
    text: str = ""
    embedding: List[float] = []
    metadata: Dict[str, Any] = {}


class EmbeddingClient:
    """Client for generating embeddings using Azure OpenAI"""
    def __init__(self, azure_api_version: str = "2023-05-15", embeddings_model: str = "text-embedding-3-large"):
        self.azure_api_version = azure_api_version
        self.embeddings_model = embeddings_model
        self.direct_azure_client = None
        
        try:
            self.direct_azure_client = self._get_direct_azure_client()
        except Exception as e:
            logger.error(f"Error initializing Azure client: {e}")
    
    def _get_direct_azure_client(self):
        """Get an Azure OpenAI client for embeddings"""
        try:
            token_provider = get_bearer_token_provider(
                DefaultAzureCredential(),
                "https://cognitiveservices.azure.com/.default"
            )
            return AzureOpenAI(token_provider, self.azure_api_version)
        except Exception as e:
            logger.error(f"Error getting Azure client: {e}")
            return None
    
    def generate_embeddings(self, doc: MyDocument)->MyDocument:
        """Generate embeddings for the given document"""
        try:
            if not self.direct_azure_client:
                logger.error("Azure client not initialized")
                return doc
                
            response = self.direct_azure_client.embeddings.create(
                model=self.embeddings_model,
                input=doc.text
            ).data[0].embedding
            
            doc.embedding = response
            return doc
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return doc


## AzureChatbot components
class AzureChatbot:
    """Base chatbot using Azure OpenAI"""
    def __init__(self, config_file=CONFIG_PATH, creds_file=CREDS_PATH, cert_file=CERT_PATH):
        self.env = OSEnv(config_file, creds_file, cert_file)
        self.llm = None
        
        try:
            self._setup_chat_model()
            self.memory = ConversationBufferMemory()
            self.conversation = ConversationChain(llm=self.llm, memory=self.memory, verbose=True)
        except Exception as e:
            logger.error(f"Error initializing AzureChatbot: {e}")
    
    def _setup_chat_model(self):
        """Set up the Azure OpenAI chat model"""
        try:
            if not self.env.credential:
                logger.error("No Azure credential available")
                return
                
            token_provider = get_bearer_token_provider(
                self.env.credential,
                "https://cognitiveservices.azure.com/.default"
            )
            
            model_name = self.env.get("MODEL_NAME", "gpt-4o-mini")
            temperature = float(self.env.get("TEMPERATURE", "0.7"))
            max_tokens = int(self.env.get("MAX_TOKENS", "800"))
            api_version = self.env.get("API_VERSION", "2023-05-15")
            azure_endpoint = self.env.get("AZURE_ENDPOINT", "")
            
            if not azure_endpoint:
                logger.error("Azure endpoint not specified")
                return
                
            azure_ad_token_provider = token_provider
            
            self.llm = AzureChatOpenAI(
                model_name=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                api_version=api_version,
                azure_endpoint=azure_endpoint,
                azure_ad_token_provider=azure_ad_token_provider
            )
            
            logger.info(f"Chat model initialized: {model_name} on {azure_endpoint}")
        except Exception as e:
            logger.error(f"Error setting up chat model: {e}")
            raise


# Define models for the GraphRAG components
class QueryUnderstanding(LCBaseModel):
    """Model for understanding the user's query"""
    intent: str = Field(description="The main intent of the user's query")
    entities: List[Dict[str, str]] = Field(description="Key entities mentioned in the query")
    filters: Optional[List[Dict[str, Any]]] = Field(default=None, description="Any filters or constraints specified")
    question_type: str = Field(description="The type of question (e.g., 'factual', 'analytical', 'comparative')")


class SparqlQuery(LCBaseModel):
    """Model for a SPARQL query"""
    query_id: str
    original_query: str
    sparql_query: str
    is_valid: bool = False
    error_message: Optional[str] = None


class QueryResult(LCBaseModel):
    """Model for query results"""
    query_id: str
    original_query: str
    sparql_query: str
    results: List[Dict[str, Any]]
    result_count: int


class BusinessReport(LCBaseModel):
    """Model for the final business report"""
    query_id: str
    original_query: str
    report_text: str
    insights: Optional[List[str]] = None
    recommendations: Optional[List[str]] = None


class QueryValidation(LCBaseModel):
    """Model for query validation results"""
    is_valid: bool = Field(description="Whether the query is valid")
    corrected_query: Optional[str] = Field(default=None, description="The corrected query if the original was invalid")
    explanation: str = Field(description="Explanation of the errors found or why the query is valid")


class WorkflowState(BaseModel):
    """State object for the agent workflow with reasoning traces"""
    query_id: str
    original_query: str
    query_understanding: Optional[Dict[str, Any]] = None
    understanding_reasoning: Optional[str] = None
    sparql_query: Optional[str] = None
    sparql_generation_reasoning: Optional[str] = None
    validation_result: Optional[Dict[str, Any]] = None
    validation_reasoning: Optional[str] = None
    query_results: Optional[List[Dict[str, Any]]] = None
    reporting_reasoning: Optional[str] = None
    final_report: Optional[str] = None
    
    def __getitem__(self, key):
        """Allow dict-like access to state properties"""
        try:
            return getattr(self, key)
        except AttributeError:
            # Fallback for when the attribute doesn't exist
            return None
    
    def __setitem__(self, key, value):
        """Allow dict-like setting of state properties"""
        setattr(self, key, value)
    
    def get(self, key, default=None):
        """Dict-like get method with default"""
        try:
            value = getattr(self, key)
            return value if value is not None else default
        except AttributeError:
            return default
            
    class Config:
        """Pydantic configuration"""
        extra = "allow"  # Allow extra fields


# Debug helper function
def debug_state(state):
    """Debug helper to print state information"""
    logger.info(f"State type: {type(state)}")
    
    if hasattr(state, "__dict__"):
        logger.info(f"State attributes: {state.__dict__}")
    
    if hasattr(state, "__getitem__"):
        try:
            if hasattr(state, "keys"):
                logger.info(f"State keys: {list(state.keys())}")
            else:
                logger.info("State is subscriptable but has no keys method")
        except Exception as e:
            logger.info(f"Error getting keys: {e}")
    
    # Try to access specific attributes/keys
    try:
        if hasattr(state, "final_report"):
            logger.info(f"state.final_report exists: {state.final_report}")
        else:
            logger.info("state.final_report doesn't exist as attribute")
    except Exception as e:
        logger.info(f"Error accessing final_report attribute: {e}")
    
    try:
        logger.info(f"state['final_report'] access: {state['final_report']}")
    except Exception as e:
        logger.info(f"Error accessing final_report by subscription: {e}")
    
    return state  # Return the state for further processing


# Main GraphRAG chatbot class
class GraphRAGChatbot:
    """GraphRAG chatbot for querying knowledge graphs using natural language"""
    def __init__(self, 
                 config_file=CONFIG_PATH, 
                 creds_file=CREDS_PATH, 
                 cert_file=CERT_PATH, 
                 ontology_path=None):
        """
        Initialize the GraphRAG chatbot
        
        Args:
            config_file (str): Path to the config file
            creds_file (str): Path to the credentials file
            cert_file (str): Path to the certificate file
            ontology_path (str, optional): Path to the ontology file. If None, will use the value from
                                           ONTOLOGY_PATH environment variable or default to "ontology.owl"
        """
        try:
            # Initialize the base chatbot for Azure OpenAI integration
            self.base_chatbot = AzureChatbot(config_file, creds_file, cert_file)
            self.env = self.base_chatbot.env
            self.llm = self.base_chatbot.llm
            
            if not self.llm:
                logger.error("LLM not initialized - check Azure configuration")
                raise ValueError("LLM not initialized")
            
            # Initialize RDF components
            # Determine the ontology path from parameters or environment variables
            self.ontology_path = ontology_path or self.env.get("ONTOLOGY_PATH", "ontology.owl")
            logger.info(f"Using ontology file: {self.ontology_path}")
            
            # Load ontology and setup SPARQL endpoint
            self.ontology_graph = self._load_ontology()
            self.sparql_store, self.sparql_graph = self._setup_sparql_endpoint()
            
            # Initialize conversation memory
            self.memory = ConversationBufferMemory()
            
            # Initialize the agents first (this is key to fixing the issue)
            self.query_understanding_agent = self._setup_query_understanding_agent()
            self.ontology_agent = self._setup_ontology_agent()
            self.validation_agent = self._setup_query_validation_agent()
            self.reporting_agent = self._setup_reporting_agent()
            
            # Setup the agent workflow after the agents are initialized
            self.agent_workflow = self._setup_agent_workflow()
            
            logger.info("GraphRAG chatbot initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing GraphRAG chatbot: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _load_ontology(self) -> RDFGraph:
        """Load the ontology from the file (supports OWL/XML, TTL, N3, etc.)"""
        try:
            # Check if the ontology file exists
            if not is_file_readable(self.ontology_path):
                logger.warning(f"Ontology file not found or not readable: {self.ontology_path}")
                # Return an empty graph instead of raising an exception
                return RDFGraph()
            
            ontology = RDFGraph()
            
            # Determine file format based on extension
            file_ext = os.path.splitext(self.ontology_path)[1].lower()
            
            if file_ext == '.ttl':
                format_name = 'turtle'
            elif file_ext == '.n3':
                format_name = 'n3'
            elif file_ext == '.nt':
                format_name = 'nt'
            elif file_ext == '.jsonld':
                format_name = 'json-ld'
            elif file_ext in ['.owl', '.rdf', '.xml']:
                format_name = 'xml'
            else:
                # If extension is not recognized, let rdflib try to guess
                format_name = None
            
            try:
                # Parse the ontology file
                ontology.parse(self.ontology_path, format=format_name)
                logger.info(f"Loaded ontology from {self.ontology_path} with {len(ontology)} triples (format: {format_name or 'auto-detected'})")
            except Exception as format_error:
                # If parsing fails with the detected format, try without specifying a format
                logger.warning(f"Error parsing ontology with format {format_name}: {format_error}")
                logger.info(f"Trying to parse ontology without specifying format")
                ontology = RDFGraph()
                ontology.parse(self.ontology_path)
                logger.info(f"Loaded ontology with {len(ontology)} triples using auto-detection")
            
            return ontology
        except Exception as e:
            logger.error(f"Error loading ontology: {e}")
            logger.error(traceback.format_exc())
            # Return an empty graph instead of raising an exception
            return RDFGraph()
    
    def _setup_sparql_endpoint(self):
        """Setup connection to the SPARQL endpoint with SPARQLUpdateStore and ConjunctiveGraph"""
        try:
            endpoint_url = self.env.get("SPARQL_ENDPOINT_URL")
            if not endpoint_url:
                logger.warning("SPARQL endpoint URL not configured in environment variables")
                # Create a default in-memory store and graph
                default_store = rdflib.plugin.get('Memory', rdflib.store.Store)()
                default_graph = rdflib.ConjunctiveGraph(store=default_store)
                return default_store, default_graph
            
            # Hard-coded Basic auth credentials
            username = "abc"
            password = "124"
            
            logger.info(f"Connecting to SPARQL endpoint: {endpoint_url}")
            
            try:
                # Use default SPARQLUpdateStore
                store = rdflib.plugins.stores.sparqlstore.SPARQLUpdateStore()
                
                # Open the store with the endpoint URL and credentials
                store.open((endpoint_url, endpoint_url))
                
                # Set authentication directly
                store.setCredentials(username, password)
                
                # Create a ConjunctiveGraph using this store
                graph = rdflib.ConjunctiveGraph(store=store)
                
                # Test connection with a simple query
                try:
                    test_query = "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 1"
                    logger.info(f"Testing connection with query: {test_query}")
                    results = list(graph.query(test_query))
                    logger.info(f"Successfully connected to SPARQL endpoint: {endpoint_url}")
                    logger.info(f"Test query returned {len(results)} results")
                except Exception as e:
                    logger.warning(f"Test query failed but continuing: {str(e)}")
                
                return store, graph
            except Exception as e:
                logger.error(f"Error with SPARQLUpdateStore: {e}")
                
                # Try alternative connection method with SPARQLStore
                logger.info("Trying alternative connection method with SPARQLStore")
                alt_store = SPARQLStore(endpoint_url, auth=(username, password))
                alt_graph = ConjunctiveGraph(store=alt_store)
                return alt_store, alt_graph
                
        except Exception as e:
            logger.error(f"Error connecting to SPARQL endpoint: {e}")
            logger.error(traceback.format_exc())
            
            # Return a default in-memory store and graph
            default_store = rdflib.plugin.get('Memory', rdflib.store.Store)()
            default_graph = rdflib.ConjunctiveGraph(store=default_store)
            return default_store, default_graph
    
    def _setup_query_understanding_agent(self):
        """Create an agent for understanding the user query with chain-of-thought reasoning"""
        try:
            # Define the system prompt with chain-of-thought instructions
            system_prompt = """You are an expert in understanding user queries about data from knowledge graphs.
            Your task is to analyze the user's question and break down your understanding using chain-of-thought reasoning.
            
            Follow these steps in your analysis:
            1. First, identify the core question type (e.g., factual lookup, comparative analysis, relationship exploration)
            2. Next, identify all entities mentioned in the question and their types
            3. Determine any specific attributes or properties being requested
            4. Identify any filters, constraints, or conditions (temporal, numerical, categorical)
            5. Recognize any sorting, grouping, or aggregation requests
            6. Determine the overall intent of the query
            
            For each step, explicitly state your reasoning before drawing conclusions.
            
            IMPORTANT: Your final output MUST be a valid JSON object with these exact keys:
            {
              "intent": "string describing the main intent",
              "entities": [{"type": "entity_type", "name": "entity_name"}],
              "filters": [{"property": "property_name", "operator": "comparison_operator", "value": "filter_value"}],
              "question_type": "factual/analytical/comparative/etc"
            }
            
            Make sure your output is properly formatted as JSON.
            """
            
            # Create the prompt template
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=system_prompt),
                HumanMessage(content="{query}")
            ])
            
            # Use a string output parser first, then handle JSON parsing manually
            string_parser = StrOutputParser()
            
            # Create a safer chain that handles parsing errors
            def safe_parse_json(text):
                try:
                    # Try to extract JSON from the response if it's embedded
                    import re
                    import json
                    
                    # Look for JSON blocks in the text
                    json_match = re.search(r'\{.*\}', text, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        parsed = json.loads(json_str)
                        
                        # Ensure required fields exist
                        if "intent" not in parsed:
                            parsed["intent"] = "general_query"
                        if "entities" not in parsed:
                            parsed["entities"] = []
                        if "question_type" not in parsed:
                            parsed["question_type"] = "factual"
                        
                        return parsed
                    else:
                        # Fallback if no JSON found
                        logger.warning(f"No JSON found in: {text[:100]}...")
                        return {
                            "intent": "general_query",
                            "entities": [],
                            "filters": [],
                            "question_type": "factual"
                        }
                except Exception as e:
                    logger.error(f"Error parsing JSON: {e}")
                    # Return a default structure
                    return {
                        "intent": "general_query",
                        "entities": [],
                        "filters": [],
                        "question_type": "factual"
                    }
            
            # Create the chain - updated to use LangChain's modern format
            understanding_chain = (
                prompt 
                | self.llm 
                | string_parser 
                | safe_parse_json
            )
            
            return understanding_chain
        except Exception as e:
            logger.error(f"Error setting up query understanding agent: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def _setup_ontology_agent(self):
        """Create an agent for converting natural language to SPARQL based on the ontology"""
        try:
            if not self.ontology_graph:
                logger.warning("Ontology graph not loaded")
                
            # Extract ontology information
            ontology_classes = []
            ontology_properties = []
            ontology_relationships = []
            
            try:
                # Extract classes with descriptions and labels
                for cls in self.ontology_graph.subjects(RDF.type, OWL.Class):
                    if isinstance(cls, URIRef):
                        label = self.ontology_graph.value(cls, RDFS.label)
                        description = self.ontology_graph.value(cls, RDFS.comment)
                        
                        label_str = str(label) if label else cls.split('#')[-1] if '#' in str(cls) else str(cls)
                        desc_str = f" - {str(description)}" if description else ""
                        
                        ontology_classes.append(f"Class: {label_str} ({cls}){desc_str}")
                
                # Extract datatype properties with domains, ranges, and descriptions
                for prop in self.ontology_graph.subjects(RDF.type, OWL.DatatypeProperty):
                    if isinstance(prop, URIRef):
                        label = self.ontology_graph.value(prop, RDFS.label)
                        description = self.ontology_graph.value(prop, RDFS.comment)
                        domain = self.ontology_graph.value(prop, RDFS.domain)
                        range_val = self.ontology_graph.value(prop, RDFS.range)
                        
                        label_str = str(label) if label else prop.split('#')[-1] if '#' in str(prop) else str(prop)
                        domain_str = domain.split('#')[-1] if domain and '#' in str(domain) else "Unspecified"
                        range_str = range_val.split('#')[-1] if range_val and '#' in str(range_val) else "Unspecified"
                        desc_str = f" - {str(description)}" if description else ""
                        
                        ontology_properties.append(f"DatatypeProperty: {label_str} ({prop}) - Domain: {domain_str}, Range: {range_str}{desc_str}")
                
                # Extract object properties with domains, ranges, and descriptions
                for prop in self.ontology_graph.subjects(RDF.type, OWL.ObjectProperty):
                    if isinstance(prop, URIRef):
                        label = self.ontology_graph.value(prop, RDFS.label)
                        description = self.ontology_graph.value(prop, RDFS.comment)
                        domain = self.ontology_graph.value(prop, RDFS.domain)
                        range_val = self.ontology_graph.value(prop, RDFS.range)
                        
                        label_str = str(label) if label else prop.split('#')[-1] if '#' in str(prop) else str(prop)
                        domain_str = domain.split('#')[-1] if domain and '#' in str(domain) else "Unspecified"
                        range_str = range_val.split('#')[-1] if range_val and '#' in str(range_val) else "Unspecified"
                        desc_str = f" - {str(description)}" if description else ""
                        
                        relationship = f"ObjectProperty: {label_str} ({prop}) - Domain: {domain_str}, Range: {range_str}{desc_str}"
                        ontology_properties.append(relationship)
                        ontology_relationships.append(relationship)
            except Exception as extraction_err:
                logger.warning(f"Error extracting ontology information: {extraction_err}")
                
            # If no classes or properties were found, create some example data
            if not ontology_classes:
                ontology_classes = ["Class: Example (http://example.org/ontology#Example)"]
            
            if not ontology_properties:
                ontology_properties = ["Property: exampleProperty (http://example.org/ontology#exampleProperty) - Domain: Example, Range: xsd:string"]
            
            if not ontology_relationships:
                ontology_relationships = ["ObjectProperty: relatedTo (http://example.org/ontology#relatedTo) - Domain: Example, Range: Example"]
            
            # Define the system prompt with ontology information and chain of thought instructions
            system_prompt = f"""You are an expert in Semantic Web technologies, RDF, OWL ontologies, and SPARQL query generation.
            Your task is to convert a natural language query into a precise, valid SPARQL query by reasoning step-by-step through the mapping process.
            
            Here's the ontology structure:
            
            CLASSES:
            {chr(10).join(ontology_classes[:20])}
            ... {len(ontology_classes) - 20 if len(ontology_classes) > 20 else 0} more classes
            
            PROPERTIES:
            {chr(10).join(ontology_properties[:20])}
            ... {len(ontology_properties) - 20 if len(ontology_properties) > 20 else 0} more properties
            
            KEY RELATIONSHIPS:
            {chr(10).join(ontology_relationships[:15])}
            ... {len(ontology_relationships) - 15 if len(ontology_relationships) > 15 else 0} more relationships
            
            Follow these chain-of-thought reasoning steps when converting the query understanding to SPARQL:
            
            1. IDENTIFY CORE ENTITIES:
               - Determine the main classes from the ontology that correspond to entities in the query
               - Map each entity to its corresponding class URI
            
            2. IDENTIFY PROPERTIES:
               - For each attribute or relationship mentioned in the query, identify the corresponding properties
               - Determine if they are datatype properties or object properties
               - Map property names to their corresponding property URIs
            
            3. CONSTRUCT TRIPLE PATTERNS:
               - Create the basic triple patterns needed to represent the query
               - Consider subject-predicate-object patterns for each relationship
            
            4. ADD CONSTRAINTS:
               - Translate filters, conditions, or constraints into SPARQL FILTER clauses
               - Handle any comparison operators (equals, greater than, less than, etc.)
            
            5. DETERMINE QUERY TYPE:
               - Decide if this should be a SELECT, ASK, CONSTRUCT, or DESCRIBE query
               - For SELECT queries, identify the specific variables to include in the result
            
            6. ADD MODIFIERS:
               - Add any ORDER BY, GROUP BY, LIMIT, or OFFSET clauses needed
               - Consider aggregation functions (COUNT, SUM, AVG, etc.) if needed
            
            7. FINALIZE PREFIXES:
               - Add all necessary namespace prefixes based on the URIs used

            8. CHECK FOR COMMON ISSUES:
               - Verify no typos in property or class URIs
               - Ensure all variables are properly defined
               - Confirm proper syntax for FILTER expressions
            
            After completing your step-by-step reasoning, provide ONLY the final SPARQL query enclosed in triple backticks with sparql language marker, like this:
            
            ```sparql
            PREFIX ex: <http://example.org/>
            SELECT ?something WHERE {
              ?something a ex:Class .
            }
            ```
            
            Do not include any text before or after the SPARQL query block. The query must be syntactically correct according to SPARQL 1.1.
            """
            
            # Create a specialized prompt template that shows the query understanding but requests ONLY the SPARQL query
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=system_prompt),
                HumanMessage(content="Query Understanding: {query_understanding}\n\nGenerate a SPARQL query to answer this question, showing your reasoning first and then ONLY the final SPARQL query in a code block.")
            ])
            
            # Define a function to extract only the SPARQL query from the response
            def extract_sparql_query(text):
                # Look for the SPARQL query in triple backticks
                sparql_match = re.search(r"```sparql\n(.*?)\n```", text, re.DOTALL)
                if sparql_match:
                    return sparql_match.group(1).strip()
                
                # If no sparql tag, look for any code block
                code_match = re.search(r"```\n?(.*?)\n?```", text, re.DOTALL)
                if code_match:
                    return code_match.group(1).strip()
                
                # If no code block at all, extract what looks like a SPARQL query
                query_pattern = r"(PREFIX\s+[\w:]+\s*<[^>]+>\s*)*(SELECT|ASK|CONSTRUCT|DESCRIBE).*?WHERE\s*\{.*?\}"
                query_match = re.search(query_pattern, text, re.DOTALL | re.IGNORECASE)
                if query_match:
                    return query_match.group(0).strip()
                
                # If nothing can be found, provide a safe fallback
                logger.warning("Could not extract SPARQL query from LLM response")
                return "SELECT * WHERE { ?s ?p ?o } LIMIT 10"
            
            # Create the chain with the extraction step
            sparql_generation_chain = prompt | self.llm | StrOutputParser() | extract_sparql_query
            
            return sparql_generation_chain
        except Exception as e:
            logger.error(f"Error setting up ontology agent: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def _setup_query_validation_agent(self):
        """Create an agent for validating and fixing SPARQL queries with chain-of-thought reasoning"""
        try:
            # Enhanced system prompt with chain-of-thought instructions
            system_prompt = """You are an expert in SPARQL query syntax, validation, and optimization with deep knowledge of the SPARQL 1.1 specification.
            Your task is to validate the provided SPARQL query using chain-of-thought reasoning to identify and fix any issues.

            Follow these reasoning steps when validating the query:

            1. ANALYZE PREFIXES:
               - Check if all required namespace prefixes are declared
               - Verify that prefixes are correctly formatted
               - Ensure each prefix used in the query body is defined
            
            2. VALIDATE TRIPLE PATTERNS:
               - Examine each triple pattern for correct subject-predicate-object structure
               - Check variable naming conventions (should start with ? or $)
               - Verify URI formatting (should be enclosed in <> or use defined prefixes)
               - Ensure literals have appropriate datatype or language tags if needed
            
            3. REVIEW QUERY STRUCTURE:
               - Validate the query structure for the specific query form (SELECT, ASK, CONSTRUCT, DESCRIBE)
               - Check for required components (e.g., WHERE clause)
               - Ensure proper nesting of OPTIONAL, UNION, FILTER clauses
               - Verify matching of opening and closing braces and parentheses
            
            4. EXAMINE FILTER EXPRESSIONS:
               - Validate the syntax of FILTER expressions
               - Check for correct use of operators (=, !=, <, >, etc.)
               - Verify proper escaping in strings
               - Ensure type compatibility in comparisons
            
            5. VALIDATE SOLUTION MODIFIERS:
               - Check syntax of any ORDER BY, GROUP BY, HAVING, LIMIT, or OFFSET clauses
               - Verify that variables used in these clauses are defined in the query
               - Ensure aggregation functions have proper syntax
            
            6. IDENTIFY SEMANTIC ISSUES:
               - Check for common logical errors that might produce empty results
               - Look for redundant patterns
               - Identify overly complex constructions that could be simplified
            
            7. OPTIMIZE IF NEEDED:
               - Suggest optimizations for better performance
               - Identify any inefficient patterns that could be rewritten
            
            After completing your analysis, I need you to provide a JSON object with the following structure:
            {
              "is_valid": true/false,
              "corrected_query": "fixed query if needed",
              "explanation": "explanation of the issues or why the query is valid"
            }
            
            IMPORTANT: Your final output MUST be a valid JSON object using exactly these keys.
            """
            
            # Enhanced prompt template that shows the query in formatted code block
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=system_prompt),
                HumanMessage(content="SPARQL Query to validate:\n\n```sparql\n{sparql_query}\n```\n\nPlease analyze this query thoroughly using the chain-of-thought process.")
            ])
            
            # Use a string output parser first, then handle JSON parsing manually
            string_parser = StrOutputParser()
            
            # Create a safer parser function
            def safe_parse_validation(text):
                try:
                    # Try to extract JSON from the response if it's embedded
                    import re
                    import json
                    
                    # Look for JSON blocks in the text
                    json_match = re.search(r'\{.*\}', text, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        parsed = json.loads(json_str)
                        
                        # Ensure required fields exist
                        if "is_valid" not in parsed:
                            parsed["is_valid"] = True  # Default to valid
                        if "explanation" not in parsed:
                            parsed["explanation"] = "Query appears to be valid."
                        
                        # Ensure corrected_query is present if not valid
                        if not parsed["is_valid"] and "corrected_query" not in parsed:
                            parsed["corrected_query"] = ""
                        
                        return parsed
                    else:
                        # If no JSON found, try to determine validity from text
                        is_valid = "valid" in text.lower() and not ("not valid" in text.lower() or "invalid" in text.lower())
                        
                        # Create default validation result
                        return {
                            "is_valid": is_valid,
                            "explanation": text[:500],  # Use first 500 chars as explanation
                            "corrected_query": "" if is_valid else ""
                        }
                except Exception as e:
                    logger.error(f"Error parsing validation JSON: {e}")
                    # Return a default structure
                    return {
                        "is_valid": True,  # Default to valid to allow execution
                        "explanation": f"Error parsing validation result: {str(e)}",
                        "corrected_query": ""
                    }
            
            # Create the chain - using LangChain's modern format
            validation_chain = (
                prompt 
                | self.llm 
                | string_parser 
                | safe_parse_validation
            )
            
            return validation_chain
        except Exception as e:
            logger.error(f"Error setting up query validation agent: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def _setup_reporting_agent(self):
        """Create an agent for generating business-friendly reports from query results with chain-of-thought reasoning"""
        try:
            system_prompt = """You are an expert in data analysis, knowledge graph interpretation, and business communication.
            Your task is to create a clear, concise, and business-friendly report based on the results of a SPARQL query through careful chain-of-thought reasoning.
            
            Follow these analytical steps when generating your report:
            
            1. UNDERSTAND THE QUESTION CONTEXT:
               - Review the original question to understand what the user was seeking
               - Identify the key business metrics or relationships they wanted to know about
               - Consider any implicit needs or context not directly stated in the question
            
            2. ANALYZE THE QUERY RESULTS:
               - Examine all returned data systematically
               - Identify patterns, trends, outliers, or anomalies
               - Note the completeness of results (any missing data or null values?)
               - Verify the data makes logical sense based on domain knowledge
            
            3. EXTRACT KEY INSIGHTS:
               - Determine the most important findings that directly answer the question
               - Identify secondary insights that provide additional value
               - Note any unexpected findings that might be of interest
               - Consider the business implications of these insights
            
            4. STRUCTURE YOUR REPORT:
               - Start with a clear, direct answer to the original question
               - Present key findings in order of importance
               - Group related information logically
               - Use appropriate formatting (lists, sections) for readability
               - Include the SPARQL query used in a code block
            
            5. TRANSLATE TECHNICAL DETAILS:
               - Convert technical terminology into business language
               - Explain complex relationships in simple terms
               - Provide context for any metrics or measurements
               - Use analogies or comparisons when helpful
            
            6. FORMULATE RECOMMENDATIONS (IF APPROPRIATE):
               - Consider what actions could be taken based on these insights
               - Suggest potential next steps or further analyses
               - Frame recommendations in terms of business impact
            
            The final report should be:
            - Accessible to non-technical stakeholders while still including the technical SPARQL query
            - Free of excessive technical jargon but transparent about the query used
            - Precise and accurate regarding the data
            - Well-structured with clear sections (including a section for the query used)
            - Tailored to the specific question and data, not following a fixed template
            - Action-oriented where appropriate
            
            IMPORTANT: Your report should be in Markdown format with proper headings and sections.
            Always include the SPARQL query used in a ```sparql code block, even if it's a complex query.
            
            After completing your analysis, provide a polished business report that someone could present to executives or include in a business document.
            """
            
            # Create the prompt template with added query details for context and handling large results
            prompt = ChatPromptTemplate.from_messages([
                SystemMessage(content=system_prompt),
                HumanMessage(content="""
                Original Question: {original_query}
                
                Query Results Summary:
                {result_summary}
                
                SPARQL Query Used:
                ```sparql
                {sparql_query}
                ```
                
                Query Results (Sample):
                {query_results}
                
                Please generate a comprehensive business-friendly report that answers the original question based on these results.
                Include the SPARQL query used in a code block as part of your report.
                Tailor your report specifically to the question asked and the data returned - don't use a fixed template.
                Use proper Markdown formatting with headers, lists, and code blocks as appropriate.
                """)
            ])
            
            # Create the chain - using LangChain's modern format, with additional error handling
            def safe_reporting_chain(inputs):
                try:
                    # Process the report content
                    report_content = self.llm.invoke(prompt.format(**inputs)).content
                    
                    # Ensure the report includes the SPARQL query if not already present
                    if "```sparql" not in report_content and inputs.get("sparql_query"):
                        report_content += f"""
                        
                        ## SPARQL Query Used
                        ```sparql
                        {inputs.get('sparql_query')}
                        ```
                        """
                    
                    return report_content
                except Exception as e:
                    logger.error(f"Error in reporting chain: {e}")
                    # Generate a minimal report on failure
                    return f"""# Query Results

## Original Question
{inputs.get('original_query', 'Unknown query')}

## Analysis
I encountered an error analyzing the full results: {str(e)}

## SPARQL Query Used
```sparql
{inputs.get('sparql_query', 'Query not available')}
```

## Results Summary
{inputs.get('result_summary', 'Result summary not available')}
"""
            
            return safe_reporting_chain
        except Exception as e:
            logger.error(f"Error setting up reporting agent: {e}")
            logger.error(traceback.format_exc())
            return None
    
    def _extract_sparql_from_reasoning(self, text: str) -> str:
        """Helper method to extract the SPARQL query from a reasoning text"""
        try:
            # Try to find a SPARQL query enclosed in triple backticks with sparql language marker
            sparql_blocks = re.findall(r"```sparql\n(.*?)\n```", text, re.DOTALL)
            if sparql_blocks:
                return sparql_blocks[0].strip()
            
            # Try to find any code block that might contain SPARQL
            code_blocks = re.findall(r"```(.*?)```", text, re.DOTALL)
            if code_blocks:
                for block in code_blocks:
                    # Check if block looks like SPARQL (contains SELECT/ASK/CONSTRUCT/DESCRIBE and WHERE)
                    if re.search(r"(SELECT|ASK|CONSTRUCT|DESCRIBE).*?WHERE", block, re.DOTALL | re.IGNORECASE):
                        return block.strip()
            
            # If no clear SPARQL blocks found, look for patterns that suggest a SPARQL query
            # More specific pattern to extract just the SPARQL query
            prefix_pattern = r"(PREFIX\s+[\w:]+\s*<[^>]+>\s*)*"
            select_pattern = r"(SELECT|ASK|CONSTRUCT|DESCRIBE)\s+.*?WHERE\s*\{.*?\}"
            potential_query = re.search(f"{prefix_pattern}{select_pattern}", text, re.DOTALL | re.IGNORECASE)
            
            if potential_query:
                return potential_query.group(0).strip()
            
            # Try a more focused approach for just extracting the query part
            select_part = re.search(r"(SELECT|ASK|CONSTRUCT|DESCRIBE).*?WHERE\s*\{", text, re.DOTALL | re.IGNORECASE)
            if select_part:
                # Find the balanced closing brace for the WHERE clause
                start_idx = select_part.start()
                open_braces = 0
                close_idx = -1
                
                for i in range(select_part.end() - 1, len(text)):
                    if text[i] == '{':
                        open_braces += 1
                    elif text[i] == '}':
                        if open_braces > 0:
                            open_braces -= 1
                        else:
                            close_idx = i + 1
                            break
                
                if close_idx > 0:
                    # Extract complete query including any prefixes
                    query_text = text[start_idx:close_idx].strip()
                    
                    # Try to find prefixes before the SELECT
                    prefix_match = re.search(r"(PREFIX\s+[\w:]+\s*<[^>]+>\s*)+", text[:start_idx], re.IGNORECASE)
                    if prefix_match:
                        query_text = prefix_match.group(0) + "\n" + query_text
                    
                    return query_text
            
            # If all else fails, look for the most basic structure
            basic_query = re.search(r"SELECT\s+\?[\w\s]+\s+WHERE\s*\{\s*[\s\S]*?\}", text, re.IGNORECASE)
            if basic_query:
                return basic_query.group(0).strip()
            
            # If all extraction methods fail
            logger.warning("Could not extract clean SPARQL query from reasoning text")
            return "SELECT * WHERE { ?s ?p ?o } LIMIT 10"  # Return a safe fallback query
        except Exception as e:
            logger.error(f"Error extracting SPARQL query: {e}")
            return "SELECT * WHERE { ?s ?p ?o } LIMIT 10"  # Return a safe fallback query on error
    
    def _execute_sparql_query(self, query: str) -> List[Dict[str, Any]]:
        """Execute a SPARQL query against the endpoint and return results"""
        try:
            if not query or not query.strip():
                logger.error("Empty query provided")
                return [{"error": "Empty query provided"}]
            
            # Sanitize the query - ensure it's a valid SPARQL query
            # Remove any markdown code block markers
            query = re.sub(r'```sparql|```', '', query).strip()
            
            # Log the query
            logger.info(f"Executing SPARQL query: {query}")
            
            # Execute the query using the graph with the connected store
            try:
                results = self.sparql_graph.query(query)
            except Exception as query_error:
                logger.error(f"Error executing SPARQL query: {query_error}")
                
                # Try to handle common errors
                if "syntax error" in str(query_error).lower():
                    # If it's a syntax error, try to clean up the query further
                    clean_query = self._try_fix_common_syntax_errors(query)
                    if clean_query != query:
                        logger.info(f"Attempting to execute fixed query: {clean_query}")
                        try:
                            results = self.sparql_graph.query(clean_query)
                        except Exception as retry_error:
                            return [{"error": f"SPARQL syntax error: {str(retry_error)}"}]
                    else:
                        return [{"error": f"SPARQL syntax error: {str(query_error)}"}]
                else:
                    return [{"error": f"Query execution error: {str(query_error)}"}]
            
            # Process results into a dictionary format
            processed_results = []
            
            # Handle empty results
            if not results or len(results) == 0:
                logger.info("Query returned no results")
                return []
            
            # Process each result row
            for row in results:
                row_dict = {}
                
                # Handle different result formats
                if hasattr(results, 'vars') and results.vars:
                    # Standard query results with variables
                    for var_name, var_value in zip(results.vars, row):
                        if var_value is None:
                            row_dict[str(var_name)] = {"value": None}
                            continue
                            
                        if isinstance(var_value, URIRef):
                            # Get label if available
                            label = self.ontology_graph.value(var_value, RDFS.label)
                            if label:
                                row_dict[str(var_name)] = {"uri": str(var_value), "label": str(label)}
                            else:
                                row_dict[str(var_name)] = {"uri": str(var_value)}
                        elif isinstance(var_value, Literal):
                            datatype = var_value.datatype.n3() if var_value.datatype else None
                            row_dict[str(var_name)] = {"value": str(var_value), "datatype": datatype}
                        else:
                            row_dict[str(var_name)] = {"value": str(var_value)}
                else:
                    # Handle boolean results (ASK queries)
                    if isinstance(row, bool):
                        row_dict["result"] = {"value": row}
                    else:
                        # Handle other result formats
                        row_dict["result"] = {"value": str(row)}
                
                processed_results.append(row_dict)
            
            logger.info(f"Query returned {len(processed_results)} results")
            return processed_results
            
        except Exception as e:
            logger.error(f"Error executing SPARQL query: {e}")
            logger.error(traceback.format_exc())
            return [{"error": str(e)}]
    
    def _try_fix_common_syntax_errors(self, query: str) -> str:
        """Try to fix common SPARQL syntax errors"""
        try:
            # Check for unclosed braces
            open_braces = query.count('{')
            close_braces = query.count('}')
            
            fixed_query = query
            
            # Add missing closing braces
            if open_braces > close_braces:
                fixed_query += ' ' + '}' * (open_braces - close_braces)
                logger.info(f"Added {open_braces - close_braces} missing closing braces")
                
            # Check for missing PREFIX declarations
            if 'PREFIX' not in query and ':' in query:
                # Extract common namespace prefixes used without declaration
                prefix_matches = re.findall(r'([a-zA-Z0-9]+):', query)
                if prefix_matches:
                    common_prefixes = {
                        'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
                        'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',
                        'owl': 'http://www.w3.org/2002/07/owl#',
                        'xsd': 'http://www.w3.org/2001/XMLSchema#'
                    }
                    
                    prefix_str = ""
                    for prefix in set(prefix_matches):
                        if prefix.lower() in common_prefixes:
                            prefix_str += f"PREFIX {prefix}: <{common_prefixes[prefix.lower()]}>\n"
                    
                    if prefix_str:
                        fixed_query = prefix_str + fixed_query
                        logger.info(f"Added missing PREFIX declarations")
            
            # Check for missing period between triple patterns
            pattern_missing_period = r'(\?[a-zA-Z0-9_]+\s+[a-zA-Z0-9_:]+\s+[^\s.;]+)\s+(\?[a-zA-Z0-9_]+)'
            if re.search(pattern_missing_period, fixed_query):
                fixed_query = re.sub(pattern_missing_period, r'\1 . \2', fixed_query)
                logger.info("Added missing periods between triple patterns")
            
            # Fix common typos
            typos = {
                'SELCET': 'SELECT',
                'WHER ': 'WHERE ',
                'WEHRE': 'WHERE',
                'FLITER': 'FILTER',
                'OPTINAL': 'OPTIONAL',
                'GRPOUP BY': 'GROUP BY',
                'OREDER BY': 'ORDER BY',
                'LIMT': 'LIMIT',
                'OFSET': 'OFFSET'
            }
            
            for typo, correction in typos.items():
                if typo in fixed_query:
                    fixed_query = fixed_query.replace(typo, correction)
                    logger.info(f"Fixed typo: {typo} -> {correction}")
                    
            return fixed_query
        except Exception as e:
            logger.error(f"Error fixing syntax errors: {e}")
            return query
    
    def _setup_agent_workflow(self):
        """Create the multi-agent workflow using LangGraph"""
        try:
            # Check if all agents were initialized
            if not all([self.query_understanding_agent, self.ontology_agent, self.validation_agent, self.reporting_agent]):
                logger.error("One or more agents failed to initialize")
                raise ValueError("Agents not properly initialized")
            
            # Define workflow nodes as functions with reasoning capture
            def understand_query(state):
                """Node for understanding the user query with chain-of-thought reasoning"""
                try:
                    # First capture the reasoning trace using a direct LLM call
                    reasoning_prompt = f"User query: {state.original_query}\n\nAnalyze this query step by step, identifying the intent, entities, filters, and question type."
                    understanding_response = self.llm.invoke(reasoning_prompt)
                    understanding_reasoning = understanding_response.content
                    
                    # Then get the structured output using the specialized agent
                    # Fixed: Pass the query as a simple dict to the agent
                    query_understanding = self.query_understanding_agent.invoke({"query": state.original_query})
                    
                    return {
                        "query_understanding": query_understanding,
                        "understanding_reasoning": understanding_reasoning
                    }
                except Exception as e:
                    logger.error(f"Error in understand_query node: {e}")
                    logger.error(traceback.format_exc())
                    # Return a default understanding to allow the workflow to continue
                    return {
                        "query_understanding": {
                            "intent": "general_query",
                            "entities": [],
                            "filters": [],
                            "question_type": "factual"
                        },
                        "understanding_reasoning": f"Error analyzing query: {str(e)}"
                    }
            
            def generate_sparql(state):
                """Node for generating a SPARQL query with chain-of-thought reasoning"""
                try:
                    # First capture the reasoning process through a direct LLM call
                    reasoning_prompt = f"Query Understanding: {json.dumps(state.query_understanding)}\n\n" + \
                                    "Based on this understanding and the ontology, generate a SPARQL query step by step."
                    sparql_generation_response = self.llm.invoke(reasoning_prompt)
                    sparql_generation_reasoning = sparql_generation_response.content
                    
                    # Get the actual SPARQL query using the specialized agent
                    # The agent now returns just the SPARQL query without any surrounding text
                    sparql_query = self.ontology_agent.invoke({
                        "query_understanding": json.dumps(state.query_understanding)
                    })
                    
                    # Log the generated query for debugging
                    logger.info(f"Generated SPARQL query: {sparql_query[:100]}...")
                    
                    # Validate the query has basic SPARQL structure
                    if not re.search(r"(SELECT|ASK|CONSTRUCT|DESCRIBE).*?WHERE", sparql_query, re.IGNORECASE | re.DOTALL):
                        logger.warning("Generated query does not have valid SPARQL structure")
                        # Use a fallback query if the structure doesn't look right
                        sparql_query = "SELECT * WHERE { ?s ?p ?o } LIMIT 10"
                    
                    return {
                        "sparql_query": sparql_query,
                        "sparql_generation_reasoning": sparql_generation_reasoning
                    }
                except Exception as e:
                    logger.error(f"Error in generate_sparql node: {e}")
                    logger.error(traceback.format_exc())
                    # Generate a simple fallback query
                    fallback_query = "SELECT * WHERE { ?s ?p ?o } LIMIT 10"
                    return {
                        "sparql_query": fallback_query,
                        "sparql_generation_reasoning": f"Error generating SPARQL: {str(e)}. Using fallback query."
                    }
            
            def validate_query(state):
                """Node for validating the SPARQL query with chain-of-thought reasoning"""
                try:
                    # Capture the validation reasoning process through a direct LLM call
                    validation_prompt = f"SPARQL Query to validate:\n\n```sparql\n{state.sparql_query}\n```\n\n" + \
                                    "Validate this SPARQL query step by step, checking for syntax errors, logical issues, and potential optimizations."
                    validation_response = self.llm.invoke(validation_prompt)
                    validation_reasoning = validation_response.content
                    
                    # Try to get the structured validation result using the specialized agent
                    # Fixed: Pass the proper parameter to the agent
                    validation_result = self.validation_agent.invoke({
                        "sparql_query": state.sparql_query
                    })
                    
                    # Ensure the result is a dictionary
                    if not isinstance(validation_result, dict):
                        logger.warning(f"Validation result is not a dictionary: {validation_result}")
                        validation_result = {
                            "is_valid": True,  # Assume valid to continue
                            "explanation": "Validation produced non-dictionary result. Assuming query is valid."
                        }
                    
                    return {
                        "validation_result": validation_result,
                        "validation_reasoning": validation_reasoning
                    }
                except Exception as e:
                    logger.error(f"Error in validate_query node: {e}")
                    logger.error(traceback.format_exc())
                    # Return a default validation result
                    return {
                        "validation_result": {
                            "is_valid": True,  # Assume valid to allow execution
                            "explanation": f"Error validating query: {str(e)}. Assuming query is valid."
                        },
                        "validation_reasoning": f"Error validating query: {str(e)}"
                    }
            
            def fix_query(state):
                """Node for fixing an invalid SPARQL query"""
                try:
                    # Use the corrected query from validation if available
                    if state.validation_result and "corrected_query" in state.validation_result and state.validation_result["corrected_query"]:
                        return {"sparql_query": state.validation_result["corrected_query"]}
                    
                    # If no corrected query is available, generate a simple query
                    logger.warning("No corrected query provided, using fallback query")
                    return {"sparql_query": "SELECT * WHERE { ?s ?p ?o } LIMIT 10"}
                except Exception as e:
                    logger.error(f"Error in fix_query node: {e}")
                    logger.error(traceback.format_exc())
                    # Return a simple fallback query
                    return {"sparql_query": "SELECT * WHERE { ?s ?p ?o } LIMIT 10"}
            
            def execute_query(state):
                """Node for executing the SPARQL query"""
                try:
                    # Log the query execution
                    logger.info(f"Executing SPARQL query: {state.sparql_query}")
                    
                    # Execute the query and get results
                    query_results = self._execute_sparql_query(state.sparql_query)
                    
                    # Log the number of results
                    logger.info(f"Query returned {len(query_results)} results")
                    
                    return {"query_results": query_results}
                except Exception as e:
                    logger.error(f"Error in execute_query node: {e}")
                    logger.error(traceback.format_exc())
                    # Return an error result
                    return {"query_results": [{"error": f"Query execution failed: {str(e)}"}]}
            
            def generate_report(state):
                """Node for generating the final report with chain-of-thought reasoning"""
                try:
                    # Ensure query_results exists with safer access
                    query_results = state.get("query_results", [{"error": "No results available"}])
                    original_query = state.get("original_query", "Unknown query")
                    sparql_query = state.get("sparql_query", "")
                    
                    # Handle string conversion for JSON serialization
                    if query_results is None:
                        query_results = [{"error": "No results available"}]
                    
                    # Process and truncate results to avoid token limit issues
                    processed_results = []
                    result_count = len(query_results)
                    sample_count = min(result_count, 100)  # Limit to 100 results maximum
                    
                    # Extract a sample of the results
                    for i, result in enumerate(query_results[:sample_count]):
                        processed_result = {}
                        for key, value in result.items():
                            # Convert complex values to simpler representations
                            if isinstance(value, dict) and "uri" in value:
                                if "label" in value:
                                    processed_result[key] = f"{value['label']} ({value['uri']})"
                                else:
                                    processed_result[key] = value['uri']
                            elif isinstance(value, dict) and "value" in value:
                                processed_result[key] = value['value']
                            else:
                                processed_result[key] = str(value)
                        processed_results.append(processed_result)
                    
                    # Add a message if results were truncated
                    result_summary = f"Showing {sample_count} of {result_count} total results." if result_count > sample_count else ""
                    
                    # Prepare the query results summary for the report
                    query_summary = {
                        "total_results": result_count,
                        "sample_shown": sample_count,
                        "results": processed_results
                    }
                    
                    query_results_str = json.dumps(query_summary, indent=2)
                    
                    # Ensure the prompt size is reasonable - truncate if needed
                    max_results_chars = 500000  # Limit to prevent token overflows
                    if len(query_results_str) > max_results_chars:
                        query_results_str = query_results_str[:max_results_chars] + "... [truncated due to size]"
                        logger.warning(f"Query results truncated from {len(query_results_str)} to {max_results_chars} characters")
                    
                    # Create a reporting prompt that emphasizes what's needed in the report
                    reporting_prompt = f"""
                    Original Question: {original_query}
                    
                    SPARQL Query Used: 
                    ```sparql
                    {sparql_query}
                    ```
                    
                    Query Results Summary:
                    - Total results: {result_count}
                    - Sample shown: {sample_count}
                    {result_summary}
                    
                    Results Sample:
                    {query_results_str[:50000]}  # Further limit the size in the reasoning prompt
                    
                    Please analyze these results and create a comprehensive business-friendly report that:
                    1. Directly answers the original question
                    2. Summarizes key findings from the data
                    3. Presents the SPARQL query used (in a code block)
                    4. Includes any relevant patterns or insights
                    5. Presents the data in a structured, easy-to-understand format
                    
                    Your report should be tailored to the specific question and results, not follow a fixed template.
                    """
                    
                    # Capture the reporting reasoning process
                    try:
                        reporting_response = self.llm.invoke(reporting_prompt)
                        reporting_reasoning = reporting_response.content
                    except Exception as reasoning_err:
                        logger.warning(f"Error generating reasoning trace (non-critical): {reasoning_err}")
                        reporting_reasoning = f"Reasoning trace generation skipped: {str(reasoning_err)}"
                    
                    # Prepare a more concise prompt for the final report generation
                    report_generation_prompt = {
                        "original_query": original_query,
                        "query_results": query_results_str[:300000],  # Further limit size for final report generation
                        "sparql_query": sparql_query,
                        "result_count": result_count,
                        "result_summary": result_summary
                    }
                    
                    # Generate the actual report using the specialized agent
                    try:
                        final_report = self.reporting_agent.invoke(report_generation_prompt)
                    except Exception as report_err:
                        logger.error(f"Error in report generation: {report_err}")
                        # Fall back to a simpler report generation approach
                        simplified_prompt = f"""
                        Create a business report answering this question: "{original_query}"
                        
                        The query returned {result_count} results. 
                        The SPARQL query used was: {sparql_query}
                        
                        First few results: {json.dumps(processed_results[:10], indent=2)}
                        """
                        final_report = self.llm.invoke(simplified_prompt).content
                    
                    # Add query information to the final report if not already present
                    if "SPARQL Query" not in final_report and "sparql query" not in final_report.lower():
                        query_info = f"""
                        
                        ## SPARQL Query Used
                        ```sparql
                        {sparql_query}
                        ```
                        """
                        final_report += query_info
                    
                    return {
                        "final_report": final_report,
                        "reporting_reasoning": reporting_reasoning
                    }
                except Exception as e:
                    logger.error(f"Error in generate_report node: {e}")
                    logger.error(traceback.format_exc())
                    # Generate a simple error report
                    error_report = f"""# Query Results Analysis

I apologize, but I encountered an error while analyzing the query results: {str(e)}

The original query was: '{state.get('original_query', 'Unknown query')}'

The SPARQL query used was:
```sparql
{state.get('sparql_query', 'Query not available')}
```

This error might be due to the large size of the results or a processing limitation.
Please try rephrasing your question to be more specific, or contact support if this issue persists.
"""
                    return {
                        "final_report": error_report,
                        "reporting_reasoning": f"Error generating report: {str(e)}"
                    }
            
            # Define the router for conditional branching
            def validation_router(state):
                """Route based on query validation results"""
                try:
                    if state.validation_result and state.validation_result.get("is_valid", False):
                        return "execute_query"
                    else:
                        return "fix_query"
                except Exception as e:
                    logger.error(f"Error in validation_router: {e}")
                    # Default to fix_query on error
                    return "fix_query"
            
            # Create the graph
            workflow = StateGraph(WorkflowState)
            
            # Add nodes to the graph
            workflow.add_node("understand_query", understand_query)
            workflow.add_node("generate_sparql", generate_sparql)
            workflow.add_node("validate_query", validate_query)
            workflow.add_node("fix_query", fix_query)
            workflow.add_node("execute_query", execute_query)
            workflow.add_node("generate_report", generate_report)
            
            # Add edges to create the workflow
            workflow.add_edge("understand_query", "generate_sparql")
            workflow.add_edge("generate_sparql", "validate_query")
            workflow.add_conditional_edges(
                "validate_query",
                validation_router,
                {
                    "execute_query": "execute_query",
                    "fix_query": "fix_query"
                }
            )
            workflow.add_edge("fix_query", "validate_query")  # Loop back for revalidation
            workflow.add_edge("execute_query", "generate_report")
            workflow.add_edge("generate_report", END)
            
            # Set the entry point
            workflow.set_entry_point("understand_query")
            
            # Compile the workflow
            return workflow.compile()
        except Exception as e:
            logger.error(f"Error setting up agent workflow: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def execute_multiple_queries(self, queries: List[str]) -> List[Dict[str, Any]]:
        """Execute multiple SPARQL queries and combine the results"""
        all_results = []
        query_info = []
        
        for i, query in enumerate(queries):
            logger.info(f"Executing query {i+1} of {len(queries)}")
            try:
                # Execute the query
                results = self._execute_sparql_query(query)
                
                # Store results and metadata
                all_results.extend(results)
                query_info.append({
                    "query_index": i+1,
                    "query_text": query,
                    "result_count": len(results),
                    "successful": True
                })
                
                # Log success
                logger.info(f"Query {i+1} returned {len(results)} results")
            except Exception as e:
                # Log failure but continue with other queries
                logger.error(f"Error executing query {i+1}: {e}")
                query_info.append({
                    "query_index": i+1, 
                    "query_text": query,
                    "result_count": 0,
                    "successful": False,
                    "error": str(e)
                })
        
        # Return the combined results and query information
        return all_results, query_info
    
    def ask(self, query: str) -> str:
        """Process a user query through the entire workflow and return the final report"""
        query_id = str(uuid.uuid4())
        
        # Initialize the workflow state
        initial_state = WorkflowState(
            query_id=query_id,
            original_query=query
        )
        
        # Execute the workflow
        try:
            logger.info(f"Processing query: {query}")
            
            # For debugging - force using a simple workflow
            if os.environ.get("DEBUG_SIMPLE_RESPONSE", "False").lower() == "true":
                logger.info("Using simple response mode for debugging")
                return f"DEBUG MODE: Your query was: {query}"
            
            # Check if we have a valid workflow
            if not self.agent_workflow:
                logger.error("Agent workflow not initialized")
                return "I'm sorry, the chatbot system has not been properly initialized. Please try again later."
            
            # Execute the workflow
            final_state = self.agent_workflow.invoke(initial_state)
            
            # Handle different return types from LangGraph
            if isinstance(final_state, dict):
                # If final_state is a dict, directly access the keys
                logger.info("Final state returned as dictionary")
                final_report = final_state.get("final_report", None)
                
                # Get the SPARQL query for completeness
                sparql_query = final_state.get("sparql_query", None)
            elif hasattr(final_state, "final_report") and hasattr(final_state, "sparql_query"):
                # If final_state is an object with attributes
                logger.info("Final state returned as object with attributes")
                final_report = final_state.final_report
                sparql_query = final_state.sparql_query
            elif hasattr(final_state, "__getitem__"):
                # If final_state is a subscriptable object (like AddableValuesDict)
                try:
                    logger.info("Final state returned as subscriptable object")
                    final_report = final_state["final_report"]
                    sparql_query = final_state.get("sparql_query", None)
                except (KeyError, TypeError) as e:
                    logger.error(f"Error accessing final_report from state: {e}")
                    final_report = None
                    sparql_query = None
            else:
                # If we can't access the final report
                logger.error(f"Unknown state type: {type(final_state)}")
                final_report = None
                sparql_query = None
            
            # If we couldn't get a final report, generate a simple response
            if not final_report:
                logger.warning("No final report in state, generating fallback response")
                
                # Check if we at least have a SPARQL query to show
                if sparql_query:
                    final_report = f"""# Query Results

I processed your query about '{query}' but couldn't generate a complete report.
This might be due to limitations in the available data or technical constraints.

## SPARQL Query Used
```sparql
{sparql_query}
```

No results were returned for this query. Try reformulating your question to get better results.
"""
                else:
                    final_report = f"I processed your query about '{query}' but couldn't generate a complete report. " \
                                 f"This might be due to limitations in the available data or technical constraints."
            
            # Ensure the report includes the SPARQL query if it doesn't already
            if sparql_query and "```sparql" not in final_report:
                query_section = f"""
                
## SPARQL Query Used
```sparql
{sparql_query}
```
"""
                final_report += query_section
            
            # Store the conversation in memory
            try:
                self.memory.save_context(
                    {"input": query},
                    {"output": final_report}
                )
            except Exception as mem_error:
                logger.warning(f"Error saving to memory: {mem_error}")
            
            return final_report
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            logger.error(traceback.format_exc())
            
            # Create a more user-friendly error report
            error_report = f"""# Query Processing Error

I encountered an error while processing your query: {str(e)}

Your original query was: "{query}"

This might be due to one of the following reasons:
- The knowledge graph might not contain the information you're looking for
- The query might be too complex for the current implementation
- There might be a technical issue with the knowledge graph connection

Please try:
- Rephrasing your question to be more specific
- Breaking down complex questions into simpler ones
- Checking if the ontology contains the concepts you're asking about
"""
            return error_report


# FastAPI implementation for API access
from fastapi import FastAPI, HTTPException, Depends, Security, status
from fastapi.security import APIKeyHeader
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel as FastAPIBaseModel
import uvicorn

# Request and response models
class ChatRequest(FastAPIBaseModel):
    query: str
    conversation_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class ChatResponse(FastAPIBaseModel):
    query_id: str
    original_query: str
    response: str
    metadata: Optional[Dict[str, Any]] = None

class HealthResponse(FastAPIBaseModel):
    status: str
    version: str

# Global chatbot instance
_chatbot_instance = None

def get_chatbot():
    """Singleton pattern to get or create the chatbot instance"""
    global _chatbot_instance
    if _chatbot_instance is None:
        try:
            _chatbot_instance = GraphRAGChatbot()
        except Exception as e:
            logger.error(f"Error creating chatbot instance: {e}")
            logger.error(traceback.format_exc())
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Error initializing chatbot: {str(e)}"
            )
    return _chatbot_instance

# API Key security
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

async def get_api_key(api_key: str = Security(api_key_header)):
    """Validate API key if enabled"""
    # Get from environment if API key security is enabled
    if os.environ.get("API_KEY_ENABLED", "False").lower() == "true":
        correct_api_key = os.environ.get("API_KEY", "")
        if not correct_api_key or api_key != correct_api_key:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid API Key",
            )
    return api_key

# Create FastAPI app
app = FastAPI(
    title="GraphRAG Chatbot API",
    description="API for querying knowledge graphs using natural language",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Modify this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return {"status": "ok", "version": "1.0.0"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, api_key: str = Depends(get_api_key)):
    """Process a chat request"""
    try:
        chatbot = get_chatbot()
        response = chatbot.ask(request.query)
        
        # Get query_id from memory or generate a new one
        query_id = str(uuid.uuid4())
        
        return {
            "query_id": query_id,
            "original_query": request.query,
            "response": response,
            "metadata": request.metadata
        }
    except Exception as e:
        logger.error(f"Error processing chat request: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error processing request: {str(e)}"
        )

@app.post("/query_with_details", response_model=Dict[str, Any])
async def query_with_details(request: ChatRequest, api_key: str = Depends(get_api_key)):
    """Process a query and return detailed workflow information with reasoning traces"""
    try:
        chatbot = get_chatbot()
        
        # Initialize the workflow with tracking enabled
        query_id = str(uuid.uuid4())
        
        # Initialize the workflow state
        initial_state = WorkflowState(
            query_id=query_id,
            original_query=request.query
        )
        
        # Execute the workflow and capture intermediate states with reasoning
        final_state = chatbot.agent_workflow.invoke(initial_state)
        
        # Handle different return types
        details = {}
        details["query_id"] = query_id
        details["original_query"] = request.query
        
        # Try to access state fields safely
        for field in ["query_understanding", "understanding_reasoning", "sparql_query", 
                     "sparql_generation_reasoning", "validation_result", "validation_reasoning", 
                     "query_results", "reporting_reasoning", "final_report"]:
            if isinstance(final_state, dict):
                details[field] = final_state.get(field)
            elif hasattr(final_state, field):
                details[field] = getattr(final_state, field)
            elif hasattr(final_state, "__getitem__"):
                try:
                    details[field] = final_state[field]
                except (KeyError, TypeError):
                    details[field] = None
            else:
                details[field] = None
        
        # Add metadata
        details["metadata"] = request.metadata
        
        return details
    except Exception as e:
        logger.error(f"Error processing detailed query: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error processing request: {str(e)}"
        )

@app.post("/reasoning_only", response_model=Dict[str, Any])
async def get_reasoning_only(request: ChatRequest, api_key: str = Depends(get_api_key)):
    """Process a query and return only the chain-of-thought reasoning for each step"""
    try:
        chatbot = get_chatbot()
        
        # Initialize the workflow
        query_id = str(uuid.uuid4())
        initial_state = WorkflowState(
            query_id=query_id,
            original_query=request.query
        )
        
        # Execute the workflow
        final_state = chatbot.agent_workflow.invoke(initial_state)
        
        # Extract reasoning fields safely
        result = {
            "query_id": query_id,
            "original_query": request.query
        }
        
        # Try to access reasoning fields safely
        for field in ["understanding_reasoning", "sparql_generation_reasoning", 
                     "validation_reasoning", "reporting_reasoning"]:
            if isinstance(final_state, dict):
                result[field] = final_state.get(field)
            elif hasattr(final_state, field):
                result[field] = getattr(final_state, field)
            elif hasattr(final_state, "__getitem__"):
                try:
                    result[field] = final_state[field]
                except (KeyError, TypeError):
                    result[field] = None
            else:
                result[field] = None
        
        result["metadata"] = request.metadata
        
        return result
    except Exception as e:
        logger.error(f"Error processing reasoning query: {e}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error processing request: {str(e)}"
        )

# Command-line interface function
def cli():
    """Command-line interface for the GraphRAG chatbot"""
    try:
        # Initialize the chatbot
        chatbot = get_chatbot()
        
        # Interactive loop
        print("GraphRAG Chatbot initialized. Type 'exit' to quit.")
        while True:
            try:
                user_query = input("\nEnter your question: ")
                if user_query.lower() in ["exit", "quit", "bye"]:
                    print("Goodbye!")
                    break
                    
                # Process query
                response = chatbot.ask(user_query)
                print("\nResponse:")
                print(response)
            except KeyboardInterrupt:
                print("\nInterrupted by user. Goodbye!")
                break
            except Exception as e:
                print(f"\nError: {str(e)}")
                print("Please try again or type 'exit' to quit.")
            
    except Exception as e:
        logger.error(f"Error in CLI function: {e}")
        logger.error(traceback.format_exc())
        print(f"Fatal error: {str(e)}")
        sys.exit(1)

# API server function
def serve_api(host="0.0.0.0", port=8000):
    """Start the API server"""
    try:
        uvicorn.run(app, host=host, port=port)
    except Exception as e:
        logger.error(f"Error starting API server: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)

# Main function that can start either the CLI or API server
def main():
    """Main function to start either the CLI or API server based on arguments"""
    import argparse
    
    parser = argparse.ArgumentParser(description="GraphRAG Chatbot")
    parser.add_argument("--api", action="store_true", help="Start the API server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="API server host")
    parser.add_argument("--port", type=int, default=8000, help="API server port")
    parser.add_argument("--ontology", type=str, help="Path to the ontology file (OWL, TTL, N3, etc.)")
    parser.add_argument("--config", type=str, help="Path to the config file")
    parser.add_argument("--creds", type=str, help="Path to the credentials file")
    parser.add_argument("--cert", type=str, help="Path to the certificate file")
    parser.add_argument("--sparql-endpoint", type=str, help="URL of the SPARQL endpoint")
    parser.add_argument("--sparql-username", type=str, help="Username for SPARQL endpoint Basic auth")
    parser.add_argument("--sparql-password", type=str, help="Password for SPARQL endpoint Basic auth")
    parser.add_argument("--debug", action="store_true", help="Enable debug mode for simple responses")
    
    args = parser.parse_args()
    
    # Set environment variables from command line arguments if provided
    if args.sparql_endpoint:
        os.environ["SPARQL_ENDPOINT_URL"] = args.sparql_endpoint
    
    # Handle authentication
    if args.sparql_username and args.sparql_password:
        os.environ["SPARQL_AUTH_ENABLED"] = "True"
        os.environ["SPARQL_AUTH_TYPE"] = "basic"
        os.environ["SPARQL_USERNAME"] = args.sparql_username
        os.environ["SPARQL_PASSWORD"] = args.sparql_password
    
    if args.ontology:
        os.environ["ONTOLOGY_PATH"] = args.ontology
        
    if args.debug:
        os.environ["DEBUG_SIMPLE_RESPONSE"] = "True"
    
    # Determine file paths
    config_path = args.config or CONFIG_PATH
    creds_path = args.creds or CREDS_PATH
    cert_path = args.cert or CERT_PATH
    ontology_path = args.ontology
    
    try:
        # Initialize the global chatbot instance with the provided paths
        global _chatbot_instance
        _chatbot_instance = GraphRAGChatbot(
            config_file=config_path,
            creds_file=creds_path,
            cert_file=cert_path,
            ontology_path=ontology_path
        )
        
        if args.api:
            print(f"Starting API server on {args.host}:{args.port}")
            serve_api(host=args.host, port=args.port)
        else:
            cli()
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        logger.error(traceback.format_exc())
        print(f"Fatal error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
