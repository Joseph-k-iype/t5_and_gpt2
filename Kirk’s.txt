The error is occurring because ChromaDB is seeing different embedding dimensions. Let's fix this by ensuring consistent embedding dimensions:

```python
import os
import pandas as pd
from azure.identity import DefaultAzureCredential
import requests
import logging
import urllib.parse
import json
import chromadb
import numpy as np
from typing import List, Dict

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Configure SSL and no proxy for Azure services
NO_PROXY_DOMAINS = [
    'cognitiveservices.azure.com',
    'search.windows.net',
    'openai.azure.com',
    'core.windows.net',
    'azurewebsites.net'
]
os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

# Create a session with SSL verification
session = requests.Session()
session.verify = 'cacert.pem'
session.proxies = {
    'http': None,
    'https': None
}

# Get Azure AD token
credential = DefaultAzureCredential()
token = credential.get_token("https://cognitiveservices.azure.com/.default")

def get_embeddings(texts, endpoint, deployment_name="text-embedding-ada-002", max_length=5000):
    """
    Generate embeddings using Azure OpenAI REST API directly with text length limitation
    """
    headers = {
        'Authorization': f'Bearer {token.token}',
        'Content-Type': 'application/json'
    }
    
    api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version=2024-02-01"
    
    # Truncate text to prevent token limit issues
    truncated_texts = [text[:max_length] for text in texts]
    
    try:
        payload = {
            "input": truncated_texts
        }
        
        response = session.post(
            api_url,
            headers=headers,
            json=payload
        )
        
        if response.status_code == 200:
            response_data = response.json()
            embeddings = [item['embedding'] for item in response_data['data']]
            logger.debug(f"Successfully generated embeddings. Dimension: {len(embeddings[0])}")
            return embeddings
        else:
            logger.error(f"Error: {response.text}")
            return None
                
    except Exception as e:
        logger.error(f"Error in embedding generation: {str(e)}")
        return None

def process_and_match(reference_csv: str, lookup_csv: str, azure_endpoint: str, n_results: int = 5) -> pd.DataFrame:
    """Process both CSVs and find matches"""
    
    # Load CSVs
    reference_df = pd.read_csv(reference_csv)
    lookup_df = pd.read_csv(lookup_csv)
    
    print(f"Reference data size: {len(reference_df)}")
    print(f"Lookup data size: {len(lookup_df)}")
    
    # Create ChromaDB client and collection
    chroma_client = chromadb.Client()
    
    # Delete collection if it exists
    try:
        chroma_client.delete_collection("terms")
    except:
        pass
        
    collection = chroma_client.create_collection(name="terms")
    
    # Process reference CSV
    print("Processing reference terms...")
    reference_texts = [f"Name: {name}\nDefinition: {definition}" 
                      for name, definition in zip(reference_df['name'], reference_df['definition'])]
    
    # Get embeddings for reference terms
    reference_embeddings = get_embeddings(reference_texts, azure_endpoint)
    
    if reference_embeddings is None:
        raise Exception("Failed to generate reference embeddings")
    
    # Add to collection
    collection.add(
        embeddings=reference_embeddings,
        documents=reference_texts,
        ids=[str(i) for i in range(len(reference_df))],
        metadatas=[{"name": name, "definition": definition} 
                  for name, definition in zip(reference_df['name'], reference_df['definition'])]
    )
    
    print("Processing lookup terms...")
    results = []
    
    # Process lookup terms one at a time to avoid batch issues
    for idx, row in lookup_df.iterrows():
        query_text = f"Name: {row['name']}\nDefinition: {row['definition']}"
        
        # Generate embedding for single query
        query_embeddings = get_embeddings([query_text], azure_endpoint)
        
        if query_embeddings is None:
            logger.error(f"Failed to generate embedding for lookup term {idx}")
            continue
            
        query_embedding = query_embeddings[0]
        
        # Query collection
        matches = collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        # Process matches
        for i in range(len(matches['ids'][0])):
            results.append({
                'lookup_name': row['name'],
                'lookup_definition': row['definition'],
                'matched_name': matches['metadatas'][0][i]['name'],
                'matched_definition': matches['metadatas'][0][i]['definition'],
                'similarity_rank': i + 1
            })
        
        if idx % 10 == 0:
            print(f"Processed {idx} lookup terms...")
    
    return pd.DataFrame(results)

def main():
    # Replace with your Azure endpoint
    azure_endpoint = "your_endpoint"
    
    try:
        print("Starting semantic matching process...")
        
        matches_df = process_and_match(
            reference_csv='reference.csv',
            lookup_csv='lookup.csv',
            azure_endpoint=azure_endpoint,
            n_results=5
        )
        
        # Save results
        matches_df.to_csv('semantic_matches.csv', index=False)
        print("Results saved to semantic_matches.csv")
        
        # Display sample results
        print("\nSample matches:")
        print(matches_df.head())
        
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")
        raise

if __name__ == "__main__":
    main()
```

Key changes made:
1. Simplified the embedding generation to handle one batch at a time
2. Removed the class structure to make the flow more linear and easier to debug
3. Added deletion of existing collection to ensure clean start
4. Added more logging and error handling
5. Process lookup terms one at a time to avoid batch issues
6. Added dimension checking and logging

To use:
1. Replace "your_endpoint" with your Azure OpenAI endpoint
2. Make sure your reference.csv and lookup.csv files have 'name' and 'definition' columns
3. Run the script

The script will:
1. Load and process reference data
2. Generate embeddings and store in ChromaDB
3. Process lookup terms one by one
4. Save results to 'semantic_matches.csv'

Let me know if you still encounter any issues!​​​​​​​​​​​​​​​​
