Let's combine the previous working get_embeddings function with ChromaDB. Here's the corrected version:

```python
import os
import pandas as pd
from azure.identity import DefaultAzureCredential
import requests
import logging
import urllib.parse
import json
import chromadb
import numpy as np
from typing import List, Dict

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Configure SSL and no proxy for Azure services
NO_PROXY_DOMAINS = [
    'cognitiveservices.azure.com',
    'search.windows.net',
    'openai.azure.com',
    'core.windows.net',
    'azurewebsites.net'
]
os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

# Create a session with SSL verification
session = requests.Session()
session.verify = 'cacert.pem'
session.proxies = {
    'http': None,
    'https': None
}

# Get Azure AD token
credential = DefaultAzureCredential()
token = credential.get_token("https://cognitiveservices.azure.com/.default")

def get_embeddings(texts, endpoint, deployment_name="text-embedding-ada-002", batch_size=100):
    """
    Generate embeddings using Azure OpenAI REST API directly
    """
    headers = {
        'Authorization': f'Bearer {token.token}',
        'Content-Type': 'application/json'
    }
    
    api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version=2024-02-01"
    
    embeddings = []
    
    # Process in batches
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        try:
            payload = {
                "input": batch
            }
            
            response = session.post(
                api_url,
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                response_data = response.json()
                batch_embeddings = [item['embedding'] for item in response_data['data']]
                embeddings.extend(batch_embeddings)
                logger.debug(f"Successfully processed batch {i//batch_size + 1}")
            else:
                logger.error(f"Error in batch {i//batch_size + 1}: {response.text}")
                embeddings.extend([None] * len(batch))
                
        except Exception as e:
            logger.error(f"Error processing batch starting at index {i}: {str(e)}")
            embeddings.extend([None] * len(batch))
            
    return embeddings

class SemanticMatcher:
    def __init__(self, azure_endpoint: str):
        self.azure_endpoint = azure_endpoint
        self.chroma_client = chromadb.Client()
        
    def process_csv_to_chroma(self, csv_path: str, collection_name: str) -> None:
        """Load CSV data into ChromaDB collection"""
        df = pd.read_csv(csv_path)
        
        # Create or get collection
        collection = self.chroma_client.get_or_create_collection(name=collection_name)
        
        # Combine name and definition for better semantic matching
        documents = [f"Name: {name}\nDefinition: {definition}" 
                    for name, definition in zip(df['name'], df['definition'])]
        
        # Generate embeddings using the working function
        embeddings = get_embeddings(documents, self.azure_endpoint)
        
        # Generate IDs
        ids = [str(i) for i in range(len(df))]
        
        # Store original data for reference
        metadatas = [{"name": name, "definition": definition} 
                    for name, definition in zip(df['name'], df['definition'])]
        
        # Add to collection
        collection.add(
            embeddings=embeddings,
            documents=documents,
            ids=ids,
            metadatas=metadatas
        )
        
        logger.info(f"Added {len(df)} items to collection {collection_name}")

    def find_matches(self, 
                    lookup_csv: str, 
                    reference_collection: str, 
                    n_results: int = 5) -> pd.DataFrame:
        """Find semantic matches for lookup CSV in reference collection"""
        # Load lookup CSV
        lookup_df = pd.read_csv(lookup_csv)
        
        # Get reference collection
        collection = self.chroma_client.get_collection(name=reference_collection)
        
        results = []
        
        # Process each row in lookup CSV
        for idx, row in lookup_df.iterrows():
            query = f"Name: {row['name']}\nDefinition: {row['definition']}"
            
            # Generate embedding for query
            query_embedding = get_embeddings([query], self.azure_endpoint)[0]
            
            # Query collection
            matches = collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # Process matches
            for i in range(len(matches['ids'][0])):
                results.append({
                    'lookup_name': row['name'],
                    'lookup_definition': row['definition'],
                    'matched_name': matches['metadatas'][0][i]['name'],
                    'matched_definition': matches['metadatas'][0][i]['definition'],
                    'similarity_rank': i + 1
                })
        
        # Convert results to DataFrame
        results_df = pd.DataFrame(results)
        return results_df

def main():
    # Replace with your Azure endpoint
    azure_endpoint = "your_endpoint"
    
    # Initialize matcher
    matcher = SemanticMatcher(azure_endpoint)
    
    try:
        # Load reference data into ChromaDB
        print("Loading reference data...")
        matcher.process_csv_to_chroma(
            csv_path='reference.csv',  # Your first CSV
            collection_name='reference_terms'
        )
        
        # Find matches for lookup data
        print("Finding matches...")
        matches_df = matcher.find_matches(
            lookup_csv='lookup.csv',  # Your second CSV
            reference_collection='reference_terms',
            n_results=5  # Number of matches to return per term
        )
        
        # Save results
        matches_df.to_csv('semantic_matches.csv', index=False)
        print("Results saved to semantic_matches.csv")
        
        # Display sample results
        print("\nSample matches:")
        print(matches_df.head())
        
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")
        raise

if __name__ == "__main__":
    main()
```

Key changes made:
1. Removed the custom embedding function class and used the previously working get_embeddings function
2. Modified ChromaDB implementation to use pre-generated embeddings
3. Kept the same proxy and SSL settings that worked before
4. Added more detailed logging

To use this:
1. Replace "your_endpoint" with your Azure OpenAI endpoint
2. Make sure your CSVs (reference.csv and lookup.csv) have 'name' and 'definition' columns
3. Run the script

The code will:
1. Generate embeddings using the working Azure OpenAI setup
2. Store the embeddings and data in ChromaDB
3. Find semantic matches between your CSVs
4. Save results to 'semantic_matches.csv'

Let me know if you encounter any issues or need adjustments!​​​​​​​​​​​​​​​​
