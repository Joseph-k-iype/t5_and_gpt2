def _read_csv_safely(file_path: Path) -> pd.DataFrame:
    """Read CSV with multiple encoding attempts and error handling."""
    encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
    errors = []
    
    for encoding in encodings:
        try:
            logger.info(f"Attempting to read CSV with {encoding} encoding")
            return pd.read_csv(file_path, encoding=encoding)
        except Exception as e:
            errors.append(f"{encoding}: {str(e)}")
            continue
    
    raise ValueError(f"Failed to read CSV with any encoding. Errors:\n" + "\n".join(errors))

class ChromaVectorStore:
    """ChromaDB vector store manager with multi-column support"""
    
    def __init__(self, env: OSEnv):
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None

    def create_collection(self, csv_path: Path, text_columns: List[str], 
                        chunk_size: int = 1000, chunk_overlap: int = 100,
                        separator: str = " | ") -> None:
        """
        Create Chroma collection from CSV with multiple text columns.
        
        Args:
            csv_path: Path to CSV file
            text_columns: List of column names to combine for vectorization
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            separator: String to use when combining text from multiple columns
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            df = self._read_csv_safely(csv_path)
            
            # Validate all requested columns exist
            missing_cols = [col for col in text_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
            
            logger.info(f"Processing columns: {', '.join(text_columns)}")
            documents, metadatas = self._process_csv_multi_column(
                df, text_columns, chunk_size, chunk_overlap, separator
            )
            
            self.collection_name = csv_path.stem.lower()
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                ids=[str(i) for i in range(len(documents))],
                metadatas=metadatas,
                persist_directory=persist_directory
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(documents)} documents")
            self.vector_store.persist()
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv_multi_column(
        self, df: pd.DataFrame, 
        text_columns: List[str],
        chunk_size: int = 1000, 
        chunk_overlap: int = 100,
        separator: str = " | "
    ) -> Tuple[List[Document], List[Dict]]:
        """Process CSV data with multiple text columns."""
        documents = []
        metadatas = []
        
        try:
            for idx, row in df.iterrows():
                # Combine text from all specified columns
                text_parts = []
                for col in text_columns:
                    if pd.notna(row[col]):
                        text_parts.append(f"{col}: {str(row[col]).strip()}")
                
                combined_text = separator.join(text_parts)
                if not combined_text.strip():
                    logger.warning(f"Empty combined text found in row {idx}, skipping")
                    continue
                
                # Create metadata from non-text columns
                metadata = {
                    col: str(row[col])
                    for col in df.columns
                    if col not in text_columns and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Chunk text if necessary
                if len(combined_text) > chunk_size:
                    chunks = self._chunk_text(combined_text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        chunk_metadata['total_chunks'] = str(len(chunks))
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                        metadatas.append(chunk_metadata)
                else:
                    documents.append(Document(page_content=combined_text, metadata=metadata))
                    metadatas.append(metadata)
                
                if (idx + 1) % 100 == 0:
                    logger.info(f"Processed {idx + 1} rows")
            
            return documents, metadatas
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

def main():
    """Main application entry point."""
    chroma_db = None
    
    try:
        # ... (previous initialization code remains the same) ...
        
        while True:
            print("\nAvailable commands:")
            print("1. Create new collection from CSV")
            print("2. Load existing collection")
            print("3. List all collections")
            print("4. Delete collection")
            print("5. Search current collection")
            print("6. Show collection statistics")
            print("7. Show environment variables")
            print("8. Exit")
            
            choice = input("\nEnter your choice (1-8): ").strip()
            
            try:
                if choice == '1':
                    # Create new collection with multiple columns
                    csv_path = Path(input("Enter CSV file path: ").strip())
                    df = _read_csv_safely(csv_path)
                    
                    print("\nAvailable columns:")
                    for idx, col in enumerate(df.columns, 1):
                        print(f"{idx}. {col}")
                    
                    # Select multiple columns
                    col_indices = input("\nEnter column numbers (comma-separated): ").strip()
                    selected_cols = [
                        df.columns[int(idx) - 1] 
                        for idx in col_indices.split(',')
                    ]
                    
                    print(f"\nSelected columns: {', '.join(selected_cols)}")
                    separator = input("Enter separator for combining columns (default ' | '): ").strip() or " | "
                    
                    chunk_size = int(input("Enter chunk size (default 1000): ") or "1000")
                    chunk_overlap = int(input("Enter chunk overlap (default 100): ") or "100")
                    
                    chroma_db.create_collection(
                        csv_path, 
                        selected_cols, 
                        chunk_size, 
                        chunk_overlap,
                        separator
                    )
                    
                elif choice == '5':
                    # Search collection with better output formatting
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    query = input("Enter search query: ")
                    k = int(input("Number of results to return (default 5): ") or "5")
                    min_score = float(input("Minimum relevance score (0-1, default 0): ") or "0")
                    
                    results = chroma_db.search(query, k, min_score)
                    print(f"\nFound {len(results)} results:")
                    for i, (text, score, metadata) in enumerate(results, 1):
                        print(f"\n{i}. Relevance Score: {score:.4f}")
                        
                        # Split and format the combined text
                        text_parts = text.split(" | ")
                        print("Content:")
                        for part in text_parts:
                            print(f"  {part}")
                        
                        print("Metadata:")
                        for key, value in metadata.items():
                            if key not in ['chunk_index', 'total_chunks']:
                                print(f"  {key}: {value}")
                        
                        if 'chunk_index' in metadata:
                            print(f"  Chunk: {metadata['chunk_index']}/{metadata['total_chunks']}")
                
                # ... (rest of the main function remains the same) ...
                
            except Exception as e:
                logger.error(f"Operation failed: {str(e)}")
                print(f"Error: {str(e)}")
                
    except Exception as e:
        logger.error(f"Application error: {str(e)}")
        print(f"Error: {str(e)}")
        
    finally:
        if chroma_db is not None:
            chroma_db.close()
