We need to limit the text length and batch size for the embeddings. Let's modify the code to handle this:

```python
def get_embeddings(texts, endpoint, deployment_name="text-embedding-ada-002", batch_size=100, max_tokens=8191):
    """
    Generate embeddings using Azure OpenAI REST API directly with text length limitation
    """
    headers = {
        'Authorization': f'Bearer {token.token}',
        'Content-Type': 'application/json'
    }
    
    api_url = f"{endpoint}/openai/deployments/{deployment_name}/embeddings?api-version=2024-02-01"
    
    embeddings = []
    
    def truncate_text(text, max_length=5000):
        """Truncate text to max_length characters"""
        return text[:max_length]
    
    # Truncate texts first
    truncated_texts = [truncate_text(text) for text in texts]
    
    # Calculate optimal batch size based on text lengths
    total_chars = sum(len(text) for text in truncated_texts)
    avg_chars_per_text = total_chars / len(truncated_texts)
    optimal_batch_size = min(batch_size, int(max_tokens / (avg_chars_per_text / 4)))  # rough estimate of tokens
    optimal_batch_size = max(1, optimal_batch_size)  # ensure at least 1
    
    logger.info(f"Using batch size of {optimal_batch_size}")
    
    # Process in batches
    for i in range(0, len(truncated_texts), optimal_batch_size):
        batch = truncated_texts[i:i + optimal_batch_size]
        
        try:
            payload = {
                "input": batch
            }
            
            response = session.post(
                api_url,
                headers=headers,
                json=payload
            )
            
            if response.status_code == 200:
                response_data = response.json()
                batch_embeddings = [item['embedding'] for item in response_data['data']]
                embeddings.extend(batch_embeddings)
                logger.debug(f"Successfully processed batch {i//optimal_batch_size + 1}")
            else:
                logger.error(f"Error in batch {i//optimal_batch_size + 1}: {response.text}")
                embeddings.extend([None] * len(batch))
                
        except Exception as e:
            logger.error(f"Error processing batch starting at index {i}: {str(e)}")
            embeddings.extend([None] * len(batch))
            
    return embeddings
```

And in the SemanticMatcher class, modify the process_csv_to_chroma method to process in smaller chunks:

```python
def process_csv_to_chroma(self, csv_path: str, collection_name: str, chunk_size=1000) -> None:
    """Load CSV data into ChromaDB collection with chunking"""
    df = pd.read_csv(csv_path)
    
    # Create or get collection
    collection = self.chroma_client.get_or_create_collection(name=collection_name)
    
    # Process in chunks
    for chunk_start in range(0, len(df), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(df))
        chunk_df = df.iloc[chunk_start:chunk_end]
        
        # Combine name and definition for better semantic matching
        documents = [f"Name: {name}\nDefinition: {definition}" 
                    for name, definition in zip(chunk_df['name'], chunk_df['definition'])]
        
        # Generate embeddings using the working function
        embeddings = get_embeddings(documents, self.azure_endpoint)
        
        # Generate IDs
        ids = [str(i) for i in range(chunk_start, chunk_end)]
        
        # Store original data for reference
        metadatas = [{"name": name, "definition": definition} 
                    for name, definition in zip(chunk_df['name'], chunk_df['definition'])]
        
        # Add chunk to collection
        collection.add(
            embeddings=embeddings,
            documents=documents,
            ids=ids,
            metadatas=metadatas
        )
        
        logger.info(f"Added chunk {chunk_start//chunk_size + 1} to collection {collection_name}")
    
    logger.info(f"Completed adding {len(df)} items to collection {collection_name}")
```

Key changes made:
1. Added text truncation to limit token count
2. Added dynamic batch size calculation based on text lengths
3. Added chunking for processing large CSV files
4. Added more detailed logging

The rest of your code can remain the same. These changes will:
1. Prevent the batch size error by automatically adjusting batch size
2. Handle large texts by truncating them
3. Process large datasets in manageable chunks

Try running this updated version with your data. Let me know if you need any adjustments to the chunk size or token limits!​​​​​​​​​​​​​​​​
