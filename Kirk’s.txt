class ChromaVectorStore:
    """ChromaDB vector store manager with Azure embeddings and local storage"""
    
    def __init__(self, env: OSEnv):
        """Initialize the vector store with environment configuration."""
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None
        self.client = self._init_chroma_client()
        logger.info("ChromaVectorStore initialized successfully")

    def _init_chroma_client(self) -> chromadb.Client:
        """Initialize ChromaDB client for local storage."""
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            client = chromadb.Client(Settings(
                anonymized_telemetry=False,
                is_persistent=True,
                persist_directory=persist_directory,
                allow_reset=True
            ))
            
            logger.info(f"Initialized local ChromaDB client at {persist_directory}")
            return client
            
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client: {str(e)}")
            raise

    def _validate_azure_credentials(self) -> None:
        """Validate Azure OpenAI credentials for embeddings."""
        required_vars = [
            "AZURE_TENANT_ID",
            "AZURE_CLIENT_ID",
            "AZURE_CLIENT_SECRET",
            "AZURE_EMBEDDING_DEPLOYMENT",
            "AZURE_EMBEDDING_MODEL",
            "AZURE_API_VERSION",
            "AZURE_OPENAI_ENDPOINT"
        ]
        
        missing = [var for var in required_vars if not self.env.get(var)]
        if missing:
            raise ValueError(f"Missing required Azure variables: {', '.join(missing)}")
        logger.info("Azure credentials validated")

    def _init_embeddings(self) -> AzureOpenAIEmbeddings:
        """Initialize Azure OpenAI embeddings with error handling."""
        try:
            token = self.env.get_azure_token(force_refresh=True)
            
            embeddings = AzureOpenAIEmbeddings(
                deployment=str(self.env.get("AZURE_EMBEDDING_DEPLOYMENT")),
                model=str(self.env.get("AZURE_EMBEDDING_MODEL")),
                api_version=str(self.env.get("AZURE_API_VERSION")),
                azure_endpoint=str(self.env.get("AZURE_OPENAI_ENDPOINT")),
                azure_ad_token=token,
                chunk_size=16  # Process in smaller batches
            )
            
            # Test the embeddings with a simple string
            try:
                logger.info("Testing Azure OpenAI connection...")
                test_embedding = embeddings.embed_query("test")
                logger.info(f"Successfully connected to Azure OpenAI (embedding dimension: {len(test_embedding)})")
            except Exception as e:
                raise ConnectionError(f"Failed to connect to Azure OpenAI: {str(e)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to initialize embeddings: {str(e)}")
            raise

    def create_collection(self, csv_path: Path, text_columns: List[str], 
                        chunk_size: int = 1000, chunk_overlap: int = 100,
                        separator: str = " | ", batch_size: int = 100) -> None:
        """
        Create Chroma collection with batched processing.
        
        Args:
            csv_path: Path to CSV file
            text_columns: List of columns to combine for embedding
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            separator: String to use when combining text columns
            batch_size: Number of documents to process at once
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            df = self._read_csv_safely(csv_path)
            
            missing_cols = [col for col in text_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
            
            logger.info(f"Processing columns: {', '.join(text_columns)}")
            all_documents = []
            total_rows = len(df)
            
            # Process in batches
            for start_idx in range(0, total_rows, batch_size):
                end_idx = min(start_idx + batch_size, total_rows)
                batch_df = df.iloc[start_idx:end_idx]
                
                batch_documents = self._process_csv_multi_column(
                    batch_df, text_columns, chunk_size, chunk_overlap, separator
                )
                all_documents.extend(batch_documents)
                
                logger.info(f"Processed rows {start_idx + 1} to {end_idx} of {total_rows}")
            
            self.collection_name = csv_path.stem.lower()
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            
            # Create the collection with local storage
            logger.info("Creating embeddings (this will make API calls to Azure OpenAI)...")
            self.vector_store = Chroma.from_documents(
                documents=all_documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                persist_directory=persist_directory,
                client=self.client
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(all_documents)} documents")
            
            # Persist to local storage
            self.vector_store.persist()
            logger.info(f"Vectors stored locally in {persist_directory}")
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv_multi_column(
        self, df: pd.DataFrame, 
        text_columns: List[str],
        chunk_size: int = 1000, 
        chunk_overlap: int = 100,
        separator: str = " | "
    ) -> List[Document]:
        """Process a batch of CSV data into documents."""
        documents = []
        
        try:
            for idx, row in df.iterrows():
                # Combine text from specified columns
                text_parts = []
                for col in text_columns:
                    if pd.notna(row[col]):
                        text = str(row[col]).strip()
                        text = text.replace('\n', ' ').replace('\r', ' ')
                        text_parts.append(f"{col}: {text}")
                
                combined_text = separator.join(text_parts)
                if not combined_text.strip():
                    logger.warning(f"Empty combined text found in row {idx}, skipping")
                    continue
                
                # Create metadata from non-text columns
                metadata = {
                    col: str(row[col])
                    for col in df.columns
                    if col not in text_columns and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Chunk text if necessary
                if len(combined_text) > chunk_size:
                    chunks = self._chunk_text(combined_text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        chunk_metadata['total_chunks'] = str(len(chunks))
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                else:
                    documents.append(Document(page_content=combined_text, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

    def search(self, query: str, k: int = 5, 
              min_relevance_score: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """
        Perform similarity search using locally stored vectors.
        
        The only API call made is to convert the search query into a vector.
        The actual similarity search happens locally.
        """
        try:
            if not self.vector_store:
                raise ValueError("No collection loaded")
                
            logger.info("Converting search query to vector via Azure OpenAI...")
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            logger.info("Performing similarity search on local vectors...")
            processed_results = []
            for doc, score in results:
                relevance = 1 - score
                if relevance >= min_relevance_score:
                    processed_results.append((doc.page_content, relevance, doc.metadata))
            
            logger.info(f"Found {len(processed_results)} relevant results")
            return processed_results
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            raise

    # ... (rest of the methods remain the same) ...
