class ChromaVectorStore:
    """ChromaDB vector store manager with multi-column support"""
    
    def __init__(self, env: OSEnv):
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None

    @staticmethod
    def _read_csv_safely(file_path: Path) -> pd.DataFrame:
        """Read CSV with multiple encoding attempts and error handling."""
        encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
        errors = []
        
        for encoding in encodings:
            try:
                logger.info(f"Attempting to read CSV with {encoding} encoding")
                return pd.read_csv(file_path, encoding=encoding)
            except Exception as e:
                errors.append(f"{encoding}: {str(e)}")
                continue
        
        raise ValueError(f"Failed to read CSV with any encoding. Errors:\n" + "\n".join(errors))

    def create_collection(self, csv_path: Path, text_columns: List[str], 
                        chunk_size: int = 1000, chunk_overlap: int = 100,
                        separator: str = " | ") -> None:
        """
        Create Chroma collection from CSV with multiple text columns.
        
        Args:
            csv_path: Path to CSV file
            text_columns: List of column names to combine for vectorization
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            separator: String to use when combining text from multiple columns
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            df = self._read_csv_safely(csv_path)
            
            # Validate all requested columns exist
            missing_cols = [col for col in text_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
            
            logger.info(f"Processing columns: {', '.join(text_columns)}")
            documents, metadatas = self._process_csv_multi_column(
                df, text_columns, chunk_size, chunk_overlap, separator
            )
            
            self.collection_name = csv_path.stem.lower()
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                ids=[str(i) for i in range(len(documents))],
                metadatas=metadatas,
                persist_directory=persist_directory
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(documents)} documents")
            self.vector_store.persist()
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv_multi_column(
        self, df: pd.DataFrame, 
        text_columns: List[str],
        chunk_size: int = 1000, 
        chunk_overlap: int = 100,
        separator: str = " | "
    ) -> Tuple[List[Document], List[Dict]]:
        """Process CSV data with multiple text columns."""
        documents = []
        metadatas = []
        
        try:
            for idx, row in df.iterrows():
                # Combine text from all specified columns
                text_parts = []
                for col in text_columns:
                    if pd.notna(row[col]):
                        text_parts.append(f"{col}: {str(row[col]).strip()}")
                
                combined_text = separator.join(text_parts)
                if not combined_text.strip():
                    logger.warning(f"Empty combined text found in row {idx}, skipping")
                    continue
                
                # Create metadata from non-text columns
                metadata = {
                    col: str(row[col])
                    for col in df.columns
                    if col not in text_columns and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Chunk text if necessary
                if len(combined_text) > chunk_size:
                    chunks = self._chunk_text(combined_text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        chunk_metadata['total_chunks'] = str(len(chunks))
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                        metadatas.append(chunk_metadata)
                else:
                    documents.append(Document(page_content=combined_text, metadata=metadata))
                    metadatas.append(metadata)
                
                if (idx + 1) % 100 == 0:
                    logger.info(f"Processed {idx + 1} rows")
            
            return documents, metadatas
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Split text into overlapping chunks."""
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            
            # Adjust chunk to not break words
            if end < text_len:
                last_space = chunk.rfind(' ')
                if last_space != -1:
                    end = start + last_space + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
            
        return chunks

    # ... (rest of the ChromaVectorStore methods remain the same) ...

def main():
    """Main application entry point."""
    chroma_db = None
    
    try:
        # Initialize environment and ChromaVectorStore
        base_dir = Path(__file__).resolve().parent.parent
        env_dir = base_dir / "env"
        
        # Define paths
        config_path = env_dir / "config.env"
        creds_path = env_dir / "credentials.env"
        cert_path = env_dir / "cacert.pem"
        
        # Initialize environment and vector store
        env = OSEnv(
            config_file=str(config_path),
            creds_file=str(creds_path),
            certificate_path=str(cert_path)
        )
        
        chroma_db = ChromaVectorStore(env)
        
        while True:
            print("\nAvailable commands:")
            print("1. Create new collection from CSV")
            print("2. Load existing collection")
            print("3. List all collections")
            print("4. Delete collection")
            print("5. Search current collection")
            print("6. Show collection statistics")
            print("7. Show environment variables")
            print("8. Exit")
            
            choice = input("\nEnter your choice (1-8): ").strip()
            
            try:
                if choice == '1':
                    # Create new collection with multiple columns
                    csv_path = Path(input("Enter CSV file path: ").strip())
                    df = chroma_db._read_csv_safely(csv_path)
                    
                    print("\nAvailable columns:")
                    for idx, col in enumerate(df.columns, 1):
                        print(f"{idx}. {col}")
                    
                    # Select multiple columns
                    col_indices = input("\nEnter column numbers (comma-separated): ").strip()
                    selected_cols = [
                        df.columns[int(idx.strip()) - 1] 
                        for idx in col_indices.split(',')
                    ]
                    
                    print(f"\nSelected columns: {', '.join(selected_cols)}")
                    separator = input("Enter separator for combining columns (default ' | '): ").strip() or " | "
                    
                    chunk_size = int(input("Enter chunk size (default 1000): ") or "1000")
                    chunk_overlap = int(input("Enter chunk overlap (default 100): ") or "100")
                    
                    chroma_db.create_collection(
                        csv_path, 
                        selected_cols, 
                        chunk_size, 
                        chunk_overlap,
                        separator
                    )
                
                # ... (rest of the menu options remain the same) ...
                
            except Exception as e:
                logger.error(f"Operation failed: {str(e)}")
                print(f"Error: {str(e)}")
                
    except Exception as e:
        logger.error(f"Application error: {str(e)}")
        print(f"Error: {str(e)}")
        
    finally:
        if chroma_db is not None:
            chroma_db.close()

if __name__ == "__main__":
    main()
