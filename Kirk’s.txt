def _init_embeddings(self) -> AzureOpenAIEmbeddings:
    """Initialize Azure OpenAI embeddings using direct endpoint."""
    try:
        # Get Azure AD token for authentication
        token = self.env.get_azure_token(force_refresh=True)
        
        # Get required configuration
        azure_endpoint = str(self.env.get("AZURE_OPENAI_ENDPOINT"))
        deployment = str(self.env.get("AZURE_EMBEDDING_DEPLOYMENT"))
        api_version = str(self.env.get("AZURE_API_VERSION", "2024-02-15"))
        
        logger.info(f"Initializing embeddings with endpoint: {azure_endpoint}")
        logger.info(f"Using deployment: {deployment}")
        
        # Initialize embeddings with direct endpoint configuration
        embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=azure_endpoint,
            deployment=deployment,
            api_version=api_version,
            azure_ad_token=token,
            openai_api_type="azure_ad",
            model=deployment  # Use deployment name as model name
        )
        
        # Test the connection
        logger.info("Testing embeddings connection...")
        test_embedding = embeddings.embed_query("test")
        logger.info(f"Successfully connected to Azure OpenAI embeddings. Dimension: {len(test_embedding)}")
        
        return embeddings
        
    except Exception as e:
        logger.error(f"Failed to initialize Azure OpenAI embeddings: {str(e)}")
        raise

def create_collection(self, csv_path: Path, text_columns: List[str], 
                     chunk_size: int = 1000, chunk_overlap: int = 100,
                     separator: str = " | ", batch_size: int = 100) -> None:
    """Create collection with direct Azure OpenAI embeddings."""
    try:
        logger.info(f"Reading CSV file: {csv_path}")
        df = self._read_csv_safely(csv_path)
        
        missing_cols = [col for col in text_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
        
        logger.info(f"Processing columns: {', '.join(text_columns)}")
        all_documents = []
        total_rows = len(df)
        
        # Process in batches
        for start_idx in range(0, total_rows, batch_size):
            end_idx = min(start_idx + batch_size, total_rows)
            batch_df = df.iloc[start_idx:end_idx]
            
            batch_documents = self._process_csv_multi_column(
                batch_df, text_columns, chunk_size, chunk_overlap, separator
            )
            all_documents.extend(batch_documents)
            
            logger.info(f"Processed rows {start_idx + 1} to {end_idx} of {total_rows}")
        
        self.collection_name = csv_path.stem.lower()
        persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
        os.makedirs(persist_directory, exist_ok=True)
        
        # Create the vector store with the embeddings
        logger.info("Creating Chroma collection...")
        self.vector_store = Chroma.from_documents(
            documents=all_documents,
            embedding=self.embeddings,
            collection_name=self.collection_name,
            persist_directory=persist_directory,
            client=self.client
        )
        
        logger.info(f"Created collection '{self.collection_name}' with {len(all_documents)} documents")
        self.vector_store.persist()
        logger.info(f"Vectors stored locally in {persist_directory}")
        
    except Exception as e:
        logger.error(f"Collection creation failed: {str(e)}")
        raise
