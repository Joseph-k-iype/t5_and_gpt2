I'll help you create a system to generate embeddings for both CSVs and use ChromaDB for semantic matching. Here's the solution:

```python
import os
import pandas as pd
from azure.identity import DefaultAzureCredential
import requests
import logging
import urllib.parse
import json
import chromadb
from chromadb.utils import embedding_functions
import numpy as np
from typing import List, Dict

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Configure SSL and no proxy for Azure services
NO_PROXY_DOMAINS = [
    'cognitiveservices.azure.com',
    'search.windows.net',
    'openai.azure.com',
    'core.windows.net',
    'azurewebsites.net'
]
os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)

# Create a session with SSL verification
session = requests.Session()
session.verify = 'cacert.pem'
session.proxies = {
    'http': None,
    'https': None
}

class CustomAzureEmbeddingFunction(embedding_functions.EmbeddingFunction):
    def __init__(self, azure_endpoint: str, credential=None):
        self.azure_endpoint = azure_endpoint
        self.credential = credential or DefaultAzureCredential()
        self.session = session
        self.token = self.credential.get_token("https://cognitiveservices.azure.com/.default")

    def __call__(self, texts: List[str]) -> List[List[float]]:
        if not texts:
            return []
        
        headers = {
            'Authorization': f'Bearer {self.token.token}',
            'Content-Type': 'application/json'
        }
        
        api_url = f"{self.azure_endpoint}/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-02-01"
        
        try:
            response = self.session.post(
                api_url,
                headers=headers,
                json={"input": texts}
            )
            
            if response.status_code == 200:
                response_data = response.json()
                return [item['embedding'] for item in response_data['data']]
            else:
                logger.error(f"Error getting embeddings: {response.text}")
                raise Exception(f"Failed to get embeddings: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Error in embedding generation: {str(e)}")
            raise

class SemanticMatcher:
    def __init__(self, azure_endpoint: str):
        self.azure_endpoint = azure_endpoint
        self.embedding_function = CustomAzureEmbeddingFunction(azure_endpoint)
        self.chroma_client = chromadb.Client()
        
    def process_csv_to_chroma(self, csv_path: str, collection_name: str) -> None:
        """Load CSV data into ChromaDB collection"""
        df = pd.read_csv(csv_path)
        
        # Create or get collection
        collection = self.chroma_client.get_or_create_collection(
            name=collection_name,
            embedding_function=self.embedding_function
        )
        
        # Combine name and definition for better semantic matching
        documents = [f"Name: {name}\nDefinition: {definition}" 
                    for name, definition in zip(df['name'], df['definition'])]
        
        # Generate IDs
        ids = [str(i) for i in range(len(df))]
        
        # Store original data for reference
        metadatas = [{"name": name, "definition": definition} 
                    for name, definition in zip(df['name'], df['definition'])]
        
        # Add to collection
        collection.add(
            documents=documents,
            ids=ids,
            metadatas=metadatas
        )
        
        logger.info(f"Added {len(df)} items to collection {collection_name}")

    def find_matches(self, 
                    lookup_csv: str, 
                    reference_collection: str, 
                    n_results: int = 5) -> pd.DataFrame:
        """Find semantic matches for lookup CSV in reference collection"""
        # Load lookup CSV
        lookup_df = pd.read_csv(lookup_csv)
        
        # Get reference collection
        collection = self.chroma_client.get_collection(
            name=reference_collection,
            embedding_function=self.embedding_function
        )
        
        results = []
        
        # Process each row in lookup CSV
        for idx, row in lookup_df.iterrows():
            query = f"Name: {row['name']}\nDefinition: {row['definition']}"
            
            # Query collection
            matches = collection.query(
                query_texts=[query],
                n_results=n_results
            )
            
            # Process matches
            for i in range(len(matches['ids'][0])):
                results.append({
                    'lookup_name': row['name'],
                    'lookup_definition': row['definition'],
                    'matched_name': matches['metadatas'][0][i]['name'],
                    'matched_definition': matches['metadatas'][0][i]['definition'],
                    'similarity_rank': i + 1
                })
        
        # Convert results to DataFrame
        results_df = pd.DataFrame(results)
        return results_df

def main():
    # Replace with your Azure endpoint
    azure_endpoint = "your_endpoint"
    
    # Initialize matcher
    matcher = SemanticMatcher(azure_endpoint)
    
    try:
        # Load reference data into ChromaDB
        print("Loading reference data...")
        matcher.process_csv_to_chroma(
            csv_path='reference.csv',  # Your first CSV
            collection_name='reference_terms'
        )
        
        # Find matches for lookup data
        print("Finding matches...")
        matches_df = matcher.find_matches(
            lookup_csv='lookup.csv',  # Your second CSV
            reference_collection='reference_terms',
            n_results=5  # Number of matches to return per term
        )
        
        # Save results
        matches_df.to_csv('semantic_matches.csv', index=False)
        print("Results saved to semantic_matches.csv")
        
        # Display sample results
        print("\nSample matches:")
        print(matches_df.head())
        
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")
        raise

if __name__ == "__main__":
    main()
```

To use this code:

1. Save your reference data (first CSV) as 'reference.csv'
2. Save your lookup data (second CSV) as 'lookup.csv'
3. Replace "your_endpoint" with your Azure OpenAI endpoint
4. Run the script

The code will:
1. Create embeddings for all terms and definitions using your Azure OpenAI setup
2. Store the reference data in ChromaDB with the embeddings
3. For each term in the lookup CSV, find the top 5 semantic matches
4. Save all matches to 'semantic_matches.csv'

The output CSV will contain:
- lookup_name: Original name from lookup CSV
- lookup_definition: Original definition from lookup CSV
- matched_name: Matched name from reference CSV
- matched_definition: Matched definition from reference CSV
- similarity_rank: Rank of the match (1 is best match)

You can adjust the number of matches returned by changing n_results in the main() function.

Some additional features you might want to add:
1. Similarity score threshold filtering
2. Custom weighting between name and definition matching
3. Batch processing for large CSVs
4. Export of matched embeddings

Would you like me to add any of these features or explain any part in more detail?​​​​​​​​​​​​​​​​
