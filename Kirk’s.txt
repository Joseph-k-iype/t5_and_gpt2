import os
import time
import logging
import pandas as pd
import chromadb
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dotenv import dotenv_values
from azure.identity import ClientSecretCredential
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from chromadb.config import Settings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chromadb.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ChromaVectorStore:
    """ChromaDB vector store manager with Azure embeddings and local storage"""
    
    def __init__(self, env: OSEnv):
        """
        Initialize ChromaVectorStore with environment configuration.
        
        Args:
            env: OSEnv instance containing environment configuration
        """
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None
        self.client = self._init_chroma_client()
        logger.info("ChromaVectorStore initialized successfully")

    def _init_chroma_client(self) -> chromadb.Client:
        """
        Initialize ChromaDB client for local storage with telemetry disabled.
        
        Returns:
            chromadb.Client: Configured ChromaDB client
        """
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            client = chromadb.Client(Settings(
                anonymized_telemetry=False,
                is_persistent=True,
                persist_directory=persist_directory,
                allow_reset=True
            ))
            
            logger.info(f"Initialized local ChromaDB client at {persist_directory}")
            return client
            
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client: {str(e)}")
            raise

    def _validate_azure_credentials(self) -> None:
        """Validate required Azure OpenAI credentials."""
        required_vars = [
            "AZURE_TENANT_ID",
            "AZURE_CLIENT_ID",
            "AZURE_CLIENT_SECRET",
            "AZURE_EMBEDDING_DEPLOYMENT",
            "AZURE_EMBEDDING_MODEL",
            "AZURE_API_VERSION",
            "AZURE_OPENAI_ENDPOINT"
        ]
        
        missing = [var for var in required_vars if not self.env.get(var)]
        if missing:
            raise ValueError(f"Missing required Azure variables: {', '.join(missing)}")
        logger.info("Azure credentials validated successfully")

    def _init_embeddings(self) -> AzureOpenAIEmbeddings:
        """
        Initialize Azure OpenAI embeddings with connection testing.
        
        Returns:
            AzureOpenAIEmbeddings: Configured embeddings instance
        """
        try:
            token = self.env.get_azure_token(force_refresh=True)
            
            embeddings = AzureOpenAIEmbeddings(
                deployment=str(self.env.get("AZURE_EMBEDDING_DEPLOYMENT")),
                model=str(self.env.get("AZURE_EMBEDDING_MODEL")),
                api_version=str(self.env.get("AZURE_API_VERSION")),
                azure_endpoint=str(self.env.get("AZURE_OPENAI_ENDPOINT")),
                azure_ad_token=token,
                chunk_size=16  # Process in smaller batches
            )
            
            # Test the embeddings connection
            try:
                logger.info("Testing Azure OpenAI connection...")
                test_embedding = embeddings.embed_query("test")
                logger.info(f"Successfully connected to Azure OpenAI (embedding dimension: {len(test_embedding)})")
            except Exception as e:
                raise ConnectionError(f"Failed to connect to Azure OpenAI: {str(e)}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Failed to initialize embeddings: {str(e)}")
            raise

    @staticmethod
    def _read_csv_safely(file_path: Path) -> pd.DataFrame:
        """
        Read CSV with multiple encoding attempts.
        
        Args:
            file_path: Path to CSV file
            
        Returns:
            pd.DataFrame: Loaded CSV data
        """
        encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
        errors = []
        
        for encoding in encodings:
            try:
                logger.info(f"Attempting to read CSV with {encoding} encoding")
                return pd.read_csv(file_path, encoding=encoding)
            except Exception as e:
                errors.append(f"{encoding}: {str(e)}")
                continue
        
        raise ValueError(f"Failed to read CSV with any encoding. Errors:\n" + "\n".join(errors))

    def create_collection(self, csv_path: Path, text_columns: List[str], 
                        chunk_size: int = 1000, chunk_overlap: int = 100,
                        separator: str = " | ", batch_size: int = 100) -> None:
        """
        Create Chroma collection with batched processing.
        
        Args:
            csv_path: Path to CSV file
            text_columns: List of columns to combine for embedding
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            separator: String to use when combining text columns
            batch_size: Number of documents to process at once
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            df = self._read_csv_safely(csv_path)
            
            missing_cols = [col for col in text_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
            
            logger.info(f"Processing columns: {', '.join(text_columns)}")
            all_documents = []
            total_rows = len(df)
            
            # Process in batches
            for start_idx in range(0, total_rows, batch_size):
                end_idx = min(start_idx + batch_size, total_rows)
                batch_df = df.iloc[start_idx:end_idx]
                
                batch_documents = self._process_csv_multi_column(
                    batch_df, text_columns, chunk_size, chunk_overlap, separator
                )
                all_documents.extend(batch_documents)
                
                logger.info(f"Processed rows {start_idx + 1} to {end_idx} of {total_rows}")
            
            self.collection_name = csv_path.stem.lower()
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            
            # Create embeddings and store locally
            logger.info("Creating embeddings (this will make API calls to Azure OpenAI)...")
            self.vector_store = Chroma.from_documents(
                documents=all_documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                persist_directory=persist_directory,
                client=self.client
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(all_documents)} documents")
            
            # Persist to local storage
            self.vector_store.persist()
            logger.info(f"Vectors stored locally in {persist_directory}")
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv_multi_column(
        self, df: pd.DataFrame, 
        text_columns: List[str],
        chunk_size: int = 1000, 
        chunk_overlap: int = 100,
        separator: str = " | "
    ) -> List[Document]:
        """
        Process CSV data batch into documents with metadata.
        
        Args:
            df: DataFrame containing the batch of rows
            text_columns: Columns to combine for text content
            chunk_size: Maximum size of text chunks
            chunk_overlap: Overlap between chunks
            separator: String to use when combining columns
            
        Returns:
            List[Document]: Processed documents with metadata
        """
        documents = []
        
        try:
            for idx, row in df.iterrows():
                # Combine text from specified columns
                text_parts = []
                for col in text_columns:
                    if pd.notna(row[col]):
                        text = str(row[col]).strip()
                        text = text.replace('\n', ' ').replace('\r', ' ')
                        text_parts.append(f"{col}: {text}")
                
                combined_text = separator.join(text_parts)
                if not combined_text.strip():
                    logger.warning(f"Empty combined text found in row {idx}, skipping")
                    continue
                
                # Create metadata from non-text columns
                metadata = {
                    col: str(row[col])
                    for col in df.columns
                    if col not in text_columns and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Chunk text if necessary
                if len(combined_text) > chunk_size:
                    chunks = self._chunk_text(combined_text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        chunk_metadata['total_chunks'] = str(len(chunks))
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                else:
                    documents.append(Document(page_content=combined_text, metadata=metadata))
            
            return documents
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """
        Split text into overlapping chunks.
        
        Args:
            text: Text to split
            chunk_size: Maximum size of each chunk
            overlap: Number of characters to overlap between chunks
            
        Returns:
            List[str]: List of text chunks
        """
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            
            # Adjust chunk to not break words
            if end < text_len:
                last_space = chunk.rfind(' ')
                if last_space != -1:
                    end = start + last_space + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
            
        return chunks

    def load_existing_collection(self, collection_name: str) -> None:
        """
        Load an existing collection from local storage.
        
        Args:
            collection_name: Name of the collection to load
        """
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            if not os.path.exists(persist_directory):
                raise ValueError(f"Persist directory not found: {persist_directory}")
            
            collection_path = Path(persist_directory) / collection_name
            if not collection_path.exists():
                raise ValueError(f"Collection '{collection_name}' not found in {persist_directory}")
            
            self.collection_name = collection_name
            self.vector_store = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=persist_directory,
                client=self.client
            )
            
            collection_count = len(self.vector_store.get()['ids'])
            logger.info(f"Loaded existing collection '{collection_name}' with {collection_count} documents")
            
        except Exception as e:
            logger.error(f"Failed to load collection: {str(e)}")
            raise

    def delete_collection(self, collection_name: str) -> None:
        """
        Delete a collection from local storage.
        
        Args:
            collection_name: Name of the collection to delete
        """
        try:
            if collection_name in self.client.list_collections():
                self.client.delete_collection(collection_name)
                logger.info(f"Deleted collection '{collection_name}'")
                
                # Reset current store if it was the active collection
                if self.collection_name == collection_name:
                    self.vector_store = None
                    self.collection_name = None
            else:
                raise ValueError(f"Collection '{collection_name}' not found")
                
        except Exception as e:
            logger.error(f"Failed to delete collection: {str(e)}")
            raise

    def list_collections(self) -> List[str]:
        """
        List all available collections in local storage.
        
        Returns:
            List[str]: List of collection names
        """
        try:
            collections = [col.name for col in self.client.list_collections()]
            logger.info(f"Found {len(collections)} local collections: {collections}")
            return collections
            
        except Exception as e:
            logger.error(f"Failed to list collections: {str(e)}")
            raise

    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Get statistics about the current collection.
        
        Returns:
            Dict[str, Any]: Collection statistics
        """
        try:
            if not self.collection_name or not self.vector_store:
                raise ValueError("No collection loaded")
            
            collection = self.client.get_collection(self.collection_name)
            count = collection.count()
            
            # Get sample document to determine embedding dimension
            if count > 0:
                sample = collection.peek(1)
                embedding_dim = len(sample['embeddings'][0]) if sample['embeddings'] else None
            else:
                embedding_dim = None
            
            stats = {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_dimension": embedding_dim,
                "storage_path": self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            }
            
            logger.info(f"Collection statistics: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get collection statistics: {str(e)}")
            raise

    def search(self, query: str, k: int = 5, 
              min_relevance_score: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """
        Perform similarity search using locally stored vectors.
        
        Args:
            query: Search query
            k: Number of results to return
            min_relevance_score: Minimum similarity score (0-1)
            
        Returns:
            List[Tuple[str, float, Dict]]: List of (content, score, metadata) tuples
        """
        try:
            if not self.vector_store:
                raise ValueError("No collection loaded")
                
            logger.info("Converting search query to vector via Azure OpenAI...")
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            logger.info("Performing similarity search on local vectors...")
            processed_results = []
            for doc, score in results:
                relevance = 1 - score  # Convert distance to similarity score
                if relevance >= min_relevance_score:
                    processed_results.append((doc.page_content, relevance, doc.metadata))
            
            logger.info(f"Found {len(processed_results)} relevant results")
            return processed_results
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            raise

    def add_texts(self, texts: List[str], metadatas: Optional[List[Dict]] = None) -> None:
        """
        Add new texts to an existing collection.
        
        Args:
            texts: List of texts to add
            metadatas: Optional list of metadata dictionaries for each text
        """
        try:
            if not self.vector_store:
                raise ValueError("No collection loaded")
            
            logger.info(f"Adding {len(texts)} new texts to collection '{self.collection_name}'")
            
            if metadatas and len(metadatas) != len(texts):
                raise ValueError("Number of metadata items must match number of texts")
            
            # Create Document objects
            documents = []
            for i, text in enumerate(texts):
                metadata = metadatas[i] if metadatas else {}
                documents.append(Document(page_content=text, metadata=metadata))
            
            # Add to vector store
            logger.info("Creating embeddings for new texts...")
            self.vector_store.add_documents(documents)
            
            # Persist changes
            self.vector_store.persist()
            logger.info("New texts added and persisted to local storage")
            
        except Exception as e:
            logger.error(f"Failed to add texts: {str(e)}")
            raise

    def update_collection_settings(self, chunk_size: Optional[int] = None,
                                chunk_overlap: Optional[int] = None) -> None:
        """
        Update collection processing settings.
        
        Args:
            chunk_size: New maximum chunk size for text splitting
            chunk_overlap: New overlap size for text chunks
        """
        try:
            if not self.collection_name:
                raise ValueError("No collection loaded")
            
            if chunk_size is not None:
                self.chunk_size = chunk_size
                logger.info(f"Updated chunk size to {chunk_size}")
            
            if chunk_overlap is not None:
                self.chunk_overlap = chunk_overlap
                logger.info(f"Updated chunk overlap to {chunk_overlap}")
            
        except Exception as e:
            logger.error(f"Failed to update settings: {str(e)}")
            raise

    def refresh_token(self) -> None:
        """Refresh the Azure AD token for embeddings."""
        try:
            token = self.env.get_azure_token(force_refresh=True)
            self.embeddings = AzureOpenAIEmbeddings(
                deployment=str(self.env.get("AZURE_EMBEDDING_DEPLOYMENT")),
                model=str(self.env.get("AZURE_EMBEDDING_MODEL")),
                api_version=str(self.env.get("AZURE_API_VERSION")),
                azure_endpoint=str(self.env.get("AZURE_OPENAI_ENDPOINT")),
                azure_ad_token=token,
                chunk_size=16
            )
            logger.info("Azure AD token refreshed successfully")
            
        except Exception as e:
            logger.error(f"Failed to refresh token: {str(e)}")
            raise

    def backup_collection(self, backup_dir: Optional[str] = None) -> None:
        """
        Create a backup of the current collection.
        
        Args:
            backup_dir: Optional custom backup directory
        """
        try:
            if not self.collection_name:
                raise ValueError("No collection loaded")
            
            source_dir = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            if backup_dir is None:
                backup_dir = os.path.join(source_dir, "backups", 
                                        f"{self.collection_name}_{int(time.time())}")
            
            os.makedirs(backup_dir, exist_ok=True)
            
            # Ensure collection is persisted before backup
            self.vector_store.persist()
            
            # Copy collection files
            collection_path = os.path.join(source_dir, self.collection_name)
            import shutil
            shutil.copytree(collection_path, 
                           os.path.join(backup_dir, self.collection_name),
                           dirs_exist_ok=True)
            
            logger.info(f"Collection backup created at {backup_dir}")
            
        except Exception as e:
            logger.error(f"Backup failed: {str(e)}")
            raise

    def restore_collection(self, backup_path: str) -> None:
        """
        Restore a collection from backup.
        
        Args:
            backup_path: Path to the backup directory
        """
        try:
            if not os.path.exists(backup_path):
                raise ValueError(f"Backup path not found: {backup_path}")
            
            # Get collection name from backup path
            collection_name = os.path.basename(backup_path.rstrip("/"))
            
            # Close current collection if open
            if self.vector_store:
                self.vector_store.persist()
                self.vector_store = None
            
            # Copy backup to persist directory
            persist_dir = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            target_path = os.path.join(persist_dir, collection_name)
            
            import shutil
            if os.path.exists(target_path):
                shutil.rmtree(target_path)
            shutil.copytree(backup_path, target_path)
            
            # Load the restored collection
            self.load_existing_collection(collection_name)
            logger.info(f"Collection restored from {backup_path}")
            
        except Exception as e:
            logger.error(f"Restore failed: {str(e)}")
            raise

    def close(self) -> None:
        """Clean up resources and persist data."""
        try:
            if self.vector_store:
                self.vector_store.persist()
                logger.info("Vector store persisted successfully")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")
            raise

def main():
    """Main application entry point with interactive interface."""
    chroma_db = None
    
    try:
        # Initialize paths
        base_dir = Path(__file__).resolve().parent.parent
        env_dir = base_dir / "env"
        
        # Define paths
        config_path = env_dir / "config.env"
        creds_path = env_dir / "credentials.env"
        cert_path = env_dir / "cacert.pem"
        
        # Validate required files
        required_files = {
            'config.env': config_path,
            'credentials.env': creds_path,
            'cacert.pem': cert_path
        }
        
        missing_files = [
            name for name, path in required_files.items() 
            if not path.exists()
        ]
        
        if missing_files:
            raise FileNotFoundError(
                f"Missing required files in {env_dir}:\n" +
                "\n".join(f"- {f}" for f in missing_files)
            )
        
        # Initialize environment and vector store
        logger.info("Initializing environment...")
        env = OSEnv(
            config_file=str(config_path),
            creds_file=str(creds_path),
            certificate_path=str(cert_path)
        )
        
        logger.info("Initializing ChromaDB vector store...")
        chroma_db = ChromaVectorStore(env)
        
        while True:
            print("\nAvailable commands:")
            print("1. Create new collection from CSV")
            print("2. Load existing collection")
            print("3. List all collections")
            print("4. Delete collection")
            print("5. Search current collection")
            print("6. Show collection statistics")
            print("7. Show environment variables")
            print("8. Backup current collection")
            print("9. Restore collection from backup")
            print("10. Exit")
            
            choice = input("\nEnter your choice (1-10): ").strip()
            
            try:
                if choice == '1':
                    csv_path = Path(input("Enter CSV file path: ").strip())
                    df = chroma_db._read_csv_safely(csv_path)
                    
                    print("\nAvailable columns:")
                    for idx, col in enumerate(df.columns, 1):
                        print(f"{idx}. {col}")
                    
                    col_indices = input("\nEnter column numbers (comma-separated): ").strip()
                    selected_cols = [
                        df.columns[int(idx.strip()) - 1] 
                        for idx in col_indices.split(',')
                    ]
                    
                    print(f"\nSelected columns: {', '.join(selected_cols)}")
                    separator = input("Enter separator for combining columns (default ' | '): ").strip() or " | "
                    
                    chunk_size = int(input("Enter chunk size (default 1000): ") or "1000")
                    chunk_overlap = int(input("Enter chunk overlap (default 100): ") or "100")
                    
                    chroma_db.create_collection(
                        csv_path, 
                        selected_cols, 
                        chunk_size, 
                        chunk_overlap,
                        separator
                    )
                
                elif choice == '2':
                    collections = chroma_db.list_collections()
                    if not collections:
                        print("No collections found")
                        continue
                    
                    print("\nAvailable collections:")
                    for idx, name in enumerate(collections, 1):
                        print(f"{idx}. {name}")
                    
                    col_idx = int(input("\nEnter collection number: ")) - 1
                    if 0 <= col_idx < len(collections):
                        chroma_db.load_existing_collection(collections[col_idx])
                    else:
                        print("Invalid collection number")
                
                elif choice == '3':
                    collections = chroma_db.list_collections()
                    if collections:
                        print("\nAvailable collections:")
                        for idx, name in enumerate(collections, 1):
                            print(f"{idx}. {name}")
                    else:
                        print("No collections found")
                
                elif choice == '4':
                    collections = chroma_db.list_collections()
                    if not collections:
                        print("No collections found")
                        continue
                    
                    print("\nAvailable collections:")
                    for idx, name in enumerate(collections, 1):
                        print(f"{idx}. {name}")
                    
                    col_idx = int(input("\nEnter collection number to delete: ")) - 1
                    if 0 <= col_idx < len(collections):
                        confirm = input(f"Are you sure you want to delete '{collections[col_idx]}'? (yes/no): ")
                        if confirm.lower() == 'yes':
                            chroma_db.delete_collection(collections[col_idx])
                    else:
                        print("Invalid collection number")
                
                elif choice == '5':
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    query = input("Enter search query: ")
                    k = int(input("Number of results to return (default 5): ") or "5")
                    min_score = float(input("Minimum relevance score (0-1, default 0): ") or "0")
                    
                    results = chroma_db.search(query, k, min_score)
                    print(f"\nFound {len(results)} results:")
                    for i, (text, score, metadata) in enumerate(results, 1):
                        print(f"\n{i}. Relevance Score: {score:.4f}")
                        print("Content:")
                        for part in text.split(" | "):
                            print(f"  {part}")
                        print("Metadata:")
                        for key, value in metadata.items():
                            if key not in ['chunk_index', 'total_chunks']:
                                print(f"  {key}: {value}")
                        if 'chunk_index' in metadata:
                            print(f"  Chunk: {metadata['chunk_index']}/{metadata['total_chunks']}")
                
                elif choice == '6':
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    stats = chroma_db.get_collection_stats()
                    print("\nCollection Statistics:")
                    for key, value in stats.items():
                        print(f"{key}: {value}")
                
                elif choice == '7':
                    env.list_env_vars()
                
                elif choice == '8':
                    if not chroma_db.collection_name:
                        print("Please load a collection first")
                        continue
                    
                    backup_dir = input("Enter backup directory (optional): ").strip() or None
                    chroma_db.backup_collection(backup_dir)
                
                elif choice == '9':
                    backup_path = input("Enter backup path: ").strip()
                    chroma_db.restore_collection(backup_path)
                
                elif choice == '10':
                    print("Exiting...")
                    break
                
                else:
                    print("Invalid choice")
                
            except Exception as e:
                logger.error(f"Operation failed: {str(e)}")
                print(f"Error: {str(e)}")
        
    except Exception as e:
        logger.error(f"Application error: {str(e)}")
        print(f"Error: {str(e)}")
    
    finally:
        if chroma_db is not None:
            chroma_db.close()
            logger.info("Application shutdown complete")

if __name__ == "__main__":
    main()
