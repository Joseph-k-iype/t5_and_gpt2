I'll share a production-ready version with proper error handling, logging, configuration management, and documentation. Due to length, I'll break it into multiple parts:

Part 1 - Project Structure and Config:

```python
# config.py
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class AzureConfig:
    endpoint: str
    deployment_name: str = "text-embedding-ada-002"
    api_version: str = "2024-02-01"
    proxy_username: Optional[str] = None
    proxy_password: Optional[str] = None
    proxy_host: Optional[str] = None
    proxy_port: Optional[str] = None
    cert_path: str = "cacert.pem"
    
    @property
    def proxy_url(self) -> Optional[str]:
        if all([self.proxy_username, self.proxy_password, self.proxy_host, self.proxy_port]):
            from urllib.parse import quote
            username = quote(self.proxy_username)
            password = quote(self.proxy_password)
            return f"http://{username}:{password}@{self.proxy_host}:{self.proxy_port}"
        return None

@dataclass
class ProcessingConfig:
    max_text_length: int = 5000
    n_results: int = 5
    reference_csv: str = "reference.csv"
    lookup_csv: str = "lookup.csv"
    output_csv: str = "semantic_matches.csv"
    collection_name: str = "terms"
    name_column: str = "name"
    definition_column: str = "definition"

NO_PROXY_DOMAINS = [
    'cognitiveservices.azure.com',
    'search.windows.net',
    'openai.azure.com',
    'core.windows.net',
    'azurewebsites.net'
]
```

```python
# logger.py
import logging
import sys
from datetime import datetime

def setup_logger(name: str) -> logging.Logger:
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # Create handlers
    c_handler = logging.StreamHandler(sys.stdout)
    f_handler = logging.FileHandler(f'semantic_matcher_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
    
    # Create formatters and add it to handlers
    log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    c_handler.setFormatter(log_format)
    f_handler.setFormatter(log_format)
    
    # Add handlers to the logger
    logger.addHandler(c_handler)
    logger.addHandler(f_handler)
    
    return logger
```

Part 2 - Core Embedding and Processing:

```python
# embeddings.py
import requests
from typing import List, Optional
import numpy as np
from azure.identity import DefaultAzureCredential
from config import AzureConfig
from logger import setup_logger

logger = setup_logger(__name__)

class EmbeddingGenerator:
    def __init__(self, config: AzureConfig):
        self.config = config
        self.credential = DefaultAzureCredential()
        self._setup_session()
        
    def _setup_session(self):
        """Setup request session with proper SSL and proxy configuration"""
        self.session = requests.Session()
        self.session.verify = self.config.cert_path
        
        # Set proxy if configured
        if self.config.proxy_url:
            self.session.proxies = {
                'http': None,
                'https': None
            }
            
        # Set no-proxy domains
        os.environ['NO_PROXY'] = ','.join(NO_PROXY_DOMAINS)
        
    def _get_token(self) -> str:
        """Get fresh Azure AD token"""
        token = self.credential.get_token("https://cognitiveservices.azure.com/.default")
        return token.token
        
    def generate(self, texts: List[str], max_length: int = 5000) -> Optional[List[List[float]]]:
        """Generate embeddings for given texts"""
        try:
            # Truncate texts
            truncated_texts = [text[:max_length] for text in texts]
            
            headers = {
                'Authorization': f'Bearer {self._get_token()}',
                'Content-Type': 'application/json'
            }
            
            api_url = (f"{self.config.endpoint}/openai/deployments/{self.config.deployment_name}"
                      f"/embeddings?api-version={self.config.api_version}")
            
            response = self.session.post(
                api_url,
                headers=headers,
                json={"input": truncated_texts}
            )
            
            if response.status_code == 200:
                response_data = response.json()
                embeddings = [item['embedding'] for item in response_data['data']]
                logger.debug(f"Successfully generated embeddings. Dimension: {len(embeddings[0])}")
                return embeddings
            else:
                logger.error(f"Failed to generate embeddings: {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            return None
```

Part 3 - Semantic Matching:

```python
# semantic_matcher.py
import pandas as pd
import chromadb
from typing import List, Dict, Optional
from config import ProcessingConfig, AzureConfig
from embeddings import EmbeddingGenerator
from logger import setup_logger

logger = setup_logger(__name__)

class SemanticMatcher:
    def __init__(self, azure_config: AzureConfig, processing_config: ProcessingConfig):
        self.azure_config = azure_config
        self.processing_config = processing_config
        self.embedding_generator = EmbeddingGenerator(azure_config)
        self.chroma_client = chromadb.Client()
        
    def _prepare_collection(self) -> chromadb.Collection:
        """Prepare ChromaDB collection"""
        try:
            # Delete if exists
            try:
                self.chroma_client.delete_collection(self.processing_config.collection_name)
            except:
                pass
            
            # Create new collection
            return self.chroma_client.create_collection(name=self.processing_config.collection_name)
        except Exception as e:
            logger.error(f"Error preparing collection: {str(e)}")
            raise
            
    def _process_reference_data(self, collection: chromadb.Collection, df: pd.DataFrame) -> bool:
        """Process reference data and add to collection"""
        try:
            # Prepare texts
            texts = [
                f"Name: {row[self.processing_config.name_column]}\n"
                f"Definition: {row[self.processing_config.definition_column]}"
                for _, row in df.iterrows()
            ]
            
            # Generate embeddings
            embeddings = self.embedding_generator.generate(
                texts, 
                max_length=self.processing_config.max_text_length
            )
            
            if embeddings is None:
                raise Exception("Failed to generate reference embeddings")
                
            # Add to collection
            collection.add(
                embeddings=embeddings,
                documents=texts,
                ids=[str(i) for i in range(len(df))],
                metadatas=[{
                    "name": row[self.processing_config.name_column],
                    "definition": row[self.processing_config.definition_column]
                } for _, row in df.iterrows()]
            )
            
            return True
            
        except Exception as e:
            logger.error(f"Error processing reference data: {str(e)}")
            return False
            
    def _process_lookup_terms(self, 
                            collection: chromadb.Collection, 
                            df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """Process lookup terms and find matches"""
        try:
            results = []
            
            for idx, row in df.iterrows():
                query_text = (f"Name: {row[self.processing_config.name_column]}\n"
                            f"Definition: {row[self.processing_config.definition_column]}")
                
                # Generate embedding for query
                query_embeddings = self.embedding_generator.generate(
                    [query_text],
                    max_length=self.processing_config.max_text_length
                )
                
                if query_embeddings is None:
                    logger.error(f"Failed to generate embedding for lookup term {idx}")
                    continue
                    
                # Query collection
                matches = collection.query(
                    query_embeddings=[query_embeddings[0]],
                    n_results=self.processing_config.n_results
                )
                
                # Process matches
                for i in range(len(matches['ids'][0])):
                    results.append({
                        'lookup_name': row[self.processing_config.name_column],
                        'lookup_definition': row[self.processing_config.definition_column],
                        'matched_name': matches['metadatas'][0][i]['name'],
                        'matched_definition': matches['metadatas'][0][i]['definition'],
                        'similarity_rank': i + 1
                    })
                
                if idx % 10 == 0:
                    logger.info(f"Processed {idx} lookup terms...")
            
            return pd.DataFrame(results)
            
        except Exception as e:
            logger.error(f"Error processing lookup terms: {str(e)}")
            return None
            
    def process(self) -> Optional[pd.DataFrame]:
        """Main processing function"""
        try:
            logger.info("Starting semantic matching process...")
            
            # Load data
            reference_df = pd.read_csv(self.processing_config.reference_csv)
            lookup_df = pd.read_csv(self.processing_config.lookup_csv)
            
            logger.info(f"Loaded reference data: {len(reference_df)} rows")
            logger.info(f"Loaded lookup data: {len(lookup_df)} rows")
            
            # Prepare collection
            collection = self._prepare_collection()
            
            # Process reference data
            if not self._process_reference_data(collection, reference_df):
                raise Exception("Failed to process reference data")
                
            # Process lookup terms
            results_df = self._process_lookup_terms(collection, lookup_df)
            
            if results_df is not None:
                results_df.to_csv(self.processing_config.output_csv, index=False)
                logger.info(f"Results saved to {self.processing_config.output_csv}")
                
            return results_df
            
        except Exception as e:
            logger.error(f"Error in main process: {str(e)}")
            return None
```

Part 4 - Main Script:

```python
# main.py
from config import AzureConfig, ProcessingConfig
from semantic_matcher import SemanticMatcher
from logger import setup_logger
import sys

logger = setup_logger(__name__)

def main():
    try:
        # Configure
        azure_config = AzureConfig(
            endpoint="your_endpoint",
            deployment_name="text-embedding-ada-002",
            cert_path="cacert.pem"
        )
        
        processing_config = ProcessingConfig(
            reference_csv="reference.csv",
            lookup_csv="lookup.csv",
            output_csv="semantic_matches.csv",
            n_results=5
        )
        
        # Initialize and run matcher
        matcher = SemanticMatcher(azure_config, processing_config)
        results_df = matcher.process()
        
        if results_df is not None:
            logger.info("Processing completed successfully")
            logger.info(f"\nSample matches:\n{results_df.head()}")
            return 0
        else:
            logger.error("Processing failed")
            return 1
            
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
```

To use this code:

1. Create a new project directory and add all these files
2. Install requirements:
```bash
pip install pandas azure-identity requests chromadb numpy
```

3. Create a .env file (optional):
```
AZURE_ENDPOINT=your_endpoint
PROXY_USERNAME=your_proxy_username
PROXY_PASSWORD=your_proxy_password
PROXY_HOST=your_proxy_host
PROXY_PORT=your_proxy_port
```

4. Place your CSV files and cacert.pem in the project directory

5. Run the script:
```bash
python main.py
```

Features of this production-ready version:
1. Proper error handling and logging
2. Configuration management
3. Modular design
4. Type hints
5. Documentation
6. Separate concerns (config, logging, embedding, matching)
7. Production-level error handling
8. Progress logging
9. File logging
10. Clean interface

Let me know if you need any clarification or adjustments!​​​​​​​​​​​​​​​​
