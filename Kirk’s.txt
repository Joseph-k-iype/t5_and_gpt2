import re
import csv
import pandas as pd
import string
import os
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize
import spacy
from typing import List, Dict, Set, Tuple, Union, Optional


class LexicalMatcher:
    """
    A class to generate robust regex patterns for lexical matching based on
    terms, definitions, related terms, and acronyms using spaCy and NLTK.
    """
    
    def __init__(self, spacy_model: str = 'en_core_web_sm', nltk_data_path: str = None):
        """
        Initialize the LexicalMatcher with NLP tools.
        
        Args:
            spacy_model: The spaCy model to use (default: small English model)
            nltk_data_path: Custom path to NLTK data (optional)
        """
        # Configure NLTK to use custom data path if provided
        if nltk_data_path:
            nltk.data.path.insert(0, nltk_data_path)
            print(f"Using NLTK data from: {nltk_data_path}")
        else:
            # Check for default location of NLTK resources
            try:
                nltk.data.find('corpora/stopwords')
                nltk.data.find('corpora/wordnet')
                nltk.data.find('punkt')
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                print("NLTK data not found. Downloading required resources...")
                nltk.download('stopwords')
                nltk.download('wordnet')
                nltk.download('punkt')
                nltk.download('averaged_perceptron_tagger')
        
        # Load spaCy model
        try:
            self.nlp = spacy.load(spacy_model)
            print(f"Loaded spaCy model: {spacy_model}")
        except OSError:
            # If model is not installed, provide instructions
            print(f"SpaCy model {spacy_model} not found.")
            print(f"Please install it with: python -m spacy download {spacy_model}")
            raise
        
        # Initialize NLTK tools
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
        # Dictionary to store generated patterns
        self.patterns = {}
    
    def _escape_regex(self, text: str) -> str:
        """
        Escape special regex characters.
        
        Args:
            text: String to escape
            
        Returns:
            Escaped string safe for regex pattern
        """
        return re.escape(text)
    
    def _get_wordnet_pos(self, treebank_tag: str) -> str:
        """
        Convert Penn Treebank POS tags to WordNet POS tags.
        
        Args:
            treebank_tag: Penn Treebank POS tag
            
        Returns:
            WordNet POS tag
        """
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            # Default is noun
            return wordnet.NOUN
    
    def _lemmatize_text(self, text: str) -> str:
        """
        Lemmatize text using WordNet lemmatizer with correct POS.
        
        Args:
            text: Text to lemmatize
            
        Returns:
            Lemmatized text
        """
        tokens = word_tokenize(text)
        pos_tags = nltk.pos_tag(tokens)
        
        return ' '.join([
            self.lemmatizer.lemmatize(
                word, 
                self._get_wordnet_pos(pos)
            ) for word, pos in pos_tags
        ])
    
    def _remove_vowels(self, text: str, keep_first: bool = True) -> str:
        """
        Remove vowels from text, optionally keeping the first letter.
        
        Args:
            text: Input text
            keep_first: Whether to keep the first character regardless of vowel status
            
        Returns:
            Text with vowels removed
        """
        if not text:
            return text
            
        if keep_first:
            first_char = text[0]
            rest = text[1:]
            return first_char + re.sub(r'[aeiou]', '', rest, flags=re.IGNORECASE)
        else:
            return re.sub(r'[aeiou]', '', text, flags=re.IGNORECASE)
    
    def _extract_important_words(self, text: str) -> List[str]:
        """
        Extract important words from text using spaCy.
        
        Args:
            text: Input text
            
        Returns:
            List of important words
        """
        doc = self.nlp(text)
        
        # Get nouns, proper nouns, and adjectives
        important_words = []
        for token in doc:
            # Include nouns, proper nouns, adjectives, and verbs
            if (token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'VERB') and 
                not token.is_stop and 
                len(token.text) > 2):
                important_words.append(token.text.lower())
        
        return important_words
    
    def _get_synonyms(self, word: str) -> List[str]:
        """
        Get synonyms for a word using WordNet.
        
        Args:
            word: Input word
            
        Returns:
            List of synonyms
        """
        synonyms = []
        
        # Get lemma
        word = self.lemmatizer.lemmatize(word)
        
        # Get synonyms from WordNet
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonym = lemma.name().lower().replace('_', ' ')
                if synonym != word and synonym not in synonyms:
                    synonyms.append(synonym)
        
        # Limit to reasonable number of synonyms
        return synonyms[:3]
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        Extract named entities from text using spaCy.
        
        Args:
            text: Input text
            
        Returns:
            List of named entities
        """
        doc = self.nlp(text)
        entities = [ent.text.lower() for ent in doc.ents]
        return entities
    
    def _generate_sensible_abbreviations(self, term: str) -> List[str]:
        """
        Generate sensible abbreviations using NLP techniques.
        
        Args:
            term: Input term
            
        Returns:
            List of generated abbreviations
        """
        abbreviations = []
        term = term.lower().strip()
        
        if not term:
            return abbreviations
        
        # Use spaCy to process the term
        doc = self.nlp(term)
        
        # 1. First letter acronym (for multi-word terms)
        if ' ' in term:
            # Get all words
            words = [token.text for token in doc if not token.is_punct and not token.is_space]
            
            # Standard acronym (first letter of each word)
            standard_acronym = ''.join(word[0] for word in words if word)
            if len(standard_acronym) > 1:
                abbreviations.append(standard_acronym)
            
            # Enhanced acronym (first letter of important words)
            important_words = [token.text for token in doc 
                            if token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'VERB') 
                            and not token.is_stop]
            
            if important_words:
                enhanced_acronym = ''.join(word[0] for word in important_words)
                if len(enhanced_acronym) > 1 and enhanced_acronym != standard_acronym:
                    abbreviations.append(enhanced_acronym)
        
        # 2. Consonant skeleton variants
        # Standard consonant skeleton
        consonant_skeleton = self._remove_vowels(term, keep_first=True)
        if consonant_skeleton != term and len(consonant_skeleton) > 1:
            abbreviations.append(consonant_skeleton)
        
        # Partial consonant skeleton for longer terms
        if len(term) > 5:
            partial_skeleton = term[0] + self._remove_vowels(term[1:4], keep_first=False)
            if partial_skeleton not in abbreviations and len(partial_skeleton) > 1:
                abbreviations.append(partial_skeleton)
        
        # 3. Prefixes and truncations
        # First 2-4 characters
        if len(term) > 2:
            abbreviations.append(term[:min(3, len(term))])
            
            if len(term) > 4:
                abbreviations.append(term[:min(4, len(term))])
        
        # 4. For multi-word terms, create more abbreviations
        if ' ' in term:
            words = term.split()
            
            # For two-word terms
            if len(words) == 2:
                # First 3 chars of first word + first 3 chars of second word
                if len(words[0]) >= 2 and len(words[1]) >= 2:
                    combined = words[0][:min(3, len(words[0]))] + '_' + words[1][:min(3, len(words[1]))]
                    abbreviations.append(combined)
                
                # First word's consonant skeleton + second word's consonant skeleton
                first_cons = self._remove_vowels(words[0], keep_first=True)
                second_cons = self._remove_vowels(words[1], keep_first=True)
                if len(first_cons) > 1 and len(second_cons) > 1:
                    combined_cons = first_cons + '_' + second_cons
                    abbreviations.append(combined_cons)
            
            # For all multi-word terms
            if len(words) > 1:
                # Extract nouns and important words
                doc = self.nlp(term)
                important_pos = ('NOUN', 'PROPN')
                
                # Get all nouns in the term
                nouns = [token.text for token in doc if token.pos_ in important_pos]
                
                # If we have nouns, create abbreviations using just the nouns
                if len(nouns) > 1:
                    noun_acr = ''.join(noun[0] for noun in nouns)
                    if len(noun_acr) > 1:
                        abbreviations.append(noun_acr)
                
                # For longer compound terms (3+ words), also add first letters of first and last word
                if len(words) >= 3:
                    first_last = words[0][0] + words[-1][0]
                    if len(first_last) == 2:
                        abbreviations.append(first_last)
        
        # 5. Special handling for industry-specific patterns
        # For terms like "employee number", "customer identifier", etc.
        if ' ' in term and len(term.split()) == 2:
            words = term.split()
            
            # Check if second word is an identifier word
            id_words = ['number', 'id', 'identifier', 'code', 'key', 'reference']
            
            if any(id_word == words[1].lower() for id_word in id_words):
                # Create abbreviated version with first word abbreviated + id word abbreviated
                abbr_first = self._remove_vowels(words[0], keep_first=True)
                abbr_id = words[1][0:3]  # Take first 3 chars of id word
                abbreviations.append(f"{abbr_first}_{abbr_id}")
        
        # Remove duplicates
        return list(set(abbreviations))
    
    def create_pattern(self, name: str, definition: str = "", 
                     related_terms: Union[str, List[str]] = "", 
                     acronyms: Union[str, List[str]] = "") -> Dict:
        """
        Create a robust regex pattern for lexical matching.
        
        Args:
            name: The main term
            definition: Term definition
            related_terms: Related terms (comma-separated string or list)
            acronyms: Known acronyms (comma-separated string or list)
            
        Returns:
            Dictionary with pattern and regex object
        """
        # Normalize inputs
        name = str(name).strip().lower() if name else ""
        definition = str(definition).strip().lower() if definition else ""
        
        # Convert to lists if they're strings
        if isinstance(related_terms, str):
            related_terms_list = [term.strip().lower() for term in related_terms.split(',') if term.strip()]
        else:
            related_terms_list = [term.lower().strip() for term in related_terms if term]
        
        if isinstance(acronyms, str):
            acronyms_list = [acr.strip().lower() for acr in acronyms.split(',') if acr.strip()]
        else:
            acronyms_list = [acr.lower().strip() for acr in acronyms if acr]
        
        # Pattern collection
        patterns = set()
        
        # 1. Add exact name match
        if name:
            patterns.add(self._escape_regex(name))
            
            # Add lemmatized version if different
            lemma_name = self._lemmatize_text(name)
            if lemma_name != name:
                patterns.add(self._escape_regex(lemma_name))
        
        # 2. Generate and add sensible abbreviations from name
        generated_abbreviations = self._generate_sensible_abbreviations(name)
        for abbr in generated_abbreviations:
            patterns.add(self._escape_regex(abbr))
        
        # 3. Extract important words from name using spaCy
        if name:
            important_words = self._extract_important_words(name)
            for word in important_words:
                if len(word) > 2 and word != name:
                    patterns.add(self._escape_regex(word))
        
        # 4. Add related terms and their abbreviations
        for term in related_terms_list:
            patterns.add(self._escape_regex(term))
            
            # Add lemmatized version
            lemma_term = self._lemmatize_text(term)
            if lemma_term != term:
                patterns.add(self._escape_regex(lemma_term))
            
            # Generate and add abbreviations for related terms
            term_abbrevs = self._generate_sensible_abbreviations(term)
            for abbr in term_abbrevs:
                patterns.add(self._escape_regex(abbr))
                
            # For multi-word related terms, add important words
            important_words = self._extract_important_words(term)
            for word in important_words:
                if len(word) > 2:
                    patterns.add(self._escape_regex(word))
        
        # 5. Add explicit acronyms
        for acronym in acronyms_list:
            patterns.add(self._escape_regex(acronym))
        
        # 6. Extract meaningful terms from definition using NLP
        if definition:
            # Process with spaCy to get important words
            important_def_words = self._extract_important_words(definition)
            
            # Add top 3 important words from definition
            for word in important_def_words[:3]:
                if len(word) > 3:
                    patterns.add(self._escape_regex(word))
            
            # Extract any named entities
            entities = self._extract_entities(definition)
            for entity in entities:
                if len(entity) > 2:
                    patterns.add(self._escape_regex(entity))
        
        # 7. Generate compound patterns for special cases
        if name and related_terms_list:
            # Generate name abbreviation + related term combinations
            name_abbrevs = self._generate_sensible_abbreviations(name)
            
            for rel_term in related_terms_list:
                # Look for related terms containing identifier words
                id_words = ['number', 'code', 'id', 'identifier', 'key', 'reference']
                rel_lower = rel_term.lower()
                
                for id_word in id_words:
                    if id_word in rel_lower:
                        # For each abbreviation + id_word combination
                        for abbr in name_abbrevs:
                            if len(abbr) > 1:
                                # Create patterns like "emply_num", "emply_id", etc.
                                compound = f"{abbr}[_\\-\\s]*{id_word[:3]}"
                                patterns.add(compound)
                                
                                # Also try with just the first letter of the id word
                                compound_short = f"{abbr}[_\\-\\s]*{id_word[0]}"
                                patterns.add(compound_short)
        
        # 8. Add synonyms for important terms (limited to avoid overly broad matching)
        if name:
            # Get the main words from the name
            main_words = [word for word in name.split() if len(word) > 3 and word.lower() not in self.stop_words]
            
            # Add synonyms for the main words (limited to key terms)
            for word in main_words[:1]:  # Limit to just the first main word
                synonyms = self._get_synonyms(word)
                # Add only the first 2 synonyms to avoid too broad matching
                for synonym in synonyms[:2]:
                    if len(synonym) > 3:
                        patterns.add(self._escape_regex(synonym))
        
        # Build final pattern with word boundaries and flexible separators
        pattern_list = list(patterns)
        combined_pattern = '|'.join(pattern_list)
        
        # Final pattern with:
        # - Word boundaries
        # - Optional separators between pattern components
        # - Case insensitivity
        final_pattern = f"\\b(?:{combined_pattern})(?:[_\\-\\s]*(?:{combined_pattern}))*\\b"
        
        return {
            'pattern': final_pattern,
            'regex': re.compile(final_pattern, re.IGNORECASE),
            'original_term': name,
            'generated_abbreviations': generated_abbreviations,
            'all_terms': list(patterns)
        }
    
    def process_csv(self, file_path: str, 
                   name_col: str = 'name', 
                   def_col: str = 'definition',
                   rel_terms_col: str = 'related terms',
                   acronyms_col: str = 'acronyms',
                   encoding: str = None) -> Dict:
        """
        Process a CSV file to generate regex patterns for all terms.
        
        Args:
            file_path: Path to the CSV file
            name_col: Column name for the main term
            def_col: Column name for definition
            rel_terms_col: Column name for related terms
            acronyms_col: Column name for acronyms
            encoding: File encoding (if None, will try multiple encodings)
            
        Returns:
            Dictionary of patterns keyed by term name
        """
        # Check if file exists
        if not os.path.exists(file_path):
            print(f"CSV file not found: {file_path}")
            return {}
            
        # Common encodings to try if encoding is not specified
        encodings_to_try = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252', 'ascii'] if encoding is None else [encoding]
        
        # Try reading with different encodings
        df = None
        successful_encoding = None
        
        for enc in encodings_to_try:
            try:
                print(f"Attempting to read CSV with {enc} encoding...")
                df = pd.read_csv(file_path, encoding=enc, on_bad_lines='skip')
                successful_encoding = enc
                print(f"Successfully loaded CSV with {enc} encoding")
                break
            except UnicodeDecodeError:
                print(f"Failed to read with {enc} encoding, trying next...")
            except Exception as e:
                print(f"Error reading CSV with {enc} encoding: {str(e)}")
        
        # If all encodings failed
        if df is None:
            print("Failed to read the CSV file with any encoding. Please check the file.")
            return {}
            
        # Display CSV info
        print(f"CSV loaded with {len(df)} rows and columns: {', '.join(df.columns)}")
        
        # Ensure required columns exist
        if name_col not in df.columns:
            print(f"CSV file must contain a '{name_col}' column. Available columns: {', '.join(df.columns)}")
            # Try to suggest a similar column
            possible_name_cols = [col for col in df.columns if 'name' in col.lower() or 'term' in col.lower()]
            if possible_name_cols:
                print(f"You might want to try one of these columns instead: {', '.join(possible_name_cols)}")
            return {}
        
        # Process each row
        print(f"Generating patterns for {len(df)} terms...")
        for i, row in df.iterrows():
            try:
                name = str(row.get(name_col, ''))
                if not name or pd.isna(name) or name.strip() == '':
                    print(f"Skipping row {i+1}: No name provided")
                    continue
                
                # Get other columns, handling missing values
                definition = str(row.get(def_col, '')) if def_col in df.columns and not pd.isna(row.get(def_col)) else ''
                related_terms = str(row.get(rel_terms_col, '')) if rel_terms_col in df.columns and not pd.isna(row.get(rel_terms_col)) else ''
                acronyms = str(row.get(acronyms_col, '')) if acronyms_col in df.columns and not pd.isna(row.get(acronyms_col)) else ''
                
                print(f"Processing term {i+1}/{len(df)}: {name}")
                self.patterns[name] = self.create_pattern(name, definition, related_terms, acronyms)
            except Exception as e:
                print(f"Error processing row {i+1}: {str(e)}")
                continue
        
        print(f"Successfully generated patterns for {len(self.patterns)} terms")
        return self.patterns
    
    def save_patterns_to_csv(self, output_file: str) -> None:
        """
        Save generated patterns to a CSV file.
        
        Args:
            output_file: Path to save the patterns
        """
        try:
            rows = []
            for term, pattern_data in self.patterns.items():
                rows.append({
                    'term': term,
                    'pattern': pattern_data['pattern'],
                    'abbreviations': ', '.join(pattern_data['generated_abbreviations']),
                    'all_terms': ', '.join(pattern_data['all_terms'])
                })
            
            # Create DataFrame and save to CSV
            df = pd.DataFrame(rows)
            df.to_csv(output_file, index=False)
            print(f"Successfully saved patterns to: {output_file}")
            
        except Exception as e:
            print(f"Error saving patterns to CSV: {e}")
    
    def match(self, term: str, original_term: str) -> bool:
        """
        Check if a term matches a pattern.
        
        Args:
            term: Term to check
            original_term: Original term whose pattern to use
            
        Returns:
            True if match found, False otherwise
        """
        if original_term not in self.patterns:
            return False
            
        pattern = self.patterns[original_term]['regex']
        return bool(pattern.search(term))
    
    def get_all_patterns(self) -> Dict:
        """
        Get all generated patterns.
        
        Returns:
            Dictionary of all patterns
        """
        return self.patterns


# Main execution example
if __name__ == "__main__":
    import argparse
    
    # Create command line arguments
    parser = argparse.ArgumentParser(description='Generate regex patterns from CSV terms')
    parser.add_argument('--csv', type=str, required=True, help='Path to the CSV file with terms')
    parser.add_argument('--output', type=str, default='generated_patterns.csv', help='Output CSV file path')
    parser.add_argument('--name-col', type=str, default='name', help='Column name for the term')
    parser.add_argument('--def-col', type=str, default='definition', help='Column name for definition')
    parser.add_argument('--encoding', type=str, help='CSV file encoding (will auto-detect if not specified)')
    parser.add_argument('--nltk-path', type=str, default="c/users/joseph/nltk_data", help='Path to NLTK data')
    
    args = parser.parse_args()
    
    # Configure paths
    nltk_data_path = args.nltk_path
    csv_file_path = args.csv
    output_path = args.output
    
    print(f"Starting pattern generation...")
    print(f"CSV file: {csv_file_path}")
    print(f"Output file: {output_path}")
    print(f"NLTK data path: {nltk_data_path}")
    
    # Create the matcher with your NLTK data path
    matcher = LexicalMatcher(nltk_data_path=nltk_data_path)
    
    # Process your CSV - Adjust the column names if needed
    patterns = matcher.process_csv(
        file_path=csv_file_path,
        name_col=args.name_col,
        def_col=args.def_col,
        encoding=args.encoding
    )
    
    # Save patterns to CSV for reference
    matcher.save_patterns_to_csv(output_path)
    
    # Display some example patterns
    print("\nExample patterns generated:")
    count = 0
    for term, pattern_data in patterns.items():
        print(f"\nTerm: {term}")
        print(f"Generated abbreviations: {', '.join(pattern_data['generated_abbreviations'])}")
        print(f"Pattern: {pattern_data['pattern'][:150]}..." if len(pattern_data['pattern']) > 150 
              else f"Pattern: {pattern_data['pattern']}")
        
        count += 1
        if count >= 5:  # Show just first 5 patterns
            break
    
    print(f"\nTotal patterns generated: {len(patterns)}")
    print(f"Patterns saved to: {output_path}")
