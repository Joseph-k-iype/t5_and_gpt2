import re
import csv
import pandas as pd
import string
from typing import List, Dict, Set, Tuple, Union, Optional

# Natural language processing libraries
import spacy
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.tokenize import word_tokenize


class LexicalMatcher:
    """
    A class to generate robust regex patterns for lexical matching based on
    terms, definitions, related terms, and acronyms using spaCy and NLTK.
    """
    
    def __init__(self, spacy_model: str = 'en_core_web_sm'):
        """
        Initialize the LexicalMatcher with NLP tools.
        
        Args:
            spacy_model: The spaCy model to use (default: small English model)
        """
        # Download necessary NLTK resources if not already present
        try:
            nltk.data.find('corpora/stopwords')
            nltk.data.find('corpora/wordnet')
        except LookupError:
            nltk.download('stopwords')
            nltk.download('wordnet')
            nltk.download('punkt')
            nltk.download('averaged_perceptron_tagger')
        
        # Load spaCy model
        try:
            self.nlp = spacy.load(spacy_model)
        except OSError:
            # If model is not installed, provide instructions
            print(f"SpaCy model {spacy_model} not found.")
            print(f"Please install it with: python -m spacy download {spacy_model}")
            raise
        
        # Initialize NLTK tools
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
        # Dictionary to store generated patterns
        self.patterns = {}
    
    def _escape_regex(self, text: str) -> str:
        """
        Escape special regex characters.
        
        Args:
            text: String to escape
            
        Returns:
            Escaped string safe for regex pattern
        """
        return re.escape(text)
    
    def _get_wordnet_pos(self, treebank_tag: str) -> str:
        """
        Convert Penn Treebank POS tags to WordNet POS tags.
        
        Args:
            treebank_tag: Penn Treebank POS tag
            
        Returns:
            WordNet POS tag
        """
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            # Default is noun
            return wordnet.NOUN
    
    def _lemmatize_text(self, text: str) -> str:
        """
        Lemmatize text using WordNet lemmatizer with correct POS.
        
        Args:
            text: Text to lemmatize
            
        Returns:
            Lemmatized text
        """
        tokens = word_tokenize(text)
        pos_tags = nltk.pos_tag(tokens)
        
        return ' '.join([
            self.lemmatizer.lemmatize(
                word, 
                self._get_wordnet_pos(pos)
            ) for word, pos in pos_tags
        ])
    
    def _remove_vowels(self, text: str, keep_first: bool = True) -> str:
        """
        Remove vowels from text, optionally keeping the first letter.
        
        Args:
            text: Input text
            keep_first: Whether to keep the first character regardless of vowel status
            
        Returns:
            Text with vowels removed
        """
        if not text:
            return text
            
        if keep_first:
            first_char = text[0]
            rest = text[1:]
            return first_char + re.sub(r'[aeiou]', '', rest, flags=re.IGNORECASE)
        else:
            return re.sub(r'[aeiou]', '', text, flags=re.IGNORECASE)
    
    def _extract_important_words(self, text: str) -> List[str]:
        """
        Extract important words from text using spaCy.
        
        Args:
            text: Input text
            
        Returns:
            List of important words
        """
        doc = self.nlp(text)
        
        # Get nouns, proper nouns, and adjectives
        important_words = []
        for token in doc:
            # Include nouns, proper nouns, adjectives, and verbs
            if (token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'VERB') and 
                not token.is_stop and 
                len(token.text) > 2):
                important_words.append(token.text.lower())
        
        return important_words
    
    def _get_synonyms(self, word: str) -> List[str]:
        """
        Get synonyms for a word using WordNet.
        
        Args:
            word: Input word
            
        Returns:
            List of synonyms
        """
        synonyms = []
        
        # Get lemma
        word = self.lemmatizer.lemmatize(word)
        
        # Get synonyms from WordNet
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonym = lemma.name().lower().replace('_', ' ')
                if synonym != word and synonym not in synonyms:
                    synonyms.append(synonym)
        
        # Limit to reasonable number of synonyms
        return synonyms[:3]
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        Extract named entities from text using spaCy.
        
        Args:
            text: Input text
            
        Returns:
            List of named entities
        """
        doc = self.nlp(text)
        entities = [ent.text.lower() for ent in doc.ents]
        return entities
    
    def _generate_sensible_abbreviations(self, term: str) -> List[str]:
        """
        Generate sensible abbreviations using NLP techniques.
        
        Args:
            term: Input term
            
        Returns:
            List of generated abbreviations
        """
        abbreviations = []
        term = term.lower().strip()
        
        if not term:
            return abbreviations
        
        # Use spaCy to process the term
        doc = self.nlp(term)
        
        # 1. First letter acronym (for multi-word terms)
        if ' ' in term:
            # Get all words
            words = [token.text for token in doc if not token.is_punct and not token.is_space]
            
            # Standard acronym (first letter of each word)
            standard_acronym = ''.join(word[0] for word in words if word)
            if len(standard_acronym) > 1:
                abbreviations.append(standard_acronym)
            
            # Enhanced acronym (first letter of important words)
            important_words = [token.text for token in doc 
                            if token.pos_ in ('NOUN', 'PROPN', 'ADJ', 'VERB') 
                            and not token.is_stop]
            
            if important_words:
                enhanced_acronym = ''.join(word[0] for word in important_words)
                if len(enhanced_acronym) > 1 and enhanced_acronym != standard_acronym:
                    abbreviations.append(enhanced_acronym)
        
        # 2. Consonant skeleton variants
        # Standard consonant skeleton
        consonant_skeleton = self._remove_vowels(term, keep_first=True)
        if consonant_skeleton != term and len(consonant_skeleton) > 1:
            abbreviations.append(consonant_skeleton)
        
        # Partial consonant skeleton for longer terms
        if len(term) > 5:
            partial_skeleton = term[0] + self._remove_vowels(term[1:4], keep_first=False)
            if partial_skeleton not in abbreviations and len(partial_skeleton) > 1:
                abbreviations.append(partial_skeleton)
        
        # 3. Prefixes and truncations
        # First 2-4 characters
        if len(term) > 2:
            abbreviations.append(term[:min(3, len(term))])
            
            if len(term) > 4:
                abbreviations.append(term[:min(4, len(term))])
        
        # 4. For multi-word terms, create more abbreviations
        if ' ' in term:
            words = term.split()
            
            # For two-word terms
            if len(words) == 2:
                # First 3 chars of first word + first 3 chars of second word
                if len(words[0]) >= 2 and len(words[1]) >= 2:
                    combined = words[0][:min(3, len(words[0]))] + '_' + words[1][:min(3, len(words[1]))]
                    abbreviations.append(combined)
                
                # First word's consonant skeleton + second word's consonant skeleton
                first_cons = self._remove_vowels(words[0], keep_first=True)
                second_cons = self._remove_vowels(words[1], keep_first=True)
                if len(first_cons) > 1 and len(second_cons) > 1:
                    combined_cons = first_cons + '_' + second_cons
                    abbreviations.append(combined_cons)
            
            # For all multi-word terms
            if len(words) > 1:
                # Extract nouns and important words
                doc = self.nlp(term)
                important_pos = ('NOUN', 'PROPN')
                
                # Get all nouns in the term
                nouns = [token.text for token in doc if token.pos_ in important_pos]
                
                # If we have nouns, create abbreviations using just the nouns
                if len(nouns) > 1:
                    noun_acr = ''.join(noun[0] for noun in nouns)
                    if len(noun_acr) > 1:
                        abbreviations.append(noun_acr)
                
                # For longer compound terms (3+ words), also add first letters of first and last word
                if len(words) >= 3:
                    first_last = words[0][0] + words[-1][0]
                    if len(first_last) == 2:
                        abbreviations.append(first_last)
        
        # 5. Special handling for industry-specific patterns
        # For terms like "employee number", "customer identifier", etc.
        if ' ' in term and len(term.split()) == 2:
            words = term.split()
            
            # Check if second word is an identifier word
            id_words = ['number', 'id', 'identifier', 'code', 'key', 'reference']
            
            if any(id_word == words[1].lower() for id_word in id_words):
                # Create abbreviated version with first word abbreviated + id word abbreviated
                abbr_first = self._remove_vowels(words[0], keep_first=True)
                abbr_id = words[1][0:3]  # Take first 3 chars of id word
                abbreviations.append(f"{abbr_first}_{abbr_id}")
        
        # Remove duplicates
        return list(set(abbreviations))
    
    def create_pattern(self, name: str, definition: str = "", 
                     related_terms: Union[str, List[str]] = "", 
                     acronyms: Union[str, List[str]] = "") -> Dict:
        """
        Create a robust regex pattern for lexical matching.
        
        Args:
            name: The main term
            definition: Term definition
            related_terms: Related terms (comma-separated string or list)
            acronyms: Known acronyms (comma-separated string or list)
            
        Returns:
            Dictionary with pattern and regex object
        """
        # Normalize inputs
        name = str(name).strip().lower() if name else ""
        definition = str(definition).strip().lower() if definition else ""
        
        # Convert to lists if they're strings
        if isinstance(related_terms, str):
            related_terms_list = [term.strip().lower() for term in related_terms.split(',') if term.strip()]
        else:
            related_terms_list = [term.lower().strip() for term in related_terms if term]
        
        if isinstance(acronyms, str):
            acronyms_list = [acr.strip().lower() for acr in acronyms.split(',') if acr.strip()]
        else:
            acronyms_list = [acr.lower().strip() for acr in acronyms if acr]
        
        # Pattern collection
        patterns = set()
        
        # 1. Add exact name match
        if name:
            patterns.add(self._escape_regex(name))
            
            # Add lemmatized version if different
            lemma_name = self._lemmatize_text(name)
            if lemma_name != name:
                patterns.add(self._escape_regex(lemma_name))
        
        # 2. Generate and add sensible abbreviations from name
        generated_abbreviations = self._generate_sensible_abbreviations(name)
        for abbr in generated_abbreviations:
            patterns.add(self._escape_regex(abbr))
        
        # 3. Extract important words from name using spaCy
        if name:
            important_words = self._extract_important_words(name)
            for word in important_words:
                if len(word) > 2 and word != name:
                    patterns.add(self._escape_regex(word))
        
        # 4. Add related terms and their abbreviations
        for term in related_terms_list:
            patterns.add(self._escape_regex(term))
            
            # Add lemmatized version
            lemma_term = self._lemmatize_text(term)
            if lemma_term != term:
                patterns.add(self._escape_regex(lemma_term))
            
            # Generate and add abbreviations for related terms
            term_abbrevs = self._generate_sensible_abbreviations(term)
            for abbr in term_abbrevs:
                patterns.add(self._escape_regex(abbr))
                
            # For multi-word related terms, add important words
            important_words = self._extract_important_words(term)
            for word in important_words:
                if len(word) > 2:
                    patterns.add(self._escape_regex(word))
        
        # 5. Add explicit acronyms
        for acronym in acronyms_list:
            patterns.add(self._escape_regex(acronym))
        
        # 6. Extract meaningful terms from definition using NLP
        if definition:
            # Process with spaCy to get important words
            important_def_words = self._extract_important_words(definition)
            
            # Add top 3 important words from definition
            for word in important_def_words[:3]:
                if len(word) > 3:
                    patterns.add(self._escape_regex(word))
            
            # Extract any named entities
            entities = self._extract_entities(definition)
            for entity in entities:
                if len(entity) > 2:
                    patterns.add(self._escape_regex(entity))
        
        # 7. Generate compound patterns for special cases
        if name and related_terms_list:
            # Generate name abbreviation + related term combinations
            name_abbrevs = self._generate_sensible_abbreviations(name)
            
            for rel_term in related_terms_list:
                # Look for related terms containing identifier words
                id_words = ['number', 'code', 'id', 'identifier', 'key', 'reference']
                rel_lower = rel_term.lower()
                
                for id_word in id_words:
                    if id_word in rel_lower:
                        # For each abbreviation + id_word combination
                        for abbr in name_abbrevs:
                            if len(abbr) > 1:
                                # Create patterns like "emply_num", "emply_id", etc.
                                compound = f"{abbr}[_\\-\\s]*{id_word[:3]}"
                                patterns.add(compound)
                                
                                # Also try with just the first letter of the id word
                                compound_short = f"{abbr}[_\\-\\s]*{id_word[0]}"
                                patterns.add(compound_short)
        
        # 8. Add synonyms for important terms (limited to avoid overly broad matching)
        if name:
            # Get the main words from the name
            main_words = [word for word in name.split() if len(word) > 3 and word not in self.stop_words]
            
            # Add synonyms for the main words (limited to key terms)
            for word in main_words[:1]:  # Limit to just the first main word
                synonyms = self._get_synonyms(word)
                # Add only the first 2 synonyms to avoid too broad matching
                for synonym in synonyms[:2]:
                    if len(synonym) > 3:
                        patterns.add(self._escape_regex(synonym))
        
        # Build final pattern with word boundaries and flexible separators
        pattern_list = list(patterns)
        combined_pattern = '|'.join(pattern_list)
        
        # Final pattern with:
        # - Word boundaries
        # - Optional separators between pattern components
        # - Case insensitivity
        final_pattern = f"\\b(?:{combined_pattern})(?:[_\\-\\s]*(?:{combined_pattern}))*\\b"
        
        return {
            'pattern': final_pattern,
            'regex': re.compile(final_pattern, re.IGNORECASE),
            'original_term': name,
            'generated_abbreviations': generated_abbreviations,
            'all_terms': list(patterns)
        }
    
    def process_csv(self, file_path: str, 
                   name_col: str = 'name', 
                   def_col: str = 'definition',
                   rel_terms_col: str = 'related terms',
                   acronyms_col: str = 'acronyms') -> Dict:
        """
        Process a CSV file to generate regex patterns for all terms.
        
        Args:
            file_path: Path to the CSV file
            name_col: Column name for the main term
            def_col: Column name for definition
            rel_terms_col: Column name for related terms
            acronyms_col: Column name for acronyms
            
        Returns:
            Dictionary of patterns keyed by term name
        """
        try:
            df = pd.read_csv(file_path)
            
            # Ensure required columns exist
            if name_col not in df.columns:
                raise ValueError(f"CSV file must contain a '{name_col}' column")
            
            # Process each row
            for _, row in df.iterrows():
                name = row.get(name_col, '')
                if not name:
                    continue
                    
                definition = row.get(def_col, '') if def_col in df.columns else ''
                related_terms = row.get(rel_terms_col, '') if rel_terms_col in df.columns else ''
                acronyms = row.get(acronyms_col, '') if acronyms_col in df.columns else ''
                
                self.patterns[name] = self.create_pattern(name, definition, related_terms, acronyms)
            
            return self.patterns
            
        except Exception as e:
            print(f"Error processing CSV: {e}")
            return {}
    
    def match(self, term: str, original_term: str) -> bool:
        """
        Check if a term matches a pattern.
        
        Args:
            term: Term to check
            original_term: Original term whose pattern to use
            
        Returns:
            True if match found, False otherwise
        """
        if original_term not in self.patterns:
            return False
            
        pattern = self.patterns[original_term]['regex']
        return bool(pattern.search(term))
    
    def get_all_patterns(self) -> Dict:
        """
        Get all generated patterns.
        
        Returns:
            Dictionary of all patterns
        """
        return self.patterns


# Example usage
if __name__ == "__main__":
    # Create a matcher
    matcher = LexicalMatcher()
    
    # Example with single term
    employee_pattern = matcher.create_pattern(
        name="employee",
        definition="A person who works for a company",
        related_terms="training, expense account, number",
        acronyms="emp, ee"
    )
    
    print(f"Original term: {employee_pattern['original_term']}")
    print(f"Generated abbreviations: {', '.join(employee_pattern['generated_abbreviations'])}")
    print(f"Regex pattern: {employee_pattern['pattern']}")
    
    # Test matching
    test_terms = [
        "EMPLY_NUM", "employee", "Employee Number", "emp",
        "employee training", "EMPLOYEE_TRAINING", "expense account",
        "number", "expns_acct", "something unrelated", "empl"
    ]
    
    print("\nMatching test:")
    for test in test_terms:
        match = employee_pattern['regex'].search(test) is not None
        print(f"  \"{test}\" => {'✓ Match' if match else '✗ No match'}")
    
    # Example with CSV processing
    print("\nTo process a CSV file:")
    print("matcher = LexicalMatcher()")
    print("patterns = matcher.process_csv('your_terms.csv')")
    
    # Explanation of installation requirements
    print("\n# Requirements Installation:")
    print("# pip install spacy nltk pandas")
    print("# python -m spacy download en_core_web_sm")
