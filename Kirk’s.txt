class ChromaVectorStore:
    """ChromaDB vector store manager with multi-column support"""
    
    def __init__(self, env: OSEnv):
        self.env = env
        self._validate_azure_credentials()
        self.embeddings = self._init_embeddings()
        self.vector_store = None
        self.collection_name = None

    @staticmethod
    def _read_csv_safely(file_path: Path) -> pd.DataFrame:
        """Read CSV with multiple encoding attempts and error handling."""
        encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
        errors = []
        
        for encoding in encodings:
            try:
                logger.info(f"Attempting to read CSV with {encoding} encoding")
                return pd.read_csv(file_path, encoding=encoding)
            except Exception as e:
                errors.append(f"{encoding}: {str(e)}")
                continue
        
        raise ValueError(f"Failed to read CSV with any encoding. Errors:\n" + "\n".join(errors))

    def create_collection(self, csv_path: Path, text_columns: List[str], 
                        chunk_size: int = 1000, chunk_overlap: int = 100,
                        separator: str = " | ") -> None:
        """
        Create Chroma collection from CSV with multiple text columns.
        """
        try:
            logger.info(f"Reading CSV file: {csv_path}")
            df = self._read_csv_safely(csv_path)
            
            # Validate all requested columns exist
            missing_cols = [col for col in text_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
            
            logger.info(f"Processing columns: {', '.join(text_columns)}")
            documents = self._process_csv_multi_column(
                df, text_columns, chunk_size, chunk_overlap, separator
            )
            
            self.collection_name = csv_path.stem.lower()
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            os.makedirs(persist_directory, exist_ok=True)
            
            # Create Chroma collection
            self.vector_store = Chroma.from_documents(
                documents=documents,
                embedding=self.embeddings,
                collection_name=self.collection_name,
                persist_directory=persist_directory
            )
            
            logger.info(f"Created collection '{self.collection_name}' with {len(documents)} documents")
            self.vector_store.persist()
            
        except Exception as e:
            logger.error(f"Collection creation failed: {str(e)}")
            raise

    def _process_csv_multi_column(
        self, df: pd.DataFrame, 
        text_columns: List[str],
        chunk_size: int = 1000, 
        chunk_overlap: int = 100,
        separator: str = " | "
    ) -> List[Document]:
        """Process CSV data with multiple text columns."""
        documents = []
        
        try:
            for idx, row in df.iterrows():
                # Combine text from all specified columns
                text_parts = []
                for col in text_columns:
                    if pd.notna(row[col]):
                        text_parts.append(f"{col}: {str(row[col]).strip()}")
                
                combined_text = separator.join(text_parts)
                if not combined_text.strip():
                    logger.warning(f"Empty combined text found in row {idx}, skipping")
                    continue
                
                # Create metadata from non-text columns
                metadata = {
                    col: str(row[col])
                    for col in df.columns
                    if col not in text_columns and pd.notna(row[col])
                }
                metadata['row_index'] = str(idx)
                
                # Chunk text if necessary
                if len(combined_text) > chunk_size:
                    chunks = self._chunk_text(combined_text, chunk_size, chunk_overlap)
                    for i, chunk in enumerate(chunks):
                        chunk_metadata = metadata.copy()
                        chunk_metadata['chunk_index'] = str(i)
                        chunk_metadata['total_chunks'] = str(len(chunks))
                        documents.append(Document(page_content=chunk, metadata=chunk_metadata))
                else:
                    documents.append(Document(page_content=combined_text, metadata=metadata))
                
                if (idx + 1) % 100 == 0:
                    logger.info(f"Processed {idx + 1} rows")
            
            return documents
            
        except Exception as e:
            logger.error(f"Error processing CSV row {idx}: {str(e)}")
            raise

    def search(self, query: str, k: int = 5, 
              min_relevance_score: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """Perform similarity search with metadata and relevance filtering."""
        try:
            if self.vector_store is None:
                raise ValueError("No collection loaded. Please create or load a collection first.")
                
            logger.info(f"Performing search with query: {query}")
            results = self.vector_store.similarity_search_with_score(query, k=k)
            
            # Process and filter results
            processed_results = []
            for doc, score in results:
                relevance = 1 - score  # Convert distance to similarity score
                if relevance >= min_relevance_score:
                    processed_results.append((doc.page_content, relevance, doc.metadata))
            
            logger.info(f"Found {len(processed_results)} relevant results")
            return processed_results
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            raise

    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Split text into overlapping chunks."""
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = start + chunk_size
            chunk = text[start:end]
            
            # Adjust chunk to not break words
            if end < text_len:
                last_space = chunk.rfind(' ')
                if last_space != -1:
                    end = start + last_space + 1
                    chunk = text[start:end]
            
            chunks.append(chunk)
            start = end - overlap
            
        return chunks

    def load_existing_collection(self, collection_name: str) -> None:
        """Load an existing Chroma collection."""
        try:
            persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
            if not os.path.exists(persist_directory):
                raise ValueError(f"Persist directory not found: {persist_directory}")
            
            self.collection_name = collection_name
            self.vector_store = Chroma(
                collection_name=collection_name,
                embedding_function=self.embeddings,
                persist_directory=persist_directory
            )
            
            collection_count = len(self.vector_store.get()['ids'])
            logger.info(f"Loaded existing collection '{collection_name}' with {collection_count} documents")
            
        except Exception as e:
            logger.error(f"Failed to load collection: {str(e)}")
            raise

    def close(self) -> None:
        """Clean up resources and persist data."""
        try:
            if self.vector_store:
                self.vector_store.persist()
                logger.info("Vector store persisted successfully")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {str(e)}")
            raise
