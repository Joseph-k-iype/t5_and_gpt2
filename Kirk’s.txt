def _init_embeddings(self) -> AzureOpenAIEmbeddings:
    """Initialize Azure OpenAI embeddings using specific configuration."""
    try:
        # Get Azure AD token
        token = self.env.get_azure_token(force_refresh=True)
        
        # Use the specific endpoint from your configuration
        azure_endpoint = "https://abc.openai.azure.com/openai/deployments/text-embedding-1-large"
        
        logger.info(f"Initializing embeddings with endpoint: {azure_endpoint}")
        
        # Initialize embeddings with your specific configuration
        embeddings = AzureOpenAIEmbeddings(
            deployment="text-embedding-1-large",  # From your MODEL setting
            model="text-embedding-1-large",       # Match the deployment
            api_version="2023-05-15",            # From your configuration
            azure_endpoint=azure_endpoint,
            azure_ad_token=token,
            openai_api_type="azure_ad",
            chunk_size=8,                        # Smaller batch size for reliability
            max_retries=3                        # Add retries for reliability
        )
        
        # Test the connection
        logger.info("Testing embeddings connection...")
        test_embedding = embeddings.embed_query("test")
        logger.info(f"Successfully connected to Azure OpenAI embeddings. Dimension: {len(test_embedding)}")
        
        return embeddings
        
    except Exception as e:
        error_msg = f"Failed to initialize Azure OpenAI embeddings: {str(e)}"
        logger.error(error_msg)
        logger.error("Full error details:", exc_info=True)
        raise Exception(error_msg)

def create_collection(self, csv_path: Path, text_columns: List[str], 
                     chunk_size: int = 1000, chunk_overlap: int = 100,
                     separator: str = " | ", batch_size: int = 100) -> None:
    """Create collection with specific Azure OpenAI configuration."""
    try:
        logger.info(f"Reading CSV file: {csv_path}")
        df = self._read_csv_safely(csv_path)
        
        missing_cols = [col for col in text_columns if col not in df.columns]
        if missing_cols:
            raise ValueError(f"Columns not found in CSV: {', '.join(missing_cols)}")
        
        logger.info(f"Processing columns: {', '.join(text_columns)}")
        all_documents = []
        total_rows = len(df)
        
        # Process in smaller batches for reliability
        batch_size = min(batch_size, 50)  # Limit batch size
        
        for start_idx in range(0, total_rows, batch_size):
            end_idx = min(start_idx + batch_size, total_rows)
            batch_df = df.iloc[start_idx:end_idx]
            
            batch_documents = self._process_csv_multi_column(
                batch_df, text_columns, chunk_size, chunk_overlap, separator
            )
            all_documents.extend(batch_documents)
            
            logger.info(f"Processed rows {start_idx + 1} to {end_idx} of {total_rows}")
        
        self.collection_name = csv_path.stem.lower()
        persist_directory = self.env.get("CHROMA_PERSIST_DIR", "./chroma_db")
        os.makedirs(persist_directory, exist_ok=True)
        
        # Disable telemetry explicitly
        chromadb.Client(Settings(anonymized_telemetry=False))
        
        # Create the vector store with retries
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"Attempt {attempt + 1} to create Chroma collection...")
                
                # Start with a small batch to test
                initial_documents = all_documents[:5]
                self.vector_store = Chroma.from_documents(
                    documents=initial_documents,
                    embedding=self.embeddings,
                    collection_name=self.collection_name,
                    persist_directory=persist_directory,
                    client=self.client
                )
                
                # If initial batch succeeds, add the rest
                if len(all_documents) > 5:
                    logger.info("Adding remaining documents...")
                    for i in range(5, len(all_documents), batch_size):
                        batch = all_documents[i:i + batch_size]
                        self.vector_store.add_documents(batch)
                        logger.info(f"Added documents {i} to {i + len(batch)}")
                
                logger.info(f"Created collection '{self.collection_name}' with {len(all_documents)} documents")
                self.vector_store.persist()
                logger.info(f"Vectors stored locally in {persist_directory}")
                break
                
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"Attempt {attempt + 1} failed: {str(e)}. Retrying...")
                    time.sleep(2 ** attempt)  # Exponential backoff
                else:
                    raise Exception(f"Failed to create collection after {max_retries} attempts: {str(e)}")
        
    except Exception as e:
        error_msg = f"Collection creation failed: {str(e)}"
        logger.error(error_msg)
        logger.error("Full error details:", exc_info=True)
        raise Exception(error_msg)
